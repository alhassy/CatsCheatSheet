#+TITLE: Reference Sheet for Elementary Category Theory
#+AUTHOR: [[http://www.cas.mcmaster.ca/~alhassm/][Musa Al-hassy]] @@latex:{\tiny\hspace{5.5em}\url{https://github.com/alhassy/CatsCheatSheet}}@@
#+EMAIL: alhassy@gmail.com
#+DESCRIPTION: Cheatsheet of category theory: Adjunctions, [co]limits, [co]products, [co]terminals, etc.
#+TODO: TODO | space
#+OPTIONS: d:nil

# (shell-command "rm CheatSheet.aux")

* More :ignore:

#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{newunicodechar}
#+LATEX_HEADER: \newunicodechar{ï¹”}{\ensuremath{\raisebox{0.4ex}{\tiny \,;\,}}}  %% forward composition ï¹”

#+LATEX_HEADER: \newunicodechar{Î”}{\ensuremath{\Delta}}
#+LATEX_HEADER: \newunicodechar{ğ’³}{\ensuremath{\mathcal{X}}}
#+LATEX_HEADER: \newunicodechar{ğ’±}{\ensuremath{\mathcal{V}}}
#+LATEX_HEADER: \newunicodechar{ğ’²}{\ensuremath{\mathcal{W}}}

#+LATEX_HEADER: \newunicodechar{Ì‡}{\ensuremath{.}}
#+LATEX_HEADER: \newunicodechar{â€²}{\ensuremath{'}}


#+LATEX_HEADER: \usepackage{calculation}

#+INCLUDE: CheatSheet/CheatSheetSetup.org

# https://en.wikipedia.org/wiki/Linguistic_relativity#Programming_languages

:nonstop:
(setq org-latex-pdf-process
      '("pdflatex  -shell-escape -interaction nonstopmode -output-directory %o %f"))

(setq org-latex-pdf-process
      '("pdflatex  -shell-escape -output-directory %o %f"))

:End:

# (async-shell-command "rm CheatSheet.tex")

# latex_header: \newunicodechar{ğ’}{\mathcal{C}}

* COMMENT In Defence of Backwards Composition :for_future_self:

For some time, I've preferred forwards composition as in Z notation.
Especially since my academic setting has people using that notation as well.

However here are some reasons I've decided to switch to the traditional format:

+ It is much more ubiquitous; therefore easier to understand without the necessary simple explanation.

+ It is Haskell's notion of function composition, therefore allowing the immediate translation
  of categorically derived programs to be implemented in Haskell. It also permits a nice
  implementation of some categorical concepts without must trouble.

+ I want to motivate the â€œcalculational styleâ€, so I do not want to deviate in orthogonal
  directions.

* LaTeX Setup :ignore:

# INCLUDE: /Users/musa/MyUnicodeSymbols/MyUnicodeSymbols.sty
#+LATEX_HEADER: \usepackage{CheatSheet/UnicodeSymbols}
# LATEX_HEADER: \usepackage[utf8]{inputenc}
# LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{amsmath}
# LATEX_HEADER: \usepackage{newunicodechar}

# â© \pointer from wasysym package
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \newunicodechar{Î±}{\ensuremath{\alpha}}
#+LATEX_HEADER: \newunicodechar{Ï}{\ensuremath{\rho}}
#+LATEX_HEADER: \newunicodechar{â©}{\,{\text{\pointer}}\,}

#+LATEX_HEADER: \newunicodechar{Ï•}{\ensuremath{\phi}}
#+LATEX_HEADER: \newunicodechar{âŸµ}{\ensuremath{\longleftarrow}}
#+LATEX_HEADER: \newunicodechar{â€¼}{\ensuremath{! \! !}}

#+begin_export latex
\def\providedS{ \qquad\Leftarrow\qquad }

\def\impliesS{ \qquad\Rightarrow\qquad }

\def\landS{ \qquad\land\qquad }
\def\lands{ \quad\land\quad }

\def\eqs{ \quad=\quad}

\def\equivs{ \quad\equiv\quad}
\def\equivS{ \qquad\equiv\qquad}

\def\begineqns{ \begingroup\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt} }
\def\endeqns{ \endgroup }
% \def\endeqns{ \endgroup \setlength{\belowdisplayskip}{2pt} } % put belowspace back to desired setting
#+END_EXPORT

# See defn-Type, below for an expanded usage; \eqn{name}{formula}
# LaTeX: \setlength{\abovedisplayskip}{5pt} \setlength{\belowdisplayskip}{2pt}
#+LaTeX: \def\eqn#1#2{ \begin{flalign*} #2 && \tag*{\sc #1} \label{#1} \end{flalign*}  }

# LATEX_HEADER: \setlength{\parskip}{1em}
# LaTeX: \setlength{\parskip}{0.5em}

#+LaTeX: \def\room{\vspace{0.5em}}

#+BEGIN_EXPORT latex
\def\Obj{\mathsf{Obj}}
\def\Hom{\mathsf{Hom}}
\def\src{\mathsf{src}}
\def\tgt{\mathsf{tgt}}
\def\Id{\mathsf{Id}}

\def\inl{\mathsf{inl}}
\def\inr{\mathsf{inr}}

\def\bin{I\!\!I}
#+END_EXPORT

* COMMENT Compact Notation

 Why do people look for compact notations? A compact notation leads to
 shorter documents (less lines of code in programming) in which patterns are easier
 to identify and to reason about. Properties can be stated in clear-cut, one-line long
 equations which are easy to memorize. And diagrams such as (2.4) can be easily
 drawn which enable us to visualize maths in a graphical format.

* Categories
#
# The morphisms are a "dependent type" ;-)
#
A *category* ğ’ consists of a collection of â€œobjectsâ€ $\Obj\, ğ’$,
  a collection of  â€œ(homo)morphismsâ€ $\Hom_ğ’(a,b)$ for any $a,b : \Obj\,ğ’$
  ---also denoted â€œ$a \,\to_ğ’\, b$â€---,
  an operation $\Id$ associating a morphism $\Idâ‚ : \Hom(a,a)$ to each object $a$,
  and a dependently-typed â€œcompositionâ€ operation
  $\_âˆ˜\_ : âˆ€\{A \, B \, C : \Obj\} â†’ \Hom(B,C) â†’ \Hom(A,B) â†’ \Hom(A,C)$
  that is required to be associative with $\Id$ as identity.

# For other approaches see https://tex.stackexchange.com/a/12035/69371
#
# As we can see from \eqref{defn-Type}\ldots
#

#+LaTeX: \room

  It is convenient to define a pair of operations $\src, \tgt$ from morphisms to objects
  as follows:
\begin{flalign*}
    f : X \to_ğ’ Y \quad\equiv\quad \mathsf{src}\; f = X \;\land\; \mathsf{tgt}\; f = Y
   &&
   \tag*{$\src,\tgt$-Definition}
   \label{src-tgt-Definition}
\end{flalign*}

Instead of $\Hom_ğ’$ we can instead assume primitive a ternary relation
$\_:\_â†’_ğ’\_$ and regain $\Hom_ğ’$ precisely when the relation is functional
in its last two arguments:
\eqn{type-Unique}{f : A \to_ğ’ B \;\;\land\;\; f : A' \to_ğ’ B' \;\implies\; A=A' \;\land\; B=B'}
When this condition is dropped, we obtain a /pre-category/; e.g., the familiar /Sets/
is a pre-category that is usually treated as a category by making morphisms
contain the information about their source and target: $(A, f, B) : A â†’ B$
rather than just $f$.
\newline
 /This is sometimes easier to give than Hom! C.f. Alg(F)./
\room

Here's an equivalence-preserving property that is useful in algebraic calculations,
#+LaTeX: \eqn{Composition}{ f : A â†’ B \lands g : B â†’ A \equivs g âˆ˜ f : A â†’ A \lands f âˆ˜ g : B â†’ B}

# A categorical statement is an expression built from notations for objects,
# typing, morphisms, composition, and identities by means of the usual logical
# connectives and quantifications and equality.
#

Examples:

+ [[https://arxiv.org/abs/1312.4818][Linear Algebra:]] Matrices with real number values determine a category whose objects are the natural numbers,
  morphisms $n â†’ m$ are $n Ã— m$ matrices, $\Id$ is the identity matrix, and composition
  is matrix multiplication.

+ Each preorder determines a category: The objects are the elements
  and there is a morphism $a â†’ b$ named, say, â€œ$(a, b)$â€, precisely when $a \leq b$;
  composition boils down to transitivity of $\leq$.

+ Each monoid $(M, âŠ•, e)$ gives rise to a category: The objects and the arrows
  are both the elements of$M$, and $k : m â†’ n \;â‰¡\; k âŠ• m = n$.
  E.g., $(â„•, Ã—, 1)$ gives rise to a category whose products are gcd's
  and so properties of products are thus gcd theorems!

+ Each digraph determines a category: The objects are the nodes
  and the paths are the morphisms typed with their starting and ending node.
  Composition is catenation of paths and identity is the empty path.

+ Suppose we have an `interface', in the programming sense,
  of constant, function, and relation symbols ---this is also called a /signature/.

  Let ğ’¯ be any collection of sentences in the first-order language of signature $\Sigma$.
  Then we can define a category $\mathsf{Mod}\,ğ’¯$ whose objects are
  implementations of interface $\Sigma$ satisfying constraints ğ’¯, and whose morphisms
  are functions that preserve the $\Sigma$ structure.
  Ignoring constraints ğ’¯ gives us `functor algebras'.

  Particular examples include monoids and structure-preserving maps between them;
  likewise digraphs, posets, rings, etc and their homomorphisms.

\room

Even when morphisms are functions, the objects need not be sets:
Sometimes the objects are /operations/ ---with an appropriate definition
of typing for the functions. The categories of /F/-algebras are an example
of this.

* â€œGluingâ€ Morphisms Together

Traditional function application is replaced by the more generic concept of
functional /composition/ suggested by morphism-arrow chaining:


Whenever we have two morphisms such that the target type of one
of them, say $g : B â† A$ is the same as the source type of the other,
say $f : C â† B$ then â€œ$f$ after $g$â€, their /composite morphism/,
$f âˆ˜ g : C â† A$ can be defined. It â€œgluesâ€ $f$ and $g$ together,
â€œsequentiallyâ€:

#+BEGIN_EXPORT latex
\eqn{composition-Type}{
  C \overset{f}{\longleftarrow} % B \lands
  B \overset{g}{\longleftarrow} A \impliesS
  C \overset{\;f âˆ˜ g}{\longleftarrow} A
}
#+END_EXPORT

Composition is the basis for gluing morphisms together to build more complex morphisms.
However, not every two morphisms can be glued together by composition.

\room

Types provide the interface for putting morphisms together to obtain more complex functions.

\room

A /split/ arises wherever two morphisms do not compose but share the same source.
  - Since they share the same source, their outputs can be paired: $c â†¦ (f\, c, g\, c)$.
  - This duplicates the input so that the functions can be executed in â€œparallelâ€ on it.

\room

A /product/ appears when there is no explicit relationship between the types of the morphisms.
  - We regard their sources as projections of a product, whence they can be seen as /splits/.
  - This $(c, d) â†¦ (f\, c, g\, d)$ corresponds to the â€œparallelâ€ application of $f$ and $g$,
     each with its /own/ input.

\room

An /either/ arises wherever two morphisms do not compose but share the same target.
  - Apply $f$ if the input is from the â€œ$A$ sideâ€ or apply $g$ if it is from the â€œ$B$ sideâ€.
  - This is a â€œcase analysisâ€ of the input with branches being either $f$ or $g$.

\room

A /sum/ appears when there is no explicit relationship between the types of the morphisms.
  - We regard their targets as injections into a sum, whence they can be seen as /eithers/.

\room

#+LaTeX: \def\transpose#1{ \overline{#1} }
A /transpose/ arises when we need to combine a binary morphism with a unary morphism.
  - I.e., it arises when a composition chain is interrupted by an extra product argument.
  - Express $f$ as a /C/-indexed family, $f_c : A â†’ B$, of morphisms such that applying a function at any index
    behaves like $f$; i.e., $f_c \, a = f(c, a)$. Each $f_c$ can now be composed with $g$.
    Let $\transpose{(\;)}$ denote the operation $f â†¦ f_c$.

\vspace{-0.5em}

#+BEGIN_EXPORT latex
\begineqns

\eqn{split-Type}{ A \overset{f}{\longleftarrow} C \overset{g}{\longrightarrow} B
 \hspace{5.8em}\equivS A Ã— B \overset{\;\;âŸ¨f,gâŸ©}{\xleftarrow{\hspace*{0.5cm}}} C }

\eqn{x-Type}{ A \overset{f}{\longleftarrow} C \lands B \overset{g}{\longleftarrow} D
 \hspace{1.9em} \equivS A Ã— B \overset{f Ã— g}{\xleftarrow{\hspace*{0.5cm}}} C Ã— D }

\eqn{either-Type}{ A \overset{f}{\longrightarrow} C \overset{g}{\longleftarrow} B
 \hspace{5.9em}\equivS A + B \overset{[f,g]}{\xrightarrow{\hspace*{0.5cm}}} C }

\eqn{+-Type}{ A \overset{f}{\longrightarrow} C \lands B \overset{g}{\longrightarrow} D
 \hspace{2em} \equivS A + B \overset{f + g}{\xrightarrow{\hspace*{0.5cm}}} C + D }

\eqn{transpose-Type}{
  B \overset{f}{\longleftarrow}{C Ã— A}
  \hspace{6.9em} \equivS
  B^A \overset{\transpose{f}}{\longleftarrow} C
  }

  % By the well-formedness of the terms, we necessairly have
  % C = tgt g = fst (src f). Hence, both sides have the same information.
  %
\eqn{transpose-combination}{
  B \overset{f}{\longleftarrow}{C Ã— A}
  \lands
  C \overset{g}{\longleftarrow} D
  \equivS
  B^A \overset{\transpose{f} âˆ˜ g}{\xleftarrow{\hspace*{1cm}}} D
  }
\endeqns
#+END_EXPORT

\vspace{1em}

** COMMENT Importance of Compositional Combinators                   :ignore:

 # Paraphrasing Oliveira, Â§2.13
 \vspace{3ex}
 The compositional combinators put forward here are equipped with a concise /set of properties/
 which enable programmers to transform programs, reason about them, and perform useful calculations.
 This raises a /programming methodology/ which is scientific and stable.

* Functors

A *functor* /F : ğ’œ â†’ â„¬/ is a pair of mappings, denoted by one name,
from the objects, and morphisms, of ğ’œ to those of â„¬ such that
it respects the categorical structure:

#+BEGIN_EXPORT latex
{\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

\eqn{functor-Type}{F\, f : F\, A \to_â„¬ F\, B \quad\Leftarrow\quad f : A \to_ğ’œ B }

\eqn{Functor}{F\, \Id_A \;=\; \Id_{F\, A}}

\eqn{Functor}{F\, (f âˆ˜ g) \;=\; F\, f \circ F\, g}

}
#+END_EXPORT

\vspace{1em}

The two axioms are equivalent to the single statement that
/functors distribute over finite compositions, with $\Id$ being the empty composition:/
\[ F(f_0 âˆ˜ \cdots âˆ˜ f_{n-1}) \;=\; F\, f_0 âˆ˜ \cdots âˆ˜ F\, f_{n-1} \]

Use of Functors.
+ In the definition of a category, â€œobjectsâ€ are â€œjust thingsâ€ for which no internal
  structure is observable by categorical means ---composition, identities, morphisms, typing.
  /Functors form the tool to deal with â€œstructuredâ€ objects./

  Indeed in ğ’®â„¯ğ“‰ the aspect of a structure is that it has â€œconstituentsâ€, and that it is possible
  to apply a function to all the individual constituents; this is done by
  /F f : F A â†’ F B/.

+  For example, let $\bin A = A Ã— A$ and $\bin f = (x, y) â†¦ (f\, x, f\, y)$.
  So $\bin$ is or represents the structure of pairs; $\bin\, A$ is the set of pairs of /A/,
  and $\bin\, f$ is the function that applies /f/ to each constituent of a pair.

  - A /binary operation on A/ is then just a function $\bin A â†’ A$;
    in the same sense we obtain /F-ary operations/.

+ Also, /Seq/ is or represents the structure of sequences; /Seq A/ is the structure of sequences
  over /A/, and /Seq f/ is the function that applies /f/ to each constituent of a sequence.

+  Even though /F A/ is still just an object, a thing with no observable internal structure, the
  functor properties enable to exploit the â€œstructureâ€ of /F A/ by allowing us to â€œapplyâ€
  an /f/ to each â€œconstituentâ€ by using /F f/.

\vspace{1em}

Category $ğ’œlâ„Š(F)$
+ For a functor /F : ğ’œ â†’ ğ’Ÿ/, this category has /F-algebras/, /F/-ary operations in ğ’Ÿ as, objects
  --- i.e., objects are ğ’Ÿ-arrows $F\, A â†’ A$ ---
  and /F/-homomorphisms as morphisms, and it inherits composition and identities from ğ’Ÿ.

  #+BEGIN_EXPORT latex
  {\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

  \eqn{defn-Homomorphism}{f : âŠ• â†’_F âŠ— \quad\equiv\quad f âˆ˜ âŠ• = âŠ— âˆ˜ F\, f}

  \eqn{id-Homomorphism}{ \Id : âŠ• â†’_F âŠ• }

  \def\providedS{\qquad\Leftarrow\qquad}

  \eqn{comp-Homomorhism}{ g âˆ˜ f : âŠ™ â†_F âŠ• \providedS g : âŠ™ â†_F âŠ— \;\land\; f : âŠ— â†_F âŠ•}
  }
  #+END_EXPORT

  Note that category axiom \eqref{unique-Type} is not fulfilled since a function can be
  a homomorphism between several distinct operations. However, we pretend it is a category
  in the way discussed earlier, and so the carrier of an algebra is fully determined by
  the operation itself, so that the operation itself can be considered the algebra.

  #+BEGIN_CENTER
  /\ref{comp-Homomorhism} renders a semantic property as a syntactic condition!/
  #+END_CENTER

\vspace{1em}

+ A *contravariant functor* ğ’ â†’ ğ’Ÿ is just a functor /ğ’áµ’áµ– â†’ ğ’Ÿ/.
+ A *bifunctor* from ğ’ to ğ’Ÿ is just a functor /ğ’Â² â†’ ğ’Ÿ/.

* Naturality

A natural transformation is nothing but a structure preserving map between functors.
â€œStructure preservationâ€ makes sense, here, since we've seen already that a functor
is, or represents, a structure that objects might have.

\room

As discussed before for the case /F : ğ’ â†’ ğ’®â„¯ğ“‰/, each /F A/ denotes a structured set
and /F/ denotes the structure itself.

# \room
#
#+LaTeX: \paragraph{\footnotesize \textnormal{Example Structures}}\label{SeqPair-is-Pair-Seq}
\hspace{-1em}:
$\bin$ is the structure of pairs, /Seq/ is the structure of sequences,
/Seq Seq/ the structure of sequences of sequences,
$\bin \, Seq$ the structure of pairs of sequences ---which is naturally isomorphic
to $Seq \, \bin$ the structure of sequences of pairs!---, and so on.

\room

A â€œtransformationâ€ from structure /F/ to structure /G/ is a family of functions \newline
$Î· : âˆ€\{A\} â†’ F\, A â†’ G\, A$; and it is â€œnaturalâ€ if each $Î·_A$ doesn't affect the /constituents/
of the structured elements in /F A/ but only reshapes the structure of the elements,
from an /F/-structure to a /G/-structure.

\vspace{0em}

#+BEGIN_CENTER
/Reshaping the structure by Î· commutes with subjecting the constituents to an arbitrary morphism./
#+END_CENTER
# That is, $F\, f ï¹” t_B \;=\; t_A ï¹” G\, f$ for all /f : A â†’ B./

\vspace{-2em}
#+LaTeX: \eqn{ntrf-Def}{ Î· : F â†’Ì£ G \quad\equiv\quad âˆ€ f \;â€¢\; Î·_{\tgt\, f} âˆ˜ F\, f \;=\; G\, f âˆ˜ Î·_{\src\, f} }

This is `naturally' remembered: Morphism $Î·_{\tgt\, f}$ has type $F (\tgt\, f) â†’ G(\tgt\, f)$ and therefore
appears at the target side of an occurrence of /f/; similarly $Î·_{\src\, f}$ occurs at the source side of an /f/.
/Moreover/ since Î· is a transformation /from/ /F/ to /G/, functor /F/ occurs at the source side of an Î·
and functor /G/ at the target side.

\room

+ One also says /Î·â‚ is natural in/ its parameter /a/.

+ If we take $G = \Id$, then natural transformations are /F/-homomorphisms.
   Thus, naturality is a kind of homomorphism condition.
   # $F â†’Ì£ \Id$ are precisely /F/-homomorphisms.

+ Indeed, a natural transformation is a sort-of homomorphism in that the image of a morphism
  after reshaping is the same as the reshaping of the image.

\room

Example natural transformations
+ /rev : Seq â†’Ì£ Seq : [aâ‚, â€¦, aâ‚™] â†¦ [aâ‚™, â€¦, aâ‚]/
  reverses its argument thereby reshaping a sequence structure into a sequence structure without affecting the constituents.

+ /inits : Seq â†’Ì£ Seq Seq : [aâ‚, â€¦, aâ‚™] â†¦ [[], [aâ‚], â‹¯, [aâ‚, â€¦, aâ‚™]]/
  yields all initial parts of its argument
  thereby reshaping a sequence structure into a sequence of sequences structure, not affecting
  the constituents of its argument.

\room

#+BEGIN_EXPORT latex
\begineqns

\eqn{ntrf-Ftr}{ JÎ· : JF â†’Ì£ JG \providedS Î· : F â†’Ì£ G \quad \text{ where } (JÎ·)_A â‰” J(Î·_A) }

\eqn{ntrf-Poly}{ Î·K : FK â†’Ì£ GK  \hspace{-2ex}\providedS Î· : F â†’Ì£ G \quad \text{ where } (Î·K)_A â‰” Î·_{(K\, A) } }

\eqn{ntrf-Id}{ \Id_F : F â†’Ì£ F \text{\hspace{12em} where } (\Id_F)_A â‰” \Id_{(F\, A)} }

\eqn{ntrf-Compose}{ Î· âˆ˜ Îµ : H â†Ì£ F \hspace{2ex}\providedS Î· : H â†Ì£ G \lands Îµ : G â†Ì£ F
\\ \text{ where } (Î· âˆ˜ Îµ)_A = Î·_A âˆ˜ Îµ_A
 }

\endeqns
#+END_EXPORT

\room

*Category â„±ğ“Šğ“ƒğ’¸(ğ’, ğ’Ÿ)*
consists of functors /ğ’ â†’ ğ’Ÿ/ as objects and natural transformations between them as arrows.
The identity transformation is indeed an identity for transformation composition, which is associative.

\room

*Heuristic* To prove $Ï† = Ï†â‚ âˆ˜ â‹¯ âˆ˜ Ï†â‚™ : F â†’Ì£ G$ is a natural transformation, it suffices
to show that each $Ï†áµ¢$ is a natural transformation.
E.g., without even knowing the definitions, naturality of
/tails = Seq rev âˆ˜ inits âˆ˜ rev/ can be proven ---just type checking!
# thanks to \eqref{ntrf-Compose}!

\iffalse
 + Theorem \eqref{ntrf-Compose} renders proofs of semantic properties to be trivial type checking!
 + E.g., It's trivial to prove /tails = rev ï¹” inits ï¹” Seq rev/ is a natural transformation
   by type checking, but to prove the naturality equation by using the naturality equations of
   /rev/ and /inits/ ---no definitions required--- necessitates more writing, and worse: Actual thought!
\fi

* Adjunctions

An adjunction is a particular one-one correspondence between different kinds of
morphisms in different categories.

\room

An *adjunction* consists of two functors $L : ğ’œ â†’ â„¬$ and $R : â„¬ â†’ ğ’œ$,
as well as two (not necessarily natural!) transformations
$Î· : \Id â†’ RL$ and $Îµ : LR â†’ \Id$ such that

\vspace{-1em}

#+BEGIN_EXPORT latex
\eqn{Adjunction}{\text{\tiny Provided $f : A â†’_ğ’œ R B$ and $g : L A â†’_â„¬ B$ }
 \\ f = R g âˆ˜ Î·_A \equivS Îµ_B âˆ˜ L f = g
}
#+END_EXPORT

Reading right-to-left: In the equation $Îµ_B âˆ˜ L f = g$ there is a unique solution to the unknown $f$.
Dually for the other direction.

\room

That is,
/each L-algebra g is uniquely determined ---as an L-map followed by an Îµ-reduce---/
/by its restriction to the adjunction's unit Î·./

\room

A famous example is â€œFree âŠ£ Forgetfulâ€, e.g. to /define/ the list datatype, for which the above
becomes: Homomorphisms on lists are uniquely determined, as a map followed by a reduce,
by their restriction to the singleton sequences.

\room

We may call $f$ the restriction, or lowering, of $g$ to the â€œunital caseâ€
and write $f = âŒŠgâŒ‹ = R g âˆ˜ Î·_A$. Also, we may call $g$ the extension, or raising,
of $f$ to an /L/-homomorphism and write $g = âŒˆfâŒ‰ = Îµ_B âˆ˜ L f$. The above equivalence
now reads:

#+BEGIN_EXPORT latex
\begineqns

\eqn{Adjunction-Inverse}{f = âŒŠgâŒ‹ \equivS âŒˆfâŒ‰ = g}

\eqn{lad-Type}{âŒŠgâŒ‹_{A,B} = R g âˆ˜ Î·_A \; : \; A â†’_ğ’œ R B
 \text{ where } g : L A â†’_â„¬ B
 }

\eqn{rad-Type}{âŒˆfâŒ‰_{A,B} = Îµ_B âˆ˜ L f \; : \; L A â†’_â„¬ B
 \text{ where } f : A â†’_ğ’œ R B
 }

\endeqns
#+END_EXPORT

\room
\vspace{1ex}
Note that âŒˆ is like `r' and the argument to âŒˆâŒ‰ must involve the /R/-ight adjoint in its type;
# likewise for
#+LaTeX: {\textbf L}ad takes morphisms involving the {\textbf L}eft adjoint ;)

\room

This equivalence expresses that `lad' $âŒŠâŒ‹$, from \emph{l}eft \emph{ad}jungate,
and `rad' $âŒˆâŒ‰$, from \emph{r}ight \emph{ad}jungate, are each other's inverses
and constitute a correspondence between certain morphisms.
/Being a bijective pair, lad and rad are injective, surjective, and undo one another./

\room

We may think of â„¬ as having all complicated problems so we abstract
away some difficulties by \emph{r}aising up to a cleaner, simpler, domain
via rad âŒˆâŒ‰; we then solve our problem there, then go back \emph{down} to
the more complicated concrete issue via âŒŠâŒ‹, lad.
\newline
( E.g., â„¬ is the category of monoids, and ğ’œ is the category of sets; $L$ is the list functor. )

# Some useful results,
#+BEGIN_EXPORT latex
\begineqns

\eqn{ntrf-Adj}{\text{The Î· and Îµ determine each other and they are \emph{natural} transformations.}}

\vspace{2ex}
â€œzig-zag lawsâ€ The unit has a post-inverse while the counit has a pre-inverse:

\eqn{unit-Inverse}{ \Id = R Îµ âˆ˜ Î·}

\eqn{Inverse-counit}{ \Id = Îµ âˆ˜ L Î·}

\vspace{2ex}
The unit and counit can be regained from the adjunction inverses,

\eqn{unit-Def}{ Î· = âŒŠ\IdâŒ‹}

\eqn{counit-Def}{ Îµ = âŒˆ\IdâŒ‰ }

\vspace{2ex}
Lad and rad themselves are solutions to the problems of interest, \eqref{Adjunction}.

\eqn{lad-Self}{Îµ âˆ˜ L âŒŠgâŒ‹ = g}

\eqn{rad-Self}{R âŒˆfâŒ‰ âˆ˜ Î· = f }

\vspace{2ex}
The following laws assert a kind of
monoic-ness for Îµ and a kind of epic-ness for Î·.
Pragmatically they allow us to prove an equality
by shifting to a possibly easier equality obligation.

\eqn{lad-Unique}{ R g âˆ˜ Î· = R gâ€² âˆ˜ Î· \equivS g = gâ€²}

\eqn{rad-Unique}{Îµ âˆ˜ L f \,= Îµ âˆ˜ L fâ€² \,\equivS f = fâ€²}

\vspace{2ex}
Lad and rad are natural transformations in the category $â„±ğ“Šğ“ƒğ’¸(ğ’œ^{op} Ã— â„¬, ğ’®â„¯ğ“‰)$ realising
$(L X â†’ Y) â‰… (X â†’ G Y)$ where $X, Y$ are the first and second projection functors
and $(-â†’-) : ğ’^{op} Ã— ğ’ â†’ ğ’®â„¯ğ“‰$ is the hom-functor such that $(f â†’ g) h = g âˆ˜ h âˆ˜ f$.
\\ By extensionality in ğ’®â„¯ğ“‰, their naturality amounts to the laws:

\eqn{lad-Fusion}{âŒŠ y âˆ˜ g âˆ˜ L x  âŒ‹ \;=\; R y âˆ˜ âŒŠgâŒ‹ âˆ˜ x }

\eqn{rad-Fusion}{âŒˆ R y âˆ˜ f âˆ˜  x âŒ‰ \;=\; y âˆ˜ âŒˆfâŒ‰ âˆ˜ L x }

\room
\endeqns
#+END_EXPORT

Also,
+ Left adjoints preserve colimits such as initial objects and sums.
+ Right adjoints preserve limits such as terminal objects and products.

* Constant Combinators

#+LaTeX: \def\const#1{ \underline{#1} }

Opposite to the identity functions which do not lose any information,
we find functions which lose all, or almost all, information.
Regardless of their input, the output of these functions is always the
same value.

# \room
#
#+LaTeX: \def\K{\mathcal{K}}
#+LaTeX: \paragraph{\footnotesize \textnormal{The Constant Combinator}}\label{constant-combinator}
$\K : ğ’ â†’ â„±ğ“Šğ“ƒğ’¸(ğ’Ÿ,ğ’)$
+ For objects $x$, the ``constant functor'': \\
   $\K{x}\, y = x$ and $\K{x}\, f = \Id_x$ for objects $y$ and morphisms $f$.
+ For morphisms $f$, the ``constant natural transformation'': \\
   $\K{f} : \K{(\src f)} â†’Ì£ \K{(\tgt f)}$
   sending objects $y$ to morphism $\K{f}\, y = f$.

\room
Sometimes it is convenient to notate $\const{c} = \K \, c$
and refer to this as the /everywhere c/ operation.
#+LaTeX: \eqn{constant-functor-Defn}{\const{c}\, a = c}

The following property defines constant functors at the `pointfree level':
#+LaTeX: \eqn{constant-Fusion}{\const{c} âˆ˜ F = \const{c} }

Constant functors force any difference in behaviour for any two
functors to disappear:
#+LaTeX: \eqn{constant-Equality}{\const{c} âˆ˜ F = \const{c} âˆ˜ G}

Interestingly, functor composition and application
are bridged explicitly by the constant functor:
#+BEGIN_EXPORT latex
\eqn{Functor-Bridge}{ F âˆ˜ \const{c} = \const{F \, c}}
#+END_EXPORT

# Observe that /constant arrows that are epic necessarily target a terminal object./
* Monics and Epics

Identity functions and constant functions are limit points of the
functional spectrum with respect to information preservation.
All the other functions are in-between: They â€œloseâ€ some information,
which is regarded as uninteresting for some reason.

\room

How do functions lose information?
Basically in two ways: They may be â€œblindâ€ enough to confuse different
inputs, by mapping them to the same output, or they may ignore values
of their target. For instance, $\const{c}$ confuses all inputs
by mapping them all onto $c$. Moreover, it ignores all values of its
target apart from $c$.

\room

Functions which do not confuse their inputs are called /monics/:
They are â€œpost-cancellableâ€:

#+BEGIN_EXPORT latex
\eqn{monic-Defn}{ f \; \mathsf{monic} \equivS
  \left(âˆ€ h,k \;â€¢\; f âˆ˜ h = f âˆ˜ k \equivs h = k\right)
}
#+END_EXPORT

Functions which do not ignore values of their target are called
/epics/: They are â€œpre-cancellableâ€:
#+BEGIN_EXPORT latex
\eqn{epic-Defn}{ f \; \mathsf{epic} \equivS
  \left(âˆ€ h,k \;â€¢\; h âˆ˜ f = k âˆ˜ f \equivs h = k\right)
}
#+END_EXPORT

Intuitively, $h = k$ on all points of their source precisely when
they are equal on all image points of $f$, since $f$ being epic means
it outputs all values of their source.

\room

It is easy to check that â€œtheâ€ identity function is monic and epic,
while any constant function $\const{c}$ is not monic and is only
epic when its target consists only of $c$.

* Isos

#+LaTeX: \def\inverse{^{-1}}

An arrow is an /iso/ iff it is invertible; i.e., there is an â€œinverseâ€ morphism
$f\inverse$ with
\eqn{inverse-Char}{ f âˆ˜ f\inverse = \Id \landS f\inverse âˆ˜ f = \Id}

To /construct/ $f\inverse$, we begin by identifying its type which may give
insight into its necessary `shape' ---e.g., as a sum or a product---
then we pick one of these equations and try to reduce it as much as possible
until we arrive at a definition of $fË˜$, or its `components'.

  + E.g.,
    $coassocr = [\Id + \inl, \inr âˆ˜ \inr]$ of type $(A + B) + C â‰… A + (B + C)$,
    its inverse  /coassocl/ must be of the shape $[x, [y, z]]$ for unknowns
    $x,y,z$ which can be calculated
    by solving the equation $[x, [y, z]] âˆ˜ coassocr = \Id$ ---Do it!

# --E.g., the inverse of ~assoc~.

\room

The following rules can be of help if $f\inverse$ is found handier than isomorphism $f$
in reasoning,

\begineqns

\eqn{inverse-Shunting1}{ f âˆ˜ x = y \equivS x = f\inverse âˆ˜ y}

\eqn{inverse-Shunting2}{ x âˆ˜ f = y \equivS x = y âˆ˜ f\inverse}

\endeqns

\room
\room

Isos are necessarily monic and epic, but in general the other way
around is not true.

\room

Isomorphisms are very important because they convert data from one
â€œformatâ€, say $A$, to another format, say $B$, without losing
information. So $f$ and $fË˜$ are faithful protocols between the two
formats $A$ and $B$.
Of course, these formats contain the same â€œamountâ€ of information
although the same data adopts a â€œdifferentâ€ shape in each of them.
â”€c.f. \nameref{SeqPair-is-Pair-Seq}.

\room

Isomorphic data domains are regarded as â€œabstractlyâ€ the same;
then one write $A â‰… B$.

Finally, note that all classes of functions referred to so far
---identities, constants, epics, monics, and isos---
are closed under composition.

Monics to the initial object are necessarily isos!
# Simialr to the bottom of a poset.

# \newpage.\newpage
* COMMENT TODO More Morphism Properties

# \NTHM{}{Monic}{ compositionally post-cancellable ---nearly the same formulation as injective! }

If a composition is monic then its first arrow is monic.

\NTHM{}{Epic}{ compositionally pre-cancellable }

If a composition is epic then its last arrow is epic.

\NTHM{}{Split Monic}{ compositionally post-invertible }

\NTHM{}{Split Epic}{ compositionally pre-invertible }

\NTHM{}{Iso}{ Both epic and monic ; i.e., monic and split epic }

* COMMENT space newpage                                                      :ignore:
  \newpage
* Skolemisation
# â€œUp to Isomorphismâ€

If a property $P$ holds for precisely one class of isomorphic objects,
and for any two objects in the same class there is precisely one
isomorphism from one to the other, then we say that
/the P-object is unique up to unique isomorphism/.
For example, in ğ’®â„¯ğ“‰ the one-point set is unique up to a unique isomorphism,
but the two-point set is not.

\room

For example, an object /A/ is ``initial'' iff
$âˆ€ B  \;â€¢\;  âˆƒâ‚ f  \;â€¢\;  f : A â†’ B$, and such objects are unique
up to unique isomorphism ---prove it!
The formulation of the definition is clear but it's not very well suited for /algebraic manipulation/.

\room

A convenient formulation is obtained by `skolemisation': An assertion of the form
\[ âˆ€ x \;â€¢\; âˆƒâ‚ y \;â€¢\; R \, x \, y \]
is equivalent to: There's a function â„± such that
\[ \, \hspace{13em} âˆ€ x, y \;â€¢\; R \, x \, y \;â‰¡\; y = â„±\, x  \hspace{8em}\text{\sc(â„±-Char)} \]
In the former formulation it is the existential quantification â€œ$âˆƒ y$â€ inside the scope of a universal
one that hinders effective calculation. In the latter formulation the existence claim is brought to a
more global level: A reasoning need no longer be interrupted by the declaration and naming of the
existence of a unique $y$ that depends on $x$; it can be denoted just $â„±\, x$.
As usual, the final universal quantification can be omitted, thus simplifying the formulation once more.

\room

In view of the important role of the various $y$'s, these $y$'s deserve a particular notation that
triggers the reader of their particular properties. We employ bracket notation such as $â¦‡ x â¦ˆ$
for such $â„±\, x$: An advantage of the bracket notation is that no extra parentheses are needed
for composite arguments $x$, which we expect to occur often.

\room

The formula /characterising/ $â„±$ may be called `â„±-Char' and it immediately give us some results
by truthifying each side, namely `Self' and `Id'. A bit more on the naming:

| Type        | Possibly non-syntactic constraint on notation being well-formed |
| Self        | It, itself, is a solution                                       |
| Id          | How $\Id$ can be expressed using it                             |
| Uniq        | Its problem has a unique solution                               |
| Fusion      | How it behaves with respect to composition                      |
| Composition | How two instances, in full subcategories, compose               |

# More, to consider.
#
# | Reflection   | How Id can be expressed                                         |
# | Cancellation | How it can be eliminated                                        |
# | Fusion       | How composition goes into it                                    |
# | Absorption   | How a lifted, via functor application, composition goes into it |
# | Naturality   | How it commutes with composition                                |
# | Functor      | How it defines a functor                                        |
# | Definition   | How it's defined for arbitrary morphisms by being defined for the Id case; usually an instance of Absorption; c.f. x/y defined by 1/y defined. |

Note that the last 3 indicate how the concept interacts with the categorical structure:
$=, ï¹”, \Id$. Also note that Self says there's at least one solution and Uniq says there is
at most one solution, so together they are equivalent to â„±-Char ---however those two proofs
are usually not easier nor more elegant than a proof of â„±-Char directly.

\room

*Proving â„±-Char* is straightforwardly accomplished by providing a definition for â„±
and establishing â„±-Char ---these two steps can be done in parallel! Almost every such
proof has the following format, or a circular implication thereof: For arbitrary $x$ and $y$,

# \vspace{2em}
#+begin_calculation latex
    R \kern0.5ex x \kern0.5ex y
\step[â‰¡]{}
    â‹®
\step[â‰¡]{}
    y = \text{â€œan expression not involving $y$â€}
\step[â‰¡]{ ğ’¹â„¯ğ’»ğ’¾ğ“ƒâ„¯ $â„± \, x$ to be the right side of the previous equation }
    y = â„± \kern0.5ex x
#+end_calculation

#  [Dangerous] Convention:
#  free variables are quantified implicitly in such a way that the laws in which they appear
#  are well-defined.

 # The relevance of universal properties, such as â„±-Char, is that it offers a way of /solving equations/
 # of the form $y = â„±\, x$. For example, can the identity be expressed, or `reflected', using this combinator?
 # We just solve the equation $\Id = â„±\, x$ for unknown(s) $x$ by appealing to the universal property.

* Initiality

# Convenient definition:
An object /0 is initial/ if there's a mapping $â¦‡-â¦ˆ$, from objects to morphisms,
such that \ref{initial-Char} holds; from which we obtain a host of useful corollaries.
Alternative notations for $â¦‡ B â¦ˆ$ are $\text{!`}_B$, or $â¦‡0 â†’ Bâ¦ˆ$ to make the
dependency on 0 explicit.

#+BEGIN_EXPORT latex
\begineqns

\eqn{initial-Char}{ f : 0 â†’ B \equivS f = â¦‡ B â¦ˆ }

\eqn{initial-Self}{ â¦‡ B â¦ˆ : 0 â†’ B }

\eqn{initial-Id}{ \Idâ‚€ = â¦‡ 0 â¦ˆ }

\eqn{initial-Uniq}{ f, g : 0 â†’ B \impliesS f = g }

% Pre-absorption
\eqn{initial-Fusion}{ f : B â†’ C \impliesS f âˆ˜ â¦‡ B â¦ˆ = â¦‡ C â¦ˆ }

\vspace{2ex}
{\tiny Provided objects $B, C$ are both in ğ’œ and â„¬,
which are full subcategories of some category ğ’:}
\eqn{initial-Compose}{ â¦‡ C â† B â¦ˆ_â„¬ âˆ˜ â¦‡B â† Aâ¦ˆ_ğ’œ = â¦‡ C â† Aâ¦ˆ_ğ’œ }
%
% Recall: ğ’Ÿ is a full-subcategory of ğ’ means: ğ’Ÿ(x,y) = ğ’(x,y) for x,y : Obj ğ’Ÿ.

\vspace{2ex}
{\tiny Provided ğ’Ÿ is built on top of ğ’; i.e., ğ’Ÿ-objects are composite entities in ğ’: }
\eqn{initial-Type}{ B \text{ is an object in ğ’Ÿ } \impliesS â¦‡ B â¦ˆ \text{ is a morphism in ğ’} }

\endeqns
#+END_EXPORT

\vspace{1em}

These laws become much more interesting when the category is built upon another
one and the typing is expressed as one or more equations in the underlying
category. In particular the importance of fusion laws cannot be over-emphasised;
it is proven by a strengthening step of the form
$f âˆ˜ â¦‡Bâ¦ˆ : 0 â†’ C \providedS â¦‡Bâ¦ˆ : 0 â†’ B \lands f : B â†’ C$.

\room

For example, it can be seen that the datatype of sequences is `the' initial object
in a suitable category, and the mediator $â¦‡-â¦ˆ$ captures
â€œdefinitions by induction on the structureâ€! Hence induction arguments
can be replaced by initiality arguments! Woah!

** COMMENT Proving Initiality :moved_to_skolemisation_section:
\room

*Proving Initiality* One may prove that an object $0$ is initial by providing
a definition for $â¦‡-â¦ˆ$ and establishing initial-Char. Almost every such
proof has the following format, or a circular implication thereof: For arbitrary /f/ and /B/,

\vspace{2em}
#+begin_calculation latex
    f : A â†’ B
\step[â‰¡]{}
    â‹®
\step[â‰¡]{}
    f = \text{â€œan expression not involving $f$â€}
\step[â‰¡]{ ğ’¹â„¯ğ’»ğ’¾ğ“ƒâ„¯ $â¦‡ B â¦ˆ$ to be the right side of the previous equation }
    f = â¦‡ B â¦ˆ
#+end_calculation

\vspace{-2em}

# With the characterisation and init-Identity rules, one can show that initial objects are unique
# up to isomorphism and so I've opted out to naming a representative `0`. Dually, for final objects.
# %
# % See page 38, par 2.22 of Fokkinga's gentle introduction for a nice calculational proof.
#

\newpage

** COMMENT full, luff, subcategory definitions

 \NTHM{}{Subcategory}{ Subcollection of objects and a subcollection of morphisms
   that contains the identities for them and is closed under composition.
 }

 \NTHM{}{Full subcategory}{ Each homset of the morphism subcollection is the whole homset of the parent category.
 That is, it is generated by the objects subcollection.
 }

 \NTHM{}{lluf subcategory}{ The object subcollection contains all objects of the parent category. }

* Colimits

Each colimit is a certain initial object, and each initial object is a certain colimit.

# Colimits are precisely initiality, and generalise sums, coequalisers, and pushouts.

+ A /diagram in ğ’/ is a functor $D : ğ’Ÿ â†’ ğ’$.

    # Let $D : \mathcal{D} \to \mathcal{A}$ be a functor
    #  ---in-fact it suffices to consider this as a graph homomorphism
    #  reifying the nodes and vertices of $\mathcal{D}$ as objects
    #  and morphisms of $\mathcal{A}$.
    #

    #  When giving $D$ and $\mathcal{D}$ via a diagram, as is done below in some
    #  example constructions, then $\mathcal{D}$ is just a subcategory and so
    #  $D$ is just the inclusion functor :-)
    #

+ Recall \nameref{constant-combinator} yielding a functor on objects ---$\const{C}\, x = C$ for objects $x$ and
  $\const{C}\, f = \Id_C$ for morphisms /f/--- and a natural transformation on arrows
  ---$\const{g} = x \mapsto g : \const{A} â†’Ì£ \const{B}$ for morphism $g : A â†’ B$.

+ The category $â‹D$, built upon ğ’, has objects $Î³ : D â†’Ì£ \const{C}$ called â€œco-conesâ€, for
  some object $C =: \tgt\, Î³$, and a morphism from $Î³$ to $Î´$ is a ğ’-morphism $x$ such that $\const{x} âˆ˜ Î³ = Î´$.

 /`Cones' sit upright on their base, $D$, on a table; `CoCones' sit upright on a co-table!/

+ A /colimit for D/ is an initial object in $â‹ D$; which may or may not exist.

\room

Writing $-â•±Î³$ for $â¦‡-â¦ˆ$ and working out the definition of co-cone in terms of equations in ğ’,
  we obtain: /$Î³ : Obj(â‹D)$ is a colimit for $D$/ if there is a mapping $-â•±Î³$ such that /â•±-Type/ and
/â•±-Char/ hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{Colimit-Type}{ Î´ \text{ cocone for } D \impliesS Î´â•±Î³ : \tgt\, Î³ â†’ \tgt\,Î´}

\vspace{2ex}
Well-formedness convention: In each law the variables are quantified
in such a way that the premise of $\slash$-Type is met.
The notation $Î´â•±â‹¯$ is only senseful if Î´ is a co-cone for $D$,
like in arithmetic where the notation $m \div n$ is only sensful if $n$ differs from 0.

\eqn{Colimit-Char}{ \const{x} âˆ˜ Î³ = Î´ \equivS x = Î´ â•± Î³ }

\vspace{2ex}
The fraction notation better suggests the \emph{calculational} properties.

% # Maybe call the colimiting object $\bigsqcup D$ ? Since it is similar to the `join'
% # and a cocone is a `proof' that an object is an upper bound?

\vspace{2ex}
Notice that for given $x : C â†’ C'$ the equation $Î´ â•± Î³ = x$ \underline{defines}
Î´, since by $\slash$-Char that one equation equivales the family of equations
$Î´_A = x âˆ˜ Î³_A$. This allows us to define a natural transformation ---or `eithers' in
the case of sums--- using a single function \emph{having} the type of the mediating arrow.

\eqn{Colimit-Self ; Cancellation}{ \const{Î´â•±Î³} âˆ˜ Î³ = Î´}

\eqn{Colimit-Id}{Î³â•±Î³ = \Id}

\eqn{Colimit-Fusion}{x âˆ˜ Î´â•±Î³ = (\const{x} âˆ˜ Î´) â•± Î³}
% Direct Proof:
%
%    Î³â•²Î´ ï¹” x = Î³ â•² (Î´ ï¹” _x_)
% â‰¡  Î³ ï¹” \const{Î³â•²Î´ ï¹” x} = Î´ ï¹” _x_          â•²-char
% â‰¡  Î³ ï¹” \const{Î³â•²Î´} ï¹” _x_ = Î´ ï¹” _x_          const functor
% â‰¡  Î´ ï¹” _x_ = Î´ ï¹” _x_                         â•²-self
% â‰¡ true                                        =-reflexivity

\eqn{Colimit-Unique ; JointlyEpic}{\const{x} âˆ˜ Î³ = \const{y} âˆ˜ Î³ \impliesS x = y}

% \vspace{2ex}
% This expresses that colimits Î³ have an epic-like property: \\
% The component morphisms $Î³_A$ are \emph{jointly epic}.

\vspace{2ex}
The following law confirms the choice of notation once more
---just as the above fusion law is nearly mutliplication into the numerator.

\eqn{Colimit-Compose}{Îµâ•±Î´ âˆ˜ Î´â•±Î³  = Îµâ•±Î³}

% # ? \alert{where} $\delta$ and $\epsilon$ are $D$ cocones
% # ? and anything appearing to the left? of $\under$ being a colimit!

\vspace{2ex}
The next law tells us that functors distribute over the â•±-notation
provided the implicit well-formedness condition that
$FÎ³$ is a colimit holds ---clearly this condition is valid when $F$
preserves colimits.

\eqn{Colimit-Functor-Dist}{F(Î´â•±Î³) = FÎ´ â•± FÎ³}

\eqn{Colimit-Pre-Functor-Elim}{Î´Fâ•±Î³F = Î´â•±Î³}

% Recall $\eta F$ be the natural transformation $x \mapsto \eta_{F\, x}$.
%  Let $\gamma$ and $\gamma F$ be colimits for the same diagram, then

\endeqns
#+END_EXPORT




\room
\room

Let $()_x : \varnothing â†’Ì£ \K{x}$ be the natural transformation from the
empty functor $\varnothing : \mathbf{0} \to ğ’$ to the constant functor.
\vspace{-0.8em}
#+LaTeX:  \eqn{Initiality as colimit}{â¦‡Bâ¦ˆ \eqs ()_B â•± ()_0}

# Needless to say, my favourite:
Cocones under $D$ correspond one-to-one with arrows from its colimit:
\vspace{-0.8em}
#+LaTeX:  \eqn{Colimit as Adjunction}{(D â†’Ì£ \K C) \;\cong\; (\mathsf{CoLim}\,D \to C) \!\qquad \text{ where } \mathsf{CoLim}\,D = \tgt Î³ }
* Limits

Dually, the category $â‹€D$ has objects being â€œconesâ€ $Î³ : \const{C} â†’Ì£ D$ where $C =: \src\, Î³$
is a ğ’-object, and a morphism to $Î³$ /from/ $Î´$ is a ğ’-morphism $x$ such that $Î³ âˆ˜ \const{x} = Î´$.
In terms of ğ’, /$Î³ : Obj(â‹€ D)$ is a limit for $D$/ if there is a mapping $Î³â•²-$ such that
the following â•²-Type and â•²-Char hold, from which we obtain a host of corollaries.
As usual, there is the implicit well-formedness condition.
# Theorem â•²-Unique expresses that limits Î³ have an monic-like property:
# The component morphisms $Î³_A$ are \emph{jointly monic}.
#
# Well-formedness convention: In each law the variables are quantified
# in such a way that the premise of $/$-Type is met.
# The notation $Î´â•±â‹¯$ is only senseful if Î´ is a cone for $D$,
# like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

#+BEGIN_EXPORT latex
\begineqns

\eqn{Limit-Type}{ Î´ \text{ cone for } D \impliesS Î³ â•² Î´ : \src\, Î´ â†’ \src\,Î³}

\eqn{Limit-Char}{ Î³ âˆ˜ \const{x}  = Î´ \equivS x = Î³ â•² Î´ }

\eqn{Limit-Self}{ Î³ âˆ˜ \const{Î³ â•² Î´}  = Î´}

\eqn{Limit-Id}{Î³â•²Î³ = \Id}

\eqn{Limit-Fusion}{Î³â•²Î´  âˆ˜ x = Î³ â•² (Î´ âˆ˜ \const{x})}

\eqn{Limit-Unique ; JointlyMonic}{Î³ âˆ˜ \const{x} = Î³ âˆ˜ \const{y} \impliesS x = y}

\eqn{Limit-Functor-Dist}{F(Î³ â•² Î´) = FÎ³ â•² FÎ´}

\eqn{Limit-Pre-Functor-Elim}{Î³F â•² Î´F = Î³â•²Î´}
% Direct proof:
%
%    Î´Fâ•±Î³F = Î´â•±Î³
% â‰¡  Î´F = (Î´ â•± Î³) ï¹” Î³F
% â‰¡  (Î´ â•± Î³ ï¹” Î³)F = (Î´ â•± Î³) ï¹” Î³F
% â‰¡  (Î´ â•± Î³ ï¹” Î³)FA = ((Î´ â•± Î³) ï¹” Î³F)A
% â‰¡  Î´ â•± Î³ ; Î³FA  = Î´â•±Î³ ï¹” Î³FA
% â‰¡  true
%
\endeqns
#+END_EXPORT

\vspace{-1em}

\newpage
* Sums
Take $D$ and $ğ’Ÿ$ as suggested by $Dğ’Ÿ = \left( \overset{A}{â€¢} \;\;\; \overset{B}{â€¢} \right)$.
Then a cocone Î´ for $D$ is a two-member family $Î´ = (f, g)$ with
$f : A â†’ C$ and $g : B â†’ C$, where $C = \tgt\, Î´$.

\room

Let $Î³=(\inl, \inr)$ be a colimit for $D$, let $A + B = \tgt\,Î³$, and write
$[f, g]$ in-place of $Î³â•²(f, g)$, then the â•²-laws yield:
/$(\inl, \inr, A+B)$ form a sum of $A$ and $B$/ if there is a mapping $[-,-]$
such that \ref{[]-Type} and \ref{[]-Char} hold.

#  Take $D$ and $\mathcal{D}$ as suggested by $D\,\mathcal{D} = (\spot\!\!\!\!^A \; \spot\!\!\!\!^B)$, for given
#  objects $A = \src f$ and $B = \src g$, with colimit $\gamma = (\mathsf{inl}, \mathsf{inr})$.
# #+LaTeX:  \eqn{Sum as colimit, â•±-[]}{[f,g] \eqs (\mathsf{inl}, \mathsf{inr}) \under (f,g)}

#+BEGIN_EXPORT latex
\begineqns

\eqn{[]-Type}{f : A â†’ C \lands g : B â†’ C \impliesS [f, g] : A + B â†’ C}

\eqn{[]-Char}{ x âˆ˜ \inl = f \lands x âˆ˜ \inr = g \equivS x = [f, g] }

\eqn{[]-Cancellation; []-Self}{ [f, g] âˆ˜ \inl = f \landS [f, g] âˆ˜ \inr = g}

\eqn{[]-Id}{ [\inl, \inr] = \Id}

\eqn{[]-Unique}{ x âˆ˜ \inl = y âˆ˜ \inl \lands x âˆ˜ \inr = y âˆ˜ \inr \impliesS x = y}

\eqn{[]-Fusion}{ x âˆ˜ [f , g] = [x âˆ˜ f, x âˆ˜ g] }

\vspace{2ex}
The implicit well-formedness condition in the next law is that
$(F\,\inl, F\,\inr, F(A+B))$ form a sum of $F\,A$ and $F\, B$.
%  ---which clearly holds if $F$ preserves sums.

\eqn{[]-Functor-Dist}{F \, [f, g]_ğ’ = [F \, f , F \, g]_ğ’Ÿ \qquad\text{ where } F : ğ’ â†’ ğ’Ÿ}
%
% f : A â†’ C
% g : B â†’ C
% F[f,g]     : F(A + B) â†’ F C
% [F f, F g] : F A + F B â†’ F C
%
% Types are okay since, by assumption, F(A + B) forms the sum of FA and FB,
% That is, (F A + F B) â‰” F(A + B)
%
% Direct proof:
%
%    F[f,g] = [Ff, Fg]
% â‰¡  F[f,g] ï¹” F inl = F f  âˆ§ F[f,g] ï¹” F inr = F g
% â‰¡  F([f,g] ï¹” inl) = F f  âˆ§ F([f,g] ï¹” inr) = F g
% â‡ [f,g] ï¹” inl = f  âˆ§ [f,g] ï¹” inr = g
% â‰¡  [f,g] = [f,g]
% â‰¡  true

\endeqns
#+END_EXPORT

\room
\room

In the pointwise setting, notice that the cancellation law serves to define the casing construct [-,-].
Then that casing is a form of conditional can be read from the characterisation.
With this view, fusion, post-distributivity of composition over casing is just
the usual law that function application distributes over conditionals
and the casing extensionality law is the body-idempotency of conditionals.

\room

For categories in which sums exist, we define for $f : A â†’ B$ and $g : C â†’ D$,

\begineqns

\eqn{+-Definition}{ f + g = [ \inl âˆ˜ f, \inr âˆ˜ g] : A + C â†’ B + D}

\eqn{Injections-Naturality}{ (f + g) âˆ˜ \inl = f âˆ˜ \inl \landS (f + g) âˆ˜ \inr = \inr âˆ˜ g }

\eqn{Extensionality}{ [h âˆ˜ \inl , h âˆ˜ \inr] = h}

\eqn{Absorption}{ [h, j] âˆ˜ (f + g) = [h âˆ˜ f, j âˆ˜ g] }

\eqn{+-BiFunctoriality}{ \Id + \Id = \Id \landS (f + g) âˆ˜ (h + j) = (f âˆ˜ h) + (g âˆ˜ j)}

\eqn{Structural Equality}{ [f,g] = [h, j] \equivS f = h \lands g = j }

\endeqns

\newpage

* Products

#+BEGIN_EXPORT latex
\def\fst{\mathsf{fst}}
\def\snd{\mathsf{snd}}
#+END_EXPORT

Take $D$ and $ğ’Ÿ$ as suggested by $Dğ’Ÿ = \left( \overset{A}{â€¢} \;\;\; \overset{B}{â€¢} \right)$.
Then a cone Î´ for $D$ is a two-member family $Î´ = (f, g)$ with
$f : C â†’ A$ and $g : C â†’ B$, where $C = \tgt\, Î´$.

\room

Let $Î³=(\fst, \snd)$ be a limit for $D$, let $A + B = \tgt\,Î³$, and write
$âŸ¨f, gâŸ©$ in-place of $(f, g)â•±Î³$, then the â•±-laws yield:
/$(\fst, \snd, A Ã— B)$ form a product of A and B/ if there is an operation
$âŸ¨-,-âŸ©$ satisfying the Char and Type laws below; from which we obtain a host of corollaries.

#+BEGIN_EXPORT latex
\begineqns

\eqn{Pair-Type}{f : C â†’ A \lands g : C â†’ B \impliesS âŸ¨f, gâŸ© : C â†’ A Ã— B}

\eqn{Pair-Char}{ \fst âˆ˜ x = f \lands \snd âˆ˜ x = g \equivS x = âŸ¨f, gâŸ© }

\eqn{Pair-Cancellation; Pair-Self}{ \fst âˆ˜ âŸ¨f, gâŸ© = f \landS \snd âˆ˜ âŸ¨f, gâŸ© = g}

\eqn{Pair-Id}{ âŸ¨\fst, \sndâŸ© = \Id}

\eqn{Pair-Unique}{ \fst âˆ˜ x = \fst âˆ˜ y  \lands \snd âˆ˜ x = \snd âˆ˜ y \impliesS x = y}

\eqn{Pair-Fusion}{ âŸ¨f , gâŸ© âˆ˜ x = âŸ¨f âˆ˜ x , g âˆ˜ xâŸ© }

\eqn{Pair-Functor-Dist}{F \, âŸ¨f, gâŸ©_ğ’ = âŸ¨F \, f , F \, gâŸ©_ğ’Ÿ \qquad\text{ where } F : ğ’ â†’ ğ’Ÿ}

\endeqns
#+END_EXPORT

\room
\room

The characterisation says that the essential properties of ordered pairs
is that their components are retrievable and they are
completely determined by their components.

\room

Notice that the cancellation rule is essentially the /definitions of projections/ in the pointwise setting;
likewise absorption is akin to the pointwise definition of the product bi-map.

\room

The fusion laws give us a pointfree rendition of their usual pointwise
definitions: All applications have been lifted to compositions!
# Likewise for the absorption laws.

# These are essentially a re-write of the sum laws.

\room

For categories in which products exist, we define for $f : A â†’ B$ and $g : C â†’ D$,

\begineqns

\eqn{x-Definition}{ f Ã— g = âŸ¨ f âˆ˜ \fst, g âˆ˜ \snd âŸ© : A Ã— C â†’ B Ã— D}

\eqn{Projections-Naturality}{ \fst âˆ˜ (f Ã— g) = f âˆ˜ \fst \landS \snd âˆ˜ (f Ã— g) = g âˆ˜ \snd }

\eqn{Extensionality}{ âŸ¨\fst âˆ˜ h, \snd âˆ˜ hâŸ© = h}

\eqn{Absorption}{ (f Ã— g) âˆ˜ âŸ¨h, jâŸ© = âŸ¨f âˆ˜ h, g âˆ˜ jâŸ© }

\eqn{x-BiFunctoriality}{ \Id Ã— \Id = \Id \landS (f Ã— g) âˆ˜ (h Ã— j) = (f âˆ˜ h) Ã— (g âˆ˜ j)}

\eqn{Structural Equality}{ âŸ¨f,gâŸ© = âŸ¨h, jâŸ© \equivS f = h \lands g = j }

\endeqns

# \newpage
* Finitary Sums and Products

All properties studied for binary /splits/ and binary /eithers/ extend to the
finitary case. For the particular situation $n = 1$, we will have $âŸ¨fâŸ©=[f]=f$
and $\inl = \fst = \Id$, of course.

\room

For the particular situation $n = 0$,
finitary products â€œdegenerateâ€ to terminal object 1 and finitary sums â€œdegenerateâ€ to initial object 0.
The standard notation for the empty split $âŸ¨âŸ©$ is $!_C$, where $C$ is the source.
Dually, the standard notation for the empty either $[]$ is $?_C$.

# Exercise 2.30: Particularise the exchange law to empty products and empty coproducts; i.e., 1 and 0!

\eqn{Empty Exchange Rule}{ âŸ¨âŸ©_0 = []_1 }

# \newpage

* Mixing products and coproducts

Any $f : A + B â†’ C Ã— D$ can be expressed alternatively as an /either/
or as a /split/. It turns out that both formats are identical:
# The Interchange rule.
\vspace{-1.5em}
\eqn{Exchange Rule}{ âŸ¨[f,g], [h,j]âŸ© = [âŸ¨f,hâŸ©,âŸ¨g,jâŸ©] }

E.g., $\mathsf{undistr}  = âŸ¨[\fst, \fst], \snd + \sndâŸ© = [\Id Ã— \inl, \Id Ã— \inr] : (A Ã— B) + (A Ã— C) â†’ A Ã— (B + C)$.

# Notice that this law above is self-dual.

# Also, by the exchange rule,
\begineqns

\eqn{Cool-Property}{ [f Ã— g, h Ã— k] \;=\; âŸ¨ [f, h] âˆ˜ (\fst + \fst), [g, k] âˆ˜ (\snd + \snd)âŸ© }

\eqn{Co-cool-Property}{ âŸ¨f + g, h + kâŸ© \;=\; [ (\inl Ã— \inl) âˆ˜ âŸ¨f, hâŸ©, (\inr Ã— \inr) âˆ˜ âŸ¨g, kâŸ©] }
# Direct proof:
#
#   [ âŸ¨f, hâŸ© Í¾ (\inl Ã— \inl), âŸ¨g, kâŸ© Í¾ (\inr Ã— \inr)]
# = [ âŸ¨inl âˆ˜ f , inl âˆ˜ hâŸ©, âŸ¨inr âˆ˜ g, inr âˆ˜ kâŸ© ]      absorption
# = âŸ¨ [inl âˆ˜ f, inr âˆ˜ g], [inl âˆ˜ h, inr âˆ˜ k]âŸ©       exchange
# = âŸ¨ f + g, h + k âŸ©                               defns
\endeqns

\room

Also, since constants ignore their inputs,
#+LaTeX: \eqn{Exchange-with-constant}{ [âŸ¨ f, \const{k} âŸ©, âŸ¨ g , \const{k} âŸ©] = âŸ¨ [f, g], \const{k} âŸ© }
# Proof:
#
#   [âŸ¨ f, _k_ âŸ©, âŸ¨ g , _k_ âŸ©]
# = âŸ¨ [f, g], [ _k_ , _k_ ]âŸ©        exchange
# = âŸ¨ [f, g], _k_ âˆ˜ [ Id, Id ]âŸ©   fusion
# = âŸ¨ [f, g], _k_ âŸ©               constants

* COMMENT space vfill                                                        :ignore:
    \vfill
* References

#+LaTeX: {\color{white}.}

[[https://maartenfokkinga.github.io/utwente/mmf92b.pdf][A Gentle Introduction to Category Theory â”€ the calculational approach]]
\newline
by [[https://maartenfokkinga.github.io/utwente/][Maarten Fokkinga]]

\vspace{1em}

An excellent introduction to category theory with examples motivated from programming, in-particular
working with sequences. All steps are shown in a calculational style ---which Fokkinga
has made [[https://ctan.org/tex-archive/macros/latex/contrib/calculation][available]] for use with LaTeX--- thereby making it suitable for self-study.

\vspace{1em}

Clear, concise, and an illuminating read.

\vspace{1em}

I've deviated from his exposition by using backwards composition `âˆ˜' rather
than diagrammatic composition `;', as such my limit notation is his colimit notation! Be careful.

\vspace{1em}

I've also consulted the delightful read [[http://www4.di.uminho.pt/~jno/ps/pdbc_part.pdf][Program Design by Calculation]] of [[http://www4.di.uminho.pt/~jno/][JosÃ© Oliveira]].

\vspace{0.5em}

Very accessible for anyone who wants an introduction to functional programming!
The category theory is mostly implicit, but presented elegantly!

\vspace{-0.5em}

:More:
+ [ ] [[https://link.springer.com/content/pdf/10.1007%2F978-3-642-32202-0_2.pdf][Generic Programming with Adjunctions]]
+ [ ] [[http://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/acmmpc-calcfp.pdf][Calculating Functional Programs]]
      - [[https://www.sciencedirect.com/science/article/pii/S157106610480906X?via%3Dihub][When is a function a fold or an unfold?]]
      - [[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.1735][The Under-Appreciated Unfold]]

+ [ ] [[https://www.researchgate.net/profile/Lambert_Meertens/publication/267305882_Category_Theory_for_Program_Construction_by_Calculation/links/54dbd91f0cf2a7769d9403a6/Category-Theory-for-Program-Construction-by-Calculation.pdf][Category Theory for Program Construction by Calculation]]
+ [ ] [[http://maartenfokkinga.github.io/utwente/mmfphd.pdf][Law and Order in Algorithmics]]
+ [ ] [[http://maartenfokkinga.github.io/utwente/mmf90.pdf][Homo- and Catamorphisms, Reductions and Maps: An Overview]]
+ [ ] [[http://maartenfokkinga.github.io/utwente/#detail_0000003538][Monadic Maps and Folds for Arbitrary Datatypes]]
+ [ ] [[http://www.cs.nott.ac.uk/~pszgmh/bananas.pdf][Bananas in Space: Extending Fold and Unfold to Exponential Types]]
+ [ ] [[http://maartenfokkinga.github.io/utwente/mmf90j.pdf][Adjunctions formulated as cata/anamorphisms]]
+ [X] [[http://www.cs.ox.ac.uk/jeremy.gibbons/publications/reladj.pdf][Relational Algebra by Way of Adjunctions]]
+ [X] [[http://www.comlab.ox.ac.uk/jeremy.gibbons/publications/hodgp-journal.pdf][Design Patterns as Higherâˆ’Order Datatypeâˆ’Generic Programs]]
+ [ ] [[http://www.cs.ox.ac.uk/jeremy.gibbons/publications/proyo.pdf][What You Needa Know about Yoneda]]
      - [[http://www.cs.ox.ac.uk/people/jeremy.gibbons/publications/poptics.pdf][Profunctor Optics]]

[[http://www.cs.ox.ac.uk/people/nicolas.wu/papers/Hylomorphisms.pdf][Conjugate Hylomorphisms, Or: The Mother of All Structured Recursion Schemes]]

[[http://www.cs.ox.ac.uk/jeremy.gibbons/publications/progorn.pf][Programming with Ornaments]] ---how dependent types show binomial heaps as fancy binary numbers?
  - [[http://www.cs.ox.ac.uk/people/hsiang-shang.ko/pcOrn/pcOrn.pdf][Modularising Inductive Families]]

:End:

** COMMENT Further Reads

 + Roland Backhouse
 + Grant Malcolm
 + Lambert Meertens
 + Jaap van der Woude

 + /Adjunctions/ by Fokkinga and Meertens

 + Backus' FP and Iverson's APL are examples of pointfree programming
   languages; look them up ;-)

* To Read

  + [[http://www.cwru.edu/artsci/math/wells/pub/ttt.html][Toposes, Triples and Theories]] by Michael Barr and Charles Wells
  + [[https://arxiv.org/pdf/1803.05316.pdf][Seven Sketches in Compositionality: An Invitation to Applied Category Theory]]
  + [[https://arxiv.org/abs/math/0502550][Frobenius algebras and ambidextrous adjunctions]] by Aaron Lauda
  + [[http://www.tac.mta.ca/tac/reprints/articles/5/tr5.pdf][Functorial Semantics of Algebraic Theories]] by F. William Lawvere
  + [[http://www.tac.mta.ca/tac/reprints/articles/10/tr10.pdf][Basic Concepts of Enriched Category Theory]] by G.M. Kelly
  + [[http://golem.ph.utexas.edu/category/2008/03/physics_topology_logic_and_com.html][Rosetta Stone]]
  + [[http://www.math.mcgill.ca/triples/Barr-Wells-ctcs.pdf][Category Theory for Computing Science -- Michael Barr and Charles Wells]]

  Monoidal:
  + [[https://arxiv.org/pdf/math/0507349.pdf][Elementary remarks on units in monoidal categories]]
  + [[http://math.uchicago.edu/~may/TQFT/Boyarchenko%20on%20associativity.pdf][ASSOCIATIVITY CONSTRAINTS IN MONOIDAL CATEGORIES]]
  + [[http://mtm.ufsc.br/~ebatista/2016-2/tensor_categories.pdf][Tensor Categories]]

* space newpage                                                      :ignore:
  \newpage
* Monoidal and Closed Categories

  #+latex: \def\eval{\mathsf{eval}}

  It is rather common that we have a notion of pairing for types for which there is a unit type.
  Examples include products with the initial object, sums with the terminal object, or for
  the category of endofunctors: Functor composition with the identity functor.

  # Monoidal categories are also called â€œtensor categoriesâ€

  \room

  A /monoidal category/ /(ğ’, âŠ—, I, Î±, Î», Ï)/ consists of a category ğ’ with bifunctor
  \newline
  $\_{}âŠ—\_{} : ğ’^2 â†’ ğ’$ and object /I : Obj ğ’/
  ---referred to as the â€˜tensor productâ€™ and â€˜tensor unit---
  and three natural isomorphisms:
  \newline
  The â€˜(right-to-left) associatorâ€™ /Î±_{A, B, C} : A âŠ— (B âŠ— C) â‰… (A âŠ— B) âŠ— C/ and
  \newline
  the â€˜unitorsâ€™  /Î»_A : I âŠ— A â‰… A/ and /Ï_A : A âŠ— I â‰… A/ such that:

  0. The order of re-parensization, outer-most or inner-most first, does not matter; i.e.,
     the two obvious maps witnessing $A âŠ— (B âŠ— (C âŠ— D)) â†’ ((A âŠ— B) âŠ— C) âŠ— D$ are identical:
     $Î±_{A âŠ— B, C, D} \;âˆ˜\; Î±_{A, B, C âŠ— D} \eqs Î±_{A, B, C} âŠ— \Id_D \;âˆ˜\; Î±_{A, B âŠ— C, D}\;âˆ˜\; \Id_A âŠ— Î±_{B, C, D}$.

  1. Unit elimination paths are the same even if unnecessary associtivity is performed; i.e.,
     the two obvious maps witnessing $A âŠ— (I âŠ— B) â†’ A âŠ— B$ are identical:
     \newline
     $\Id_A âŠ— Î»_B \eqs (Ï_A âŠ— \Id_B) \;âˆ˜\; Î±_{A, I, B}$.

  Mnemonic: Î», â€˜Lâ€™ambda, is for â€˜Lâ€™eft unitor; Ï, â€˜Râ€™ho, is for â€˜Râ€™ight unitor.

  \room

  Unfolding some of that up yields:
  + $\Id âŠ• \Id = \Id$ and $(f âˆ˜ g) âŠ— (h âˆ˜ k) = (f âŠ— h) âˆ˜ (g âŠ— k)$
  + $Î± âˆ˜ (f âŠ— (g âŠ— h)) = ((f âŠ— g) âŠ— h) âˆ˜ Î±$
  + $Î» âˆ˜ (\Id âŠ— f) = f âˆ˜ Î»$
  + $Ï âˆ˜ (f âŠ— \Id) = f âˆ˜ Ï$

  *Mac Lane's coherence theorem:* Any well-typed diagram built from âŠ—, Î±, Î», Ï commutes.

  :TO_PROVE:
  + Iteration Laws ? :: $Î»_{X âŠ— Y} = (Î»_X âŠ— \Id_Y) âˆ˜ Î±$ and $Ï_{X âŠ— Y} = \Id_X âŠ— Ï_Y âˆ˜ Î±Ë˜$

    Proof

     Let's show the first one only.

          Id âŠ— Î»_{X âŠ— Y}
       =  Ï âŠ— Id_{X âŠ— Y} âˆ˜ Î±
       =  Ï âŠ— (Id_X âŠ— Id_Y) âˆ˜ Î±
       =  Î±Ë˜ âˆ˜ ((Ï âŠ— Id_X) âŠ— Id_Y) âˆ˜ Î± âˆ˜ Î±
       =  Î±Ë˜ âˆ˜ (((Id âŠ— Î»_X) âˆ˜ Î±Ë˜) âŠ— Id_Y) âˆ˜ Î± âˆ˜ Î±
       =  Î±Ë˜ âˆ˜ (Id âŠ— Î»_X) âŠ— Î±Ë˜ âˆ˜ Î± âˆ˜ Î±
       =  Î±Ë˜ âˆ˜ Î± âˆ˜ Id âŠ— (Î»_X âŠ— Î±Ë˜) âˆ˜ Î±Ë˜ âˆ˜ Î± âˆ˜ Î±
       =  Id âŠ— (Î»_X âŠ— Î±Ë˜) âˆ˜ Î±

       MA: Shuchks! ???

       Î»_{X âŠ— Y} = Î»_X âŠ— Id_Y
     â‰¡{ unit-equivalence-left, see below }
       Id âŠ— Î»_{X âŠ— Y} = Id âŠ— (Î»_X âŠ— Id_Y)
     â‰¡{ unitor law }
       (Ï âŠ— Id_{X âŠ— Y}) âˆ˜ Î± = Id âŠ— (Î»_X âŠ— Id_Y)
     â‰¡{ BiFunctoriality }
       (Ï âŠ— (Id_X âŠ— Id_Y)) âˆ˜ Î± = Id âŠ— (Î»_X âŠ— Id_Y)
     â‰¡{ Î± naturality; Î± invertible }
       Î±Ë˜ âˆ˜ ((Ï âŠ— Id_X) âŠ— Id_Y) âˆ˜ Î± âˆ˜ Î± = Id âŠ— (Î»_X âŠ— Id_Y)
     â‰¡{ unitors; Î± invertible }
       Î±Ë˜ âˆ˜ (((Id âŠ— Î»_X) âˆ˜ Î±Ë˜) âŠ— Id_Y) âˆ˜ Î± âˆ˜ Î± = Id âŠ— (Î»_X âŠ— Id_Y)
     â‰¡{ unitors; Î± invertible }
       Î±Ë˜ âˆ˜ (((Id âŠ— Î»_X) âˆ˜ Î±Ë˜) âŠ— Id_Y) âˆ˜ Î± âˆ˜ Î± = Id âŠ— (Î»_X âŠ— Id_Y)


     Î»_{X âŠ— Y} : I âŠ— (X âŠ— Y) â†’ X âŠ— Y
     (Î»_X âŠ— Id_Y) âˆ˜ Î±Ë˜

  + Agreement Law :: $Î»_I = Ï_I$
    Proof

      Since -âŠ— Id is an equivalence,

         Î»_I âŠ— Id_I
      =  Î»_{I âŠ— I} âˆ˜ Î±Ë˜     ;; iteration law
      =  (Id_I âŠ— Î»_I) âˆ˜ Î±Ë˜  ;; Î» naturality with f := Î», then eliminate Î» since it's invertible
      =  (Ï_I âŠ— Id_I) âˆ˜ Î± âˆ˜ Î±Ë˜ ;; unitors
      =   Ï_I âŠ— Id_I

      Thus, Î»_I = Ï_I.

  :END:

  :Theorems:

  1. Id âŠ— Î» âˆ˜ Î±Ë˜ = Ï âŠ— Id   ---duh, from unitors
  2. Î»_I = Ï_I

     Proof:   Î» = Ï â‡” Id âŠ— Î» = Id âŠ— Ï â‡” p âŠ— Id âˆ˜ Î± = Id âŠ— Ï â‡”{ Ï invertible} p âˆ˜ p âŠ— Id âˆ˜ Î± = p âˆ˜ Id âŠ— Ï â‡” Ï âˆ˜ Ï âˆ˜ Î± = Ï âˆ˜ Ï â‡” Î± = Id ???
  3. Î» âˆ˜ Î±Ë˜ = Î» âŠ— Id
  4. Id âŠ— Ï = Ï âˆ˜ Î±
  5. Î»_{I âŠ— X} = Id âŠ— Î»_X

     Proof: Î» âˆ˜ Id âŠ— Î» ={ naturality } Î» âˆ˜ Î»
            Whence, since Î» invertible, Id âŠ— Î» = Î»  :-)

  6. Ï_{I âŠ— X} = Ï_X âŠ— Id

     See (5)

  7. The unit object, I, is unique up to unique isomorphism; ie if Iâ€², Î»â€², Ïâ€² is another unit object then I â‰… Iâ€².

  :End:
  :More:
  Moreover, the two obvious maps (I âŠ— A) âŠ— B â†’ A âŠ— B, namely Î» âŠ— Id and Î» âˆ˜ Î±Ë˜,
  can be shown to coincide.

  Why?

     Î» âŠ— Id = Î» âˆ˜ Î±Ë˜
  â‰¡  Î» âŠ— Id âˆ˜ Î± = Î»

  --

  1. Î± âˆ˜ Î± = Î± âŠ— Id âˆ˜ Î± âˆ˜ Id âŠ— Î±  â”€associtator
  2. Id âŠ— Î» = Ï âŠ— Id âˆ˜ Î±   â”€unitors
  3. (Id âŠ—) is an equivalence
  4. âˆ´ WTS: Id âŠ— (Î» âŠ— Id) = Id âŠ— (Î» âˆ˜ Î±Ë˜)
  5. BiFunctoriality of âŠ— means WTS:
     Id âŠ— (Î» âŠ— Id) = (Id âŠ— Î») âˆ˜ (Id âŠ— Î±Ë˜).
  6. Î± âˆ˜ Id âŠ— (Î» âŠ— Id) = (Id âŠ— Î») âŠ— Id âˆ˜ Î±  â”€naturality of Î± applied to Id, Î», Id
  7. Î± âˆ˜ Id âŠ— (Î» âŠ— Id) = (Ï âŠ— Id âˆ˜ Î±) âŠ— Id âˆ˜ Î±  â”€using 2 on 6
  8. Î± âˆ˜ Id âŠ— (Î» âŠ— Id) = (Ï âŠ— Id) âŠ— Id  âˆ˜  Î± âŠ— Id  âˆ˜  Î±  â”€bifunctoriality of âŠ—
  9. Id âŠ— (Î» âŠ— Id) = Î±Ë˜ âˆ˜ (Ï âŠ— Id) âŠ— Id  âˆ˜  Î± âŠ— Id  âˆ˜  Î±  â”€invertibility of Î±
  10. Id âŠ— (Î» âŠ— Id) = Ï âŠ— (Id âŠ— Id) âˆ˜ Î±Ë˜  âˆ˜  (Î± âŠ— Id)  âˆ˜  Î±  â”€naturality of Î±
  11. Id âŠ— (Î» âŠ— Id) = Ï âŠ— Id âˆ˜ Î±Ë˜  âˆ˜  (Î± âŠ— Id)  âˆ˜  Î±  â”€bifunctoriality of âŠ—
  12. Id âŠ— (Î» âŠ— Id) = (Id âŠ— Î») âŠ— Î±Ë˜ âˆ˜ Î±Ë˜  âˆ˜  (Î± âŠ— Id)  âˆ˜  Î±  â”€using (2), note Î± invertible
  13. Id âŠ— (Î» âŠ— Id) = Î±Ë˜ âˆ˜ Id âŠ— (Î» âŠ— Î±Ë˜) âˆ˜ (Î± âŠ— Id)  âˆ˜  Î±  â”€naturality of Î±
  14. Id âŠ— (Î» âŠ— Id) = Î±Ë˜ âˆ˜ (Id âˆ˜ Î±) âŠ— ((Î» âŠ— Î±Ë˜) âˆ˜ Id) âˆ˜  Î±  â”€BiFunctoriality of âŠ—
  15. Id âŠ— (Î» âŠ— Id) = Î±Ë˜ âˆ˜ (Î± âŠ— (Î» âŠ— Î±Ë˜)) âˆ˜  Î±  â”€BiFunctoriality of âŠ—
  16. Id âŠ— (Î» âŠ— Id) = (Î± âŠ— Î») âŠ— Î±Ë˜ âˆ˜ Î±Ë˜ âˆ˜ Î±  â”€naturality of Î±
  17. Id âŠ— (Î» âŠ— Id) = (Î± âŠ— Î») âŠ— Î±Ë˜  â”€inverses and units

  derp :'(

  :End:
  :Also:

  We can show that Î» = Ï : I âŠ— I â‰… I.

  1. Î± âˆ˜ Î± = Î± âŠ— Id âˆ˜ Î± âˆ˜ Id âŠ— Î±  â”€associtator
  2. Id âŠ— Î» = Ï âŠ— Id âˆ˜ Î±   â”€unitors
  3. (âŠ— Id) is an equivalence.
  4. WTS: Î» âŠ— Id = Ï âŠ— Id
  5. Î» âˆ˜ Î±Ë˜ = Î» âŠ— Id â”€from the â€œotherâ€ unit coherence
  6. (Id âŠ— Î») âˆ˜ Î±Ë˜ = Ï âŠ— Id â”€(2)
  7. WTS: Î» âˆ˜ Î±Ë˜ = (Id âŠ— Î») âŠ— Î±Ë˜
  8. WTS: Î» = (Id âŠ— Î») âŠ— Î±Ë˜ âˆ˜ Î±

     derp :'(

  :End:

  \room

  \eqn{Unit-Equivalence-Left}{ \Id âŠ— f = \Id âŠ— g  \equivS  f = g }
  \eqn{Unit-Equivalence-Right}{ f âŠ— \Id = g âŠ— \Id \equivS  f = g }
  :Unit-equiv-pf:
  Note e is the unit of Â· precisely when e Â· e = e and both (eÂ·), (Â·e) are bijective.

  As such, (I âŠ—) and (âŠ— I) are equivalences; in particular, they reflect equalities.

    Id âŠ— f = Id âŠ— g
  â‡’ Î» âˆ˜ Id âŠ— f = Î» âˆ˜ Id âŠ— g   ;; composition
  â‡’ f âˆ˜ Î» = g âˆ˜ Î»             ;; Î» Naturality
  â‡’ f = g                     ;; Î» invertible

  Whence: Id âŠ— f = Id âŠ— g  â‰¡  f = g

  :End:
  :More_attempts:

  Id âŠ— Î» = Ï âŠ— Id âˆ˜ Î±
  Ï âŠ— Id = (Id âŠ— Î») âˆ˜ Î±Ë˜

  ??

  1. Ï âŠ— Id = (Id âŠ— Î») âˆ˜ Î±Ë˜
  2. Î» âˆ˜ Î±Ë˜ = Î» âŠ— Id
  3. Id âŠ— Ï = Ï âˆ˜ Î±

  For 1:
  1) Ï âŠ— Id = Id âŠ— Î» âˆ˜ Î±Ë˜ â”€unitors
  2)

  For 2:

  1) Î± âˆ˜ Î± = Î± âŠ— Id âˆ˜ Î± âˆ˜ Id âŠ— Î±  â”€associtator
  2) Id âŠ— Î» = Ï âŠ— Id âˆ˜ Î±   â”€unitors
  3) (Id âŠ—) is an equivalence
  4)

        Î» âˆ˜ Î±Ë˜ = Î» âŠ— Id
     â‰¡  Id âŠ— (Î» âˆ˜ Î±Ë˜) = Id âŠ— (Î» âŠ— Id)
     â‰¡  Î± âˆ˜ Id âŠ— (Î» âˆ˜ Î±Ë˜) = Î± âˆ˜ Id âŠ— (Î» âŠ— Id)
     â‰¡  Î± âˆ˜ Id âŠ— (Î» âˆ˜ Î±Ë˜) = (Id âŠ— Î») âŠ— Id âˆ˜ Î±
     â‰¡  Î± âˆ˜ Id âŠ— (Î» âˆ˜ Î±Ë˜) = ((Ï âŠ— Id) âˆ˜ Î±Ë˜) âŠ— Id âˆ˜ Î±
     â‰¡  Î± âˆ˜ Id âŠ— (Î» âˆ˜ Î±Ë˜) = ((Ï âŠ— Id) âˆ˜ Î±Ë˜) âŠ— (Id âˆ˜ Id) âˆ˜ Î±
     â‰¡  Î± âˆ˜ Id âŠ— (Î» âˆ˜ Î±Ë˜) = (Ï âŠ— Id) âŠ— Id âˆ˜ (Î±Ë˜ âŠ— Id) âˆ˜ Î±



      Id âŠ— (Î» âŠ— Id)
    = Î±Ë˜ âˆ˜ ((Id âŠ— Î») âŠ— Id) âˆ˜ Î±
    = Î±Ë˜ âˆ˜ (((Ï âŠ— Id) âŠ— Id) âˆ˜ Î±) âˆ˜ Î±
    = (Î±Ë˜ âˆ˜ ((Ï âŠ— Id) âŠ— Id)) âˆ˜ Î± âˆ˜ Î±
    = ((Ï âŠ— Id) âˆ˜ Î±Ë˜) âˆ˜ Î± âˆ˜ Î±
    = (Ï âŠ— Id) âˆ˜ Î±Ë˜ âˆ˜ Î± âˆ˜ Î±
    = (Ï âŠ— Id) âˆ˜ Î±
    = ((Id âŠ— Î») âˆ˜ Î±Ë˜) âˆ˜ Î±
    = Id âŠ— Î»


     â‡”  Id âŠ— (Î» âŠ— Id) = (Id âŠ— Î») âˆ˜ (Id âŠ— Î±Ë˜)
     â‡”  Id âŠ— (Î» âŠ— Id) = Id âŠ— (Î» âŠ— Î±Ë˜)
     â‡”  Î» âŠ— Id = Î» âŠ— Î±Ë˜   (â€¢Ì€á´—â€¢Ì)Ùˆ

  :End:

  \room

  The apparent complexity of the definition of monoidal categories vanishes
  when a [[https://qchu.wordpress.com/2012/11/05/introduction-to-string-diagrams/][geometrical notation]] is used ---the coherence laws simply become
  expected geometric operations on the plane.
  The geometric interpretation is sound and complete
  ---i.e., equal morphisms yield â€˜equalâ€™ pictures, and conversely.
  # â‡’ Easy, just check each axiom.
  # â‡ Show that a picture must be constructed from one of the monoidal axioms,
  #   induct on the number of possible constructions required to produce it.
  ( [[https://www.mathstat.dal.ca/~selinger/papers/graphical-2up.pdf][A survey of graphical languages for monoidal categories]] )


 # Theorem 29. Every monoidal category is monoidally equivalent to
 # a strict monoidal category
 #
 # Proof sketch: http://events.cs.bham.ac.uk/mgs2019/vicary.pdf

  \room

  Examples
  + Common examples include preordered monoids thought of as monoidal categories.
  + Functor categories $ğ’^ğ’$ with tensor being functor composition.
  + Any category with finite co/products is monoidal using sums or products; e.g., ğ’®â„¯ğ“‰.
  + â„›â„¯ğ“ with Cartesian product is monoidal, even though this is /not/
    a categorical product.
       # the latter having relations being performed in parallel.
      # (a, c) (R Ã— S) (b, d)  â‰¡  a R b  âˆ§  c S d
  + The â€˜free strict monoidal categoryâ€™ on ğ’ has objects being
    finite lists of ğ’-objects where an arrow exists only between equal length
    lists and it is a list of ğ’-morphisms between the corresponding components;
    tensor is catenation with the empty list as the unit object.
    # + The category ğ’ğ’¶ğ“‰ is cartesian closed: The internal-hom is the functor category of functors and natural transformations.
  + Symmetric monoidal categories are closed under products, and this has a right
    adjoint yielding functor categories of sym. mon. cats.

   + (Mat(K), âŠ—, 1): The category whose objects are natural numbers and whose arrows M : m â†’ n are n Ã— m matrices taking values in field K. Composition is matrix multiplication, the monoidal
product is multiplication of natural numbers (on objects) and the [[https://en.wikipedia.org/wiki/Kronecker_product#Definition][Kronecker product]] of matrices (on arrows). This category is essentially the category of finite-dimensional vector spaces over K, with a chosen basis for all of its objects.

  \room

  Interestingly, tensor distributes over sums: ~(A + B) âŠ— C â‰… (A âŠ— C) + (B âŠ— C)~.
  :Proof:
  By Yoneda,

   (A + B) âŠ— C  â†’ X
  â‰… A + B  â†’  C â‡¨ X
  â‰… (A  â†’  C â‡¨ X) Ã— (B  â†’  C â‡¨ X)
  â‰… (A âŠ— C  â†’  X) Ã— (B âŠ— C  â†’  X)
  â‰… (A âŠ— C) + (B âŠ— C)  â†’  X

  :End:

  \room

  A *lax monoidal functor* $F : ğ’± âŸ¶ ğ’±â€²$ is a functor that sub-factors over product:
  $F \, x_0 âŠ— â‹¯ âŠ— F\, x_n âŸ¶Ì‡ F(xâ‚€ âŠ— â‹¯ âŠ— x_n)$.
  :More_detail:
  More formally, there are transformations
  $Ï• : I âŸ¶Ì‡ F\, I$ and $Ï• : F\,X âŠ— F\,Y âŸ¶Ì‡ F(X âŠ— Y)$ that are natural in $X$ and $Y$
  that are compatible with the monoidal structure:
  The obvious arrows $I âŠ— F\, X âŸ¶ F\, X$ coincide as do those for
  $F\, X âŸµ F\, x âŠ— I$, namely $Î» = F Î» âˆ˜ Ï• âˆ˜ Ï• âŠ— \Id$ and $Ï = F Ï âˆ˜ Ï• âˆ˜ \Id âŠ— Ï•$,
  and re-associating commutes with functor application in that the two ways
  $(F\, X âŠ— F\, Y) âŠ— F\, Z âŸ¶ F\, (X âŠ— (Y âŠ— Z))$ coincide, namely
  $F\, Î± âˆ˜ Ï• âˆ˜ Ï• âŠ— \Id  = Ï• âˆ˜ \Id âŠ— Ï• âˆ˜ Î±$.
  If the transformations are isomorphims, the qualifier â€˜laxâ€˜ is dropped
  and sometimes replaced by â€˜strongâ€™.
  :End:

  \room

  \room
  A *Cartesian-closed category* is a monoidal category where the tensor
  is categorical product and all exponentials exist.
  These categories are in correspondence with the models of simply typed
  lambda-calculus. If it has all finite sums as well, then it's known as
  *bicartesian closed*, in which case products necessarily distribute over sums.

  ğ’± is *semicartesian* if any of the following equivalent statements is true.
  1. Unit object $I$ is terminal; in which case one says ğ’± is *semicartesian.*
  2. It has a natural â€˜deletionâ€™ $â€¼_X : X âŸ¶ I$ with $â€¼_I = \Id_I$.
  3. It has natural â€˜projectionsâ€™ $Ï€áµ¢ : Xâ‚ âŠ— Xâ‚‚ âŸ¶ Xáµ¢$ with $Ï€â‚ : I âŠ— I â‰… I : Î»Ë˜$.
     :FurtherLaws:
     The following can then be shown to be true?

     + Ï€â‚ âˆ˜ Ï€â‚ = Ï€â‚ âˆ˜ Î±   :  (X âŠ— Y) âŠ— Z âŸ¶ X
     + Ï€â‚‚ âˆ˜ Ï€â‚‚ = Ï€â‚‚ âˆ˜ Î±Ë˜  :  X âŠ— (Y âŠ— Z) âŸ¶ Z
     + Projections involving a unit equal to Ï or Î»; e.g.,
       Ï€â‚Ë£â± = Ï.

     :End:

  # Whence, having projections and deletion maps is a property, not a structure!
  :EquivalenceProofs:
  We demonstrate the proof chain 2 â‡’ 1 â‡’ 3 â‡’ 2 to show the equivalences.

  --------------------------------------------------------------------------------
  2 â‡’ 1:

     Let's show I â‰… ğŸ™, the terminal object with !_X : X âŸ¶ ğŸ™.
     Indeed â€¼ âˆ˜ ! : I âŸ¶ I and by naturality of !!,
     this equals !!_I which is Id_I by assumption.

     Likewise, ! âˆ˜ !! ={ naturality } !_ğŸ™ ={ unicity } Id_ğŸ™.

     Whence I â‰… 1 with !Ë˜ = !!.

     Whence, !! is a unique map X âŸ¶ I for any X; indeed
     for any f,g:

        f, g : X âŸ¶ I
     â‡’  ! âˆ˜ f, ! âˆ˜ g : X âŸ¶ ğŸ™
     â‡’  ! âˆ˜ f = ! âˆ˜ g
     â‰¡  f = g , since ! is post inverse !!.

  --------------------------------------------------------------------------------
  1 â‡’ 3:

     Let Ï€â‚ : Xâ‚ âŠ— Xâ‚‚ âŸ¶ Xâ‚ := Ï âˆ˜ (Id âŠ— !)

     This is indeed natural: For any f : X âŸ¶ Y,
     Ï€â‚ âˆ˜ (f âŠ— f) = f âˆ˜ Ï€â‚ as follows.

        Ï€â‚ âˆ˜ (f âŠ— f)
     =  Ï âˆ˜ (Id âŠ— !) âˆ˜ (f âŠ— f)
     =  Ï âˆ˜ ((Id âˆ˜ f) âŠ— (! âˆ˜ f))
     =  Ï âˆ˜ (f âŠ— !)                ;; identity and naturality of !
     =  Ï âˆ˜ (f âŠ— Id)               ;; coherency of !!_I = Id_I
     =  f âˆ˜ Ï                      ;; naturality of Ï
     = f âˆ˜ Id âˆ˜ Ï                  ;; Identities
     = f âˆ˜ Ï âˆ˜ (Id âŠ— Id)           ;; naturality of Ï
     = f âˆ˜ Ï âˆ˜ (Id âŠ— !)            ;; coherency of !!_I = Id_I
     = f âˆ˜ Ï€â‚

     This proof was constructed by â€œburning the candleâ€ at both ends;
     i.e., simultaneously rewriting both sides until they met.

     Likewise Ï€â‚‚ can be formed and found to be natural.

     The coherency condition, Ï€â‚ : I âŠ— I â‰… I is proven by finding an
     inverse i as follows.

        i âˆ˜ Ï€
     ={ i : I âŸ¶ I âŠ— I, so inverse of Î»_I = Ï_I is a good candidate for i }
        ÏË˜ âˆ˜ Ï€
     ={ definition of Ï€ }
        ÏË˜ âˆ˜ Ï âˆ˜ (Id_I âŠ— !_I)
     ={ Inverses and coherency of ! }
        Id_I âŠ— Id_I
     ={ BiFunctoriality }
        Id_I

     Neato!

  --------------------------------------------------------------------------------
  3 â‡’ 2:


     Let !!_X : X âŸ¶ I :=  Ï€â‚ âˆ˜ Î»Ë˜

     This is natural:

       !! âˆ˜ f
     = Ï€â‚ âˆ˜ Î»Ë˜ âˆ˜ f
     = Ï€â‚ âˆ˜ (Id âŠ— f) âˆ˜ Î» Ë˜  ;; naturality of Î»Ë˜ is (Id âŠ— g) âˆ˜ Î» Ë˜ = Î» Ë˜ âˆ˜ g
     = Id âˆ˜ Ï€â‚ âˆ˜ Î» Ë˜        ;; naturality of Ï€â‚
     = !!                   ;; identities and definition of !!

    Moreover, it is coherent, !! = Ï€â‚ âˆ˜ Î»Ë˜ = Id, by the coherency of Ï€â‚.
  :End:

  If in addition it is symmetric with (natural involution) $Ïƒ : X âŠ— Y âŸ¶ Y âŠ— X$
  and has a natural â€˜diagonalâ€™ $Î”_X : X âŸ¶ X âŠ— X$
  such that the obvious maps $X âŸ¶ X$ coincide
  --- i.e., $Î» âˆ˜ (! âŠ— Id) âˆ˜ Î” = Id = Ï âˆ˜ (Id âŠ— !) âˆ˜ Î”$,
  â€œduplicating data, then deleting a copy, is the same as doing nothingâ€---
  then it is necessairly cartesian!

  :Cartesian_SemiProof:
  For any A and B, let us show that A âŠ— B is their product.
  That is, for any f : C âŸ¶ A and g : C âŸ¶ B
  let us construct a unique map âŸ¨f, gâŸ© : C âŸ¶ A âŠ— B
  whose projections are f and g.

  Let us try to use whatever's lying around to construct something
  of type C âŸ¶ A âŠ— B then define that to be âŸ¨f,gâŸ©.

     f : C Â­â†’ A  âˆ§  g : C âŸ¶ B
  â‡’  f âŠ— g : C âŠ— C â†’ A âŠ— B
  â‡’  f âŠ— g âˆ˜ Î” : C â†’ A âŠ— B

  âˆ´ Define âŸ¨f, gâŸ© := f âŠ— g âˆ˜ Î”

  Components are extracted by projections:

     Ï€â‚ âˆ˜ âŸ¨f , gâŸ©
  =  Ï€â‚ âˆ˜ f âŠ— g âˆ˜ Î”
  = { naturality of Ï€â‚ }
    f âˆ˜ Ï€â‚ âˆ˜ Î”
  = { claim }
    f

  Where I claim Ï€â‚ âˆ˜ Î” : X â†’ X â€œdrop after duplicationâ€ is the identity:

     Ï€â‚ âˆ˜ Î”
  = { identities }
    Id âˆ˜ Ï€â‚ âˆ˜ Î”
  = { naturality }
    Ï€â‚ âˆ˜ (Id âŠ— !) âˆ˜ Î”
  = { coherency of Î” }
    Ï€â‚ âˆ˜ ÏË˜
  = { Projections involving a unit equal to Ï or Î» }
    Id
:End:

# So, a symmetric monoidal category
# becomes a category with finite products when we can duplicate and
# discard.
#

  \room

  An /exponential for Y/ is characterised by the following adjoint isomorphism
  that is natural in /Y/ and /Z:/
  #+begin_export latex
  \eqn{Exponential-Char}{âŒŠ\_{}âŒ‹ \;:\; X âŠ— Y â†’ Z \quadâ‰…\quad X â†’ (Y â© Z) \;:\; âŒˆ\_{}âŒ‰ }
  #+end_export

  \room
  + Note that âŒŠ_âŒ‹ generalises currying, and âŒˆ_âŒ‰ generalises uncurrying.
  + The counit $\eval_Z = âŒˆ\Id_{Y â© z}âŒ‰ : (Y â© Z) âŠ— Y â†’ Z$ is called the /evaluation morphism./

#+BEGIN_EXPORT latex
\begineqns

\eqn{Exp-Internalised-Char}{ X â© (Y â© Z)  \quadâ‰…\quad  (X âŠ— Y) â© Z}

\eqn{Exp-Unit}{ I â© Z \quadâ‰…\quad Z }

% false?
% \eqn{Exp-Distributivity}{X â© (Y âŠ— Z) \quadâ‰…\quad (X â© Y) âŠ— (X â© Z)}

% false?
% \eqn{Exp-Terminality}{ Z â© I \quadâ‰…\quad I }

% Self, Char, Id, Fusion, Unique, Compose?
%
% \eqn{CoEq-Char}{ x âˆ˜ p = q \equivS x = q â•± p }
%
% \eqn{CoEq-Self}{ qâ•±p âˆ˜ p = q}
%
% \eqn{CoEq-Id}{pâ•±p = \Id}
%
% \eqn{CoEq-Fusion}{x âˆ˜ qâ•±p = (x âˆ˜ q)â•±p}
%
% \eqn{CoEq-Unique}{x âˆ˜ p = y âˆ˜ p \impliesS x = y}
%
% \eqn{CoEq-Compose}{râ•±q âˆ˜ qâ•±p = râ•±p}
%
\endeqns
#+END_EXPORT
:Exp-Internalised-Char_PROOF:
By Yoneda,

  A â†’ X â© (Y â© Z)
â‰… A âŠ— X â†’ (Y â© Z)
â‰… (A âŠ— X) âŠ— Y â†’ Z
â‰… A âŠ— (X âŠ— Y) â†’ Z
â‰… A â†’ (X âŠ— Y) â© Z
:End:
:Exp-Unit_PROOF:
By Yoneda,

  A â†’ I â© Z
â‰… A âŠ— I â†’ Z
â‰… A â†’ Z
:End:

  \room
  When exponentials always exists, one refers to $\_{}â©\_{} : ğ’ Ã— ğ’^{op} â†’ ğ’$
  as /the internal hom/ and says /(ğ’, âŠ—, I, Î±, Î», Ï, â©)/ is a *closed monoidal category*.

  \room
  In the cartesian case, the /entire/ collection of morphisms $X â†’ Y$
  is encoded by the /single/ object $X â© Y$. That is, $X â†’ Y \quadâ‰…\quad 1 â†’ (X â© Y)$ in ğ’®â„¯ğ“‰.

  \room
  # Claim:
  # The following natural transformation is natural in $X$ and $Y$, and
  # dinatural in $W$.
  #
  # Proof?

  # Claim: The second one is dinatural in X; proof?

  #+BEGIN_EXPORT latex
  \begineqns

  \eqn{Left-Internal-Yoneda}{
  âŒˆâŒˆ \eval \,âˆ˜\, \Id âŠ— \eval \,âˆ˜\, Î±^{-1}âŒ‰âŒ‰
  \;:\; (X â© Y) â†’ (W â© X) â© (W â© Y)
  }

  \eqn{Internal-Identities}{ âŒˆ Î»_X âŒ‰ \;:\; I â†’ (X â© X) }
  \endeqns
#+END_EXPORT

  \room
  More generally, a *closed category* is a category ğ’ with a bifunctor
  $\_{}â©\_{}$ and two â€˜coherentâ€™ transformations as above.

  :Left_Yoneda__Proof:

    (X â© Y)  â†’  (W â© X) â© (W â© Y)
  â‰… ((X â© Y) âŠ— (W â© X)) âŠ— W  â†’  Y

  We now compose:

    ((X â© Y) âŠ— (W â© X)) âŠ— W
  â‰…{ Î±^{-1}_{X â© Y, W â© X, W} }
    (X â© Y) âŠ— ((W â© X) âŠ— W)
  â†’{ Id âŠ— eval_W }
    (X â© Y) âŠ— X
  â†’{ eval_X }
    Y

 Whence,
 âŒˆ âŒˆ eval_X âˆ˜ \Id âŠ— eval_W âˆ˜ Î±^{-1}_{X â© Y, W â© X, W} âŒ‰_{(X â© Y) âŠ— (W â© X),W, Y}
 âŒ‰_{X â© Y, W â© X, W â© Y}
 :End:

  :Internal_Ids_Proof:

    I â†’ (X â© X)
  â‰… I âŠ— X â†’ X

  Whence,
    âŒˆ Î»_X âŒ‰ : I â†’ (X â© X)
  :End:

  \room

  It is common to notate $X â© Y, âŒŠfâŒ‹$ by $Y^X, \transpose{f}$.
#+BEGIN_EXPORT latex
\begineqns

% i.e., g is the â€œtransposeâ€ of f.
\eqn{Transpose-Char}{ f \,=\, \eval \,âˆ˜\, g âŠ— \Id \equivS \transpose{f} = g }

\eqn{Transpose-Self}{ \eval \,âˆ˜\, \transpose{f} âŠ— \Id \eqs f}

% \eqn{Transpose-Id}{\transpose{\eval} \eqs \Id}

\eqn{Transpose-Fusion}{ \transpose{f} âˆ˜ h \eqs \transpose{f \,âˆ˜\, h âŠ— \Id } }
%
% \eqn{CoEq-Unique}{x âˆ˜ p = y âˆ˜ p \impliesS x = y}
%
% \eqn{CoEq-Compose}{râ•±q âˆ˜ qâ•±p = râ•±p}
%
\endeqns
#+END_EXPORT
:Transpose-char-pf:

Given f : X âŠ— Y â†’ Z
show âˆƒâ‚ g : X â†’ (Y â© Z) such that f = eval_A âˆ˜ g âŠ— Id_A

Indeed,

  eval âˆ˜ (g âŠ— Id)
= âŒˆ Id âŒ‰ âˆ˜ (g âŠ— Id)
= âŒˆ Id âˆ˜ g âŒ‰  , by rad-fusion
= âŒˆ gâŒ‰

Whence, f = âŒˆgâŒ‰ and so g = âŒŠ f âŒ‹ lol

:End:
:Transpose-fusion-pf:

Given f : X âŠ— A â†’ B and h : Y â†’ X,

   âŒŠfâŒ‹ âˆ˜ h = âŒŠ f âˆ˜ h âŠ— Id âŒ‹ : Y â†’ A â© B

By lad fusion, or:

   âŒŠfâŒ‹ âˆ˜ h = âŒŠ ? âŒ‹
â‰¡  âŒŠ?âŒ‹ = âŒŠfâŒ‹ âˆ˜ h                      ;; symmetry of â€˜=â€™
â‰¡  ? = eval âˆ˜ (âŒŠfâŒ‹ âˆ˜ h) âŠ— Id          ;; transpose-char
â‰¡  ? = eval âˆ˜ (âŒŠfâŒ‹ âˆ˜ h) âŠ— (Id âˆ˜ Id)   ;; identities
â‰¡  ? = eval âˆ˜ (âŒŠfâŒ‹ âŠ— Id) âˆ˜ (h âŠ— Id)   ;; bi-functoriality
â‰¡  ? = f âˆ˜ (h âŠ— Id)                   ;; transpose-self

:End:
:Transpose-compose-pf:
   âŒŠfâŒ‹ âˆ˜ âŒŠgâŒ‹ = âŒŠ?âŒ‹
 â‰¡ ? = eval âˆ˜ (âŒŠfâŒ‹ âˆ˜ âŒŠgâŒ‹) âŠ— Id
 â‰¡ ? = eval âˆ˜ (âŒŠfâŒ‹ âˆ˜ âŒŠgâŒ‹) âŠ— (Id âˆ˜ Id)
 â‰¡ ? = eval âˆ˜ (âŒŠfâŒ‹ âŠ— Id) âˆ˜ (âŒŠgâŒ‹ âŠ— Id)
 â‰¡ ? = f âˆ˜ (âŒŠgâŒ‹ âŠ— Id)
:End:

# \newpage

* Enrichment & Internal Algebraic Structures

  A *Category ğ’³ enriched in a monoidal category ğ’±* or a *ğ’±-category*
    is essentially a category but its hom-types are objects of ğ’±.
    Formally, there is a collection ~Obj ğ’³~ and for each pair ~A, B~
    of such â€˜objectsâ€™ there is a â€˜hom-objectâ€™ ~ğ’³(A, B)~ in ğ’±,
    and there are two ğ’±-morphisms:
    1. Composition: $Î¼_{A, B, C} : ğ’³(B, C) âŠ— ğ’³(A, B) âŸ¶ ğ’³(A, C)$
       - Associativity: The two obvious ways $(ğ’³(C, D) âŠ— ğ’³(B, C)) âŠ— ğ’³(A, B) âŸ¶ ğ’³(A, D)$
         coincide.
    2. Identities:  $Î·_A : I âŸ¶ ğ’³(A, A)$.
       - Unity: The obvious maps $I âŠ— ğ’³(A, B) âŸ¶ ğ’³(A, B)$ coincide,
         as do the obvious maps $ğ’³(A, B) âŠ— I âŸ¶ ğ’³(A, B)$.

   \room
   A usual category is just a ğ’®â„¯ğ“‰-category.

   \room
   A /monoid in ğ’±/ is an object $M$ along with two morphisms
   $Î¼ : M âŠ— M âŸ¶ M, Î· : I âŸ¶ M$ such that the former is associative
   and has the latter as unit. Notice that monoids /in/ ğ’± are â€˜untypedâ€™
   analogues of ğ’±-categories. A â€˜monadâ€™ is a monoid in a category
   of endofunctors with composition as tensor.
   - If M is a monoid in ğ’± and F : ğ’± âŸ¶ ğ’² is a monoidal functor,
     then F M is a monoid in ğ’². Woah!

   \room
   In a monoidal category with natural transformations â€˜discardâ€™
   $!_X : X âŸ¶Ì‡ I$ and â€˜duplicateâ€™ $Î”_X : X âŸ¶Ì‡ X âŠ— X$,
   such as Cartesian monoidal categories, a /group/ is like a monoid but with
   an additional morphism $i : M âŸ¶ M$ such that the inverse axioms
   hold; e.g., $e = x Â· x^{-1}$ takes the point-free shape
   $Î· âˆ˜ !_M = Î¼ âˆ˜ (\Id âŠ— i) âˆ˜ Î”$.
   In an arbitrary monoidal category, a /Hopf algebra/ is like a group
   where $Î”_M$ and $!_M$ exist for our specific $M$. These generalise groups.

* COMMENT Why Monads Matter

Suppose the output of the producer function $g$ is /T-times more elaborate/
than the input of the consumer function $f$, then a composition scheme
is devised for such functions:

#+BEGIN_EXAMPLE haskell
T (T C)  âŸµT fâŸµ T B âŸµgâŸµ A
â†“                  â‹®
Î¼                  â‹®
â†“                  â‹®
T C      âŸµ  fâŸµ  B

Desired:  T C âŸµ f â—‡ g âŸµ A
#+END_EXAMPLE

Given by $f â—‡ g = Î¼ âˆ˜ T f âˆ˜ g$, where $Î¼ : T â†Ì£ TÂ²$ is a suitable polymorphic function.

Together with a unit function $Î· : T â†Ì£ Id$, datatype T forms a so-called /monad/ type.

The simplest of all monads is the /identity monad/ $T X = X$, which is such that
$Î¼ = id, Î· = id, f â—‡ g = f âˆ˜ g$. So ---in a sense--- the /whole functional discipline/
studied thus far was already /monadic/, living inside the simplest of all monads: The identity.
Put in other words, such functional discipline can be framed into a wider discipline in which
an arbitrary monad is present.


Kleisli Composition is assocatiive with identity Î· ---which coincide, for the identity monad,
to the usual composition and identity laws of functions---
and Î¼ = Id â—‡ Id and we have the fusion laws:
+ (f â—‡ g) âˆ˜ h = f â—‡ (g âˆ˜ h)     -- â—‡-âˆ˜ mutual associtivity
+  (f âˆ˜ g) â—‡ h = f â—‡ (T g âˆ˜ h)  -- âˆ˜-â—‡ fusion
+ T f âˆ˜ (h â—‡ k) = (T f âˆ˜ h) â—‡ k  -- Tâˆ˜-â—‡ mutual associtvity


If Î· is the identity of â—‡, then what happens when we compose with Id?
f â—‡ Id accepts a value of type $T B$ that is passed to $f : T C â† B$,
yielding an output of type $T\, C$. This construction is called /binding/:
$>>=f \;=\; f â—‡ Id$

Expressed pointwise we get $x >>= f \;=\; (Î¼ âˆ˜ T f) x$ which is exactly $f\, x$ for
the identity monad, so $>>=$ can be regarded as denoting /monadic function application/!

+ (f â—‡ g) a  =  do {b â† g a; f b}     =  g a >>= f
+ T f     x  =  do {a â†   x; Î·(f x)}  =  x   >>= Î· âˆ˜ f

  That is: map f = unpack x, apply f, pack result using Î·.

+ x >>= f          =  do {a â† x; f x}
+ (x >>= g) >>= f  =  x >>= (f â—‡ g)

For enjoyable reading on monadic IO in Haskell see [18] chapter 18.

Adjunctions play a major role in [16]

Neato: Î¼ âˆ˜ Î· = Id

From an adjunction L âŠ£ R, a monad T = R âˆ˜ G arises defined by
Î· = âŒˆIdâŒ‰ and Î¼ = RâŒŠIdâŒ‹.

* COMMENT Old Category Theory Theorem List

** Pullbacks
 \heading{Pullbacks}

 \def\Pb{ \operatorname{\mathsf{Pb}} }

 A \emph{pullback} of $f,g$ is an object $\Pb \, f \, g$ with two morphisms $f \nearrow g$ and $g \nearrow f$,
 and an operation ``named'' $(g \nearrow f , f \nearrow g)\under(-,-)$ with the following properties

 \NTHM{}{Axiom, Pullback Definition}{ $(g \nearrow f) \fcmp f \eqs (f \nearrow g) \fcmp g$}

 \NTHM{}{Axiom, Pullback Typing}{
   $p \fcmp f \eqs q \fcmp f \impliesS (g \nearrow f , f \nearrow g)\under(p, q) : \Pb\,f\,g \to \src p$}

 \NTHM{}{Axiom, Pullback Char}{
   $(g \nearrow f , f \nearrow g) \fcmp (x, x) \eqs (p, q) \;\equivs\; x \eqs (g \nearrow f , f \nearrow g)\under(p, q) $}

 \NTHM{}{Pullback Self, projections}{ \\
   {\color{white}.\hfill}
   $
   (g \nearrow f) \fcmp \big((g \nearrow f , f \nearrow g)\under(p, q)\big) \eqs p
   \landS
   (f \nearrow g) \fcmp \big((g \nearrow f , f \nearrow g)\under(p, q)\big) \eqs q
   $
   }

 \NTHM{}{Pullback Identity}{$\Id_{\Pb\,f\,g} \eqs (g \nearrow f , f \nearrow g)\under (g \nearrow f , f \nearrow g)$}

 \NTHM{}{$\nearrow$-Uniqueness, ``Jointly Epic''}{ \\
   {\color{white}.\hfill}
   $
   (g \nearrow f , f \nearrow g) \fcmp (x, x)
   \eqs
   (g \nearrow f , f \nearrow g) \fcmp (y, y)
   \impliesS x \eqs y
   $}

 \NTHM{}{Pullback-Fusion, ``Composition into Numerator''}{ \\
     {\color{white}.\hfill}
   $
   \bigg((g \nearrow f , f \nearrow g)\under(p, q)\bigg) \fcmp x
   \eqs
   (g \nearrow f , f \nearrow g)\under\bigg( (p, q) \fcmp (x,x) \bigg)
   $
   }

 \def\eq{\approx}
 \def\Eq{\mathsf{Eq}}

** Equalisers
 \heading{Equalisers}

 Equalisers generalise the notion of ``solution set to an equation''
 or the domain of the common image of two morphisms.

 An \emph{equaliser} of $f,g$, with $\src f = \src g$, is an object $\Eq\, f\, g$, a morphism $[f \eq g] : \Eq\,f\, g \to \src\, f$, and a
 function $-\over[f \eq g]$ with the following properties

 \NTHM{}{Axiom, Definition}{ $[f \eq g] \fcmp f \eqs [f \eq g] \fcmp g$}

 \NTHM{}{Axiom, Typing}{
   $q \fcmp f \eqs q \fcmp g \impliesS q \over [f \eq g] : \src q \to \Eq\,f\,g$}

 \NTHM{}{Axiom, Characterisation}{
   $x \fcmp\, [f \eq g] \eqs q \equivS x \eqs q \over [f \eq g] $}

 \NTHM{}{Identity}{$\Id_{\Eq\,f\,g} \eqs [f \eq g] \over [f \eq g]$}

 \NTHM{}{PreFusion, ``PreComposition into Numerator''}{
   $x \fcmp ( q \over [f \eq g]) \eqs (x \fcmp q) \over [f \eq g]$}

 \NTHM{}{Self, Denominator Cancellation}{
   $ \big(q \over [f \eq g]\big) \fcmp [f \eq g] \eqs q
   \\
     \text{\hspace{18.4em}} \big(q \fcmp [f \eq g]\big) \over [f \eq g] \eqs q
   $
 }

 \NTHM{}{Epic}{
   $x \fcmp [f \sim g] \eqs y \fcmp [f \sim g] \impliesS x \eqs y$}

 \newpage

 \NTHM{}{ X }{ Y }

 \todo rewrite this section: Too much text. Introduce heuristics.
** Finality

 \heading{Finality}
 \def\final#1{ \mbox{!}_#1 }

 \NTHM{}{Axiom, final-Characterisation}{$f : B \to 1 \equivS f \eqs \final{B}$}

 \NTHM{}{final-Self}{$\final{1} : 1 \to 1$}

 \NTHM{}{final-Identity}{$\Id_1 \eqs \final{1}$}

 \NTHM{}{final-Uniqueness}{$f,g : B \to 1 \impliesS f \eqs g$}

 \NTHM{}{final-Fusion / Post-absorption}{$f : B \to C \impliesS f \fcmp \final{B} \eqs \final{C} $}

 \NTHM{}{initial-final Coincidence}{$\initial{1} \eqs \final{0}$}
** Pushouts
 \kern4ex
 \heading{Pushouts}

   Taking $D$ and $\mathcal{D}$ as suggested by
   $D\,\mathcal{D}:
     \spot\!\!\!\!_B\overset{f}{\text{\tiny$\longleftarrow$}} \!\!
     \spot\!\!\!\!_A\overset{g}{\text{\tiny$\longrightarrow$}}\!\!\spot\!\!\!\!_C
   $

 Working out the details and simplifying, we find that $\bigvee D$
 has objects morphism-pairs $p,q$ that post-equalise $f$ and $g$ in that $f \fcmp p \eqs g \fcmp q$, and morphisms
 $x : (p,q) \to (p',q') \equivS x : \tgt p \to \tgt p' \lands (p', q') \eqs (p,q) \fcmp x$.
 \iffalse details

 Since D is just the inclusion, we have that a cocone for the above diagram
 is a triple (p,q,r) to a single object and have sources (B,A,C) respectively
 such that r = f;p = g;q and so r is determined uniquely by p and q; whence it
 can be ignored.

 then a morphism between such pairs (p,q) and (p',q') is a morphism from the target
 of the former to the target of the latter such that the resulting triangles commute :-)

 Draw the diagrams!

 A \emph{pushout} of $f,g$ is an inital object in this category.

 Initiality means there exists an
 object $Po\, f\, g$ with two morphisms $f \searrow g$ and $g \searrow f$,
 and there exists a function $(f , g)\under(-,-)$ with the following properties

 \NTHM{}{Axiom, Pushout-Definition}{ $f \fcmp (f \searrow g) \eqs g \fcmp (g \searrow f)$}

 \NTHM{}{Axiom, Pushout-Typing}{
   $f \fcmp p \eqs g \fcmp q \impliesS (f , g) \under (p , q) : Po\,f\,g \to \tgt p$}

 \todo this looks wrong!\\
 \NTHM{}{Axiom, Pushout-Characterisation}{
   $(f , g) \fcmp (x , x) \eqs (p , q) \equivS x \eqs (f , g) \under (p , q) $}

 \NTHM{}{Pushout-Self, projections}{
   $f \fcmp (f , g) \under (p , q) \eqs p
   \landS
   g \fcmp (f , g) \under (p , q) \eqs q$
   }

 \NTHM{}{Pushout-Identity}{$\Id_{Po\,f\,g} \eqs (f , g) \under (f , g)$}

 \NTHM{}{$\under$-Uniqueness, ``Jointly Epic''}{
   $(f , g) \fcmp (x , x) \eqs (f , g) \fcmp (y , y) \impliesS x \eqs y$}

 \NTHM{}{Pushout-Fusion, ``Composition into Numerator''}{ \\
   $\big((f , g) \under (p , q)\big) \fcmp x \eqs (f , g) \under \big( (p , q) \fcmp (x,x)\big)$}
** Functions and Contracts

 What do we want functions for? If we ask this question to a physician or engineer
 the answer is very likely to be: one wants functions for modelling and reasoning
 about the behaviour of real things.

 So we get a naive purpose of functions: we want them to be applied to arguments
 in order to obtain results.

 A function /f : B âŸµ A/ can be regarded as a kind of â€œcontractâ€: /f/ commits
 itself to producing a /B/-value provided it is supplied with an /A/-value. How is such
 a value produced? In many situations one wishes to ignore it because one is just
 using function /f/. In others, however, one may want to inspect the internals of the
 â€œblack boxâ€ in order to know the functionâ€™s computation rule.

 How free are we to fulfill the â€œgive me an /A/ and I will give you a /B/ â€ contract of
 declaration /f : B âŸµ A/? In general, the choice of /f/ is not unique. Some /f/s will do as little
 as possible while others will laboriously compute non-trivial outputs. At one of
 the extremes, we find functions which â€œdo nothingâ€ for us, that is, the added-value
 of their output when compared to their input amounts to nothing:
 /f a = a/.
 In this case /B = A/, of course, and /f/ is said to be the identity function on /A/.

 Observe that â€œnaturality lawsâ€ /f Â· h = h Â· f = f/ have a unique solution for /h/: â€œtheâ€ id-entity function.

** Sums
 Given two functions /f : C âŸµ A/ and /g : C âŸµ B/ with a common target, we can â€œglueâ€ them
 together to obtain a function â€œeither /f/ or /g/â€ /[f , g] : C âŸµ A + B/ to that same common target but whose
 source is the sum of the sources. This function behaves like /f/ when the input comes from the
 â€œ/A/-sideâ€ and behaves like /g/ when the input comes from the â€œ/B/-sideâ€.

 The inputs can be â€œinjectedâ€ into this new composite source via two new combinators:
 /inl : A + B âŸµ A/ and /inr : A + B âŸµ B/.

 # From this property it follows that the injections do not confuse inputs, i.e., are jointly-injective.
 Indeed the property above could be thought of as ensuring /no-junk and no-confusion/.
 - /no-junk/: Everything starting at a sum /h : C âŸµ A + B/ is necessarily expressible as a case: /h = [ h . inl , h . inr].
 - /no-confusion/: Casing does not mix-up distinct inputs, /[f , g] = [h , k] â‡’ f = h âˆ§ g = k/.

** Products

 When two morphisms do not compose but instead have a common source then we can /glue/
 them with the /pairing/ combinator /âŸ¨-,-âŸ©/ by forming the product of their targets.
 When the morphisms are not necessarily related, we can still /glue/ them together via the /product/ combinator;
 namely if /f : A âŸµ C/ and /g : B âŸµ D/ then, /product of f and g/, /f Ã— g : A Ã— B âŸµ C Ã— D/.

 Both gluing combinators maintain the information of their individual arguments, in the same way
 (Cartesian) product /A Ã— B/ keeps the information of /A/ and /B/.

 + Product is essentially symmetric
   - /swap := âŸ¨ snd , fst âŸ©/ is an involution witnessing /A Ã— B â‰… B Ã— A/
   - In programming, where products are records, /swap/ can be re-interpreted as a guarantee that one
     does not lose (or gain) anything in swapping fields in struct or record datatypes.

 + Product is essentially associative
   - â€œassociate to the rightâ€, /assocr := âŸ¨ fst . fst , snd Ã— Id âŸ© : A Ã— (B Ã— C) âŸµ (A Ã— B) Ã— C/
   - This is an isomorphism with inverse /assocl := âŸ¨ Id Ã— fst , snd âˆ™ snd âŸ© : (A Ã— B) Ã— C âŸµ A Ã— (B Ã— C)/.

** The Exchange Law

 /Datatype constructions, such as A + B and A Ã— B, arise as devices for expressing
 the types of the results of gluing given morphisms./

 Two morphisms /Aâ€² âŸµ A/ and /Bâ€² âŸµ B/ can thus be combined by our combinators to
 obtain a morphism /Aâ€² Ã— Bâ€² âŸµ A + B/. Since such a morphism has source a sum, it
 can be obtained via the casing combinators; conversely, since it has target a product,
 it can be obtained via the pairing combinators. Naturally both forms coincide due to
 the /Exchange Law/ which expressed the equivalence of case-of-pairs and pair-of-cases:
 /[ âŸ¨f,gâŸ© , âŸ¨h,kâŸ© ]  =  âŸ¨ [f,h] , [g,k] âŸ©/.

 For example, applying this rule to the expression /âŸ¨ [ fst , fst ] , [ inl . snd , inr . snd] âŸ©/
 yields the familiar /un-distribute-right/ operation /undistr = [ Id Ã— inl , Id Ã— inr ] : A Ã— (B + C) âŸµ (A Ã— B) + (A Ã— C)/.

** typechecking and gluing

  Formally, a â€œtype checkingâ€ discipline can be expressed in terms of compositional rules which
 check for functional expression wellformedness.

 I really like this approach you are promoting: Construing high-order combinators as
 variations on composition.

** LaTeX Matter To Utilise

%{{{ header

\documentclass[11pt]{article}

\usepackage[hmargin=10mm, vmargin=10mm, landscape]{geometry}
\usepackage{multicol}

\usepackage[svgnames]{xcolor}
%
% See Section-4 for colors by name at:
% ftp://ftp.mackichan.com/swandswp55/tcitex/doc/latex/contrib/xcolor/xcolor.pdf

\usepackage{amsmath,amssymb}

\usepackage{CalcStyleV9} % http://calccheck.mcmaster.ca/
\usepackage{calculation} % https://ctan.org/pkg/calculation
\parindent0pt\parskip0.6ex % Better for literate code.

\pagestyle{empty}

%{{{ main control macros

% bold item and place it in a box; intended to section-off related theorems
\def\heading#1{\fbox{\bf #1}}

% handy-dandy things to try in this domain we're learning.
\newcommand\heuristic[1][]{ {\color{red} \bf Heuristic \if#1\empty\else\ldq#1\rdq\fi:} }

% a notice ``to do'' sign for current construction sites
\def\todo{ {\color{DarkSlateBlue}\;\fbox{\sc \textbf{To Do ::}}\;} }

% 1 number or referencing item
% 2 name of theorem
% 3 theorem statement
\def\theorem#1#2#3{\parbox{3em}{#1} \ \textbf{#2:} \ \ {#3}}

%}}}
%{{{ logical operators with padding

% suffixes:
% 's' for a ``little'' bit    of space
% 'S' for a ``large''  amount of space

\def\equivS{\qquad \equiv \qquad}

\def\eqs{\quad = \quad}

\def\lands{\quad \land \quad}
\def\landS{\qquad \land \qquad}

%}}}
%{{{ category theory identifiers

% identity morphism
\def\Id{\mathsf{id}}

% source and target
\def\src{\mathsf{src}\,}
\def\tgt{\mathsf{tgt}\,}

% converse morphism notation
\def\from{\leftarrow}

% natural transformation
\def\natTo{\mathrel{\ooalign{\,$\longrightarrow$\cr\quad.}}}
%}}}

\theorem{}{Principle of Duality}{
  A statement $S$ is true about $\mathcal{C}$
  iff it's dual $S[\fcmp \,:=\, \circ]$ is true about $\mathcal{C}^{op}$. }

\theorem{}{{\color{gray} Cancellation / } Projection Definitions}{ $\fst \cdot \< f , g \> = f \landS \snd \cdot \< f , g \> = g$ }

\theorem{}{{\color{gray}Fusion / } Pointwise Definition of Pairing}{ $\<f , g \> \cdot h \eqs \<f \cdot h , g \cdot h\>$ }

\theorem{}{Axiom, Product bi-map definition}{ $f \x g \,:=\, \< f \cdot \fst , g \cdot \snd \>$ }

\theorem{}{{\color{gray} Induced} Pointwise Definition of Product bi-map }
        { \hspace{-1em} $(f \x g) \cdot \<i , j \> \eqs \<f \cdot i , g \cdot j\>$ }

\theorem{}{{\color{gray}Absorption /} Induced Definition of Casing}
        { $\[ f , g \] \cdot (i \+ j) = \[ f \cdot i , g \cdot j \]$ }
** COMMENT Coequaliser

 # $Dğ’Ÿ = \left( \overset{A}{â€¢} \overset{\overset{f}{\rightrightarrows}}{g} \overset{B}{â€¢} \right)$.
 #
 Take $D$ and $ğ’Ÿ$ as suggested by $Dğ’Ÿ = \left( \overset{A}{â€¢} \rightrightarrows^f_g \overset{B}{â€¢} \right)$;
 where $f,g : A â†’ B$ are given.
 Then a cocone Î´ for $D$ is a two-member family $Î´ = (q', q)$
 with $q' : A â†’ C, q : B â†’ C, C = \tgt\,\delta$ and $Î´_A âˆ˜ \const{C} h = D h âˆ˜ Î´_B$; in-particular
 $q' = f âˆ˜ q = g âˆ˜ q$ whence $q'$ is fully-determined by $q$ alone.

 Let $Î³ = (p', p) : Obj(â‹D)$ be a colimit for $D$ and write $-pâ•±$ in-place of $-â•±Î³$, then the â•±-laws
 yield: /$p$ is a coequaliser of $(f,g)$/ if there is a mapping $-â•±p$ such that /CoEq-Type/ and
 /CoEq-Char/ hold.

 #+BEGIN_EXPORT latex
 \begineqns

 \eqn{CoEq-Type}{ f âˆ˜ q =  g âˆ˜ q \impliesS qâ•±p : \tgt\, p â†’ \tgt\,q}

 \vspace{2ex}
 Well-formedness convention: In each law the variables are quantified
 in such a way that the premise of \ref{CoEq-Type} is met.
 The notation $qâ•±â‹¯$ is only senseful if $f âˆ˜ q = g âˆ˜ q$,
 like in arithmetic where the notation $m \div n$ is only sensful if $n$ differs from 0.

 \eqn{CoEq-Char}{ x âˆ˜ p = q \equivS x = q â•± p }

 \eqn{CoEq-Self}{ qâ•±p âˆ˜ p = q}

 \eqn{CoEq-Id}{pâ•±p = \Id}

 \eqn{CoEq-Fusion}{x âˆ˜ qâ•±p = (x âˆ˜ q)â•±p}

 \eqn{CoEq-Unique}{x âˆ˜ p = y âˆ˜ p \impliesS x = y}

 \eqn{CoEq-Compose}{râ•±q âˆ˜ qâ•±p = râ•±p}
 %%
 %% \eqn{?â•²-Functor-Dist}{F(Î´â•±Î³) = FÎ´ â•± FÎ³}
 %%
 %% \eqn{?â•²-Pre-Functor-Elim}{Î´Fâ•±Î³F = Î´â•±Î³}
 %%
 \endeqns
 #+END_EXPORT

 # CoEqualisers generalise the notion of induced equivalence relation.

 \vfill

 \iffalse

   Taking $D$ and $\mathcal{D}$ as suggested by
   $D\,\mathcal{D}:$
   $ \raisebox{6pt}{$\spot$}
   \overset{ \overset{f}{\text{\tiny$\longrightarrow$}}
     }{ \overset{\longrightarrow}{\text{\tiny$g$}}  }
   \raisebox{6pt}{$\spot$}
   $

 Now call the category $\bigvee D$ by the name $\bigvee(f \,|\!|\, g)$:
 it has objects morphisms that post-equalise $f$ and $g$, and morphisms
 $x : p \to q \equivS x : \tgt p \to \tgt q \lands p \fcmp x \eqs q$

 A \emph{coequaliser} of $f,g$ is an initial object in
 $\bigvee(f \,|\!|\, g)$.

 \fi

 # coEqualiser-Fusion, ``Composition into Numerator''
** COMMENT Naturality
 0. The arrow category $ğ’^â†’$ has objects being ğ’-arrows, and arrows $f â†’ g$ are
    pairs $(h, k)$ of ğ’-morphisms that yield commuting squares: $k âˆ˜ f = g âˆ˜ h$.
    Composition is the pasting of commuting diagrams; i.e., component-wise.

    - This gives rise to two functors $ğ’®ğ“‡ğ’¸, ğ’¯â„Šğ“‰ : ğ’^â†’ â†’ ğ’$ that project that
      morphisms from the pairs.

    - Observe that ğ’-commuting cubes can be construed as objects of $(ğ’^â†’)^â†’$.

 + A natural transformation Î· : F â†’ G : ğ’ â†’ ğ’Ÿ is precisely
   a functor $Î· : ğ’ â†’ ğ’Ÿ^â†’$ where $ğ’®ğ“‡ğ’¸ âˆ˜ Î· = F \lands ğ’¯â„Šğ“‰ âˆ˜ Î· = G$.

   - That Î· is a functor to the arrow category of ğ’Ÿ means that
     for each object A of ğ’, there is a ğ’Ÿ-arrow Î·_A;
     and for each morphism f : A â†’ B, there is a ğ’Ÿ-commuting square
     $(h, k) : Î·_A â†’ Î·_B$; i.e., $k âˆ˜ Î·_A = Î·_B âˆ˜ h$.

  - By the constraints on Î·, we know $F f = (ğ’®ğ“‡ğ’¸ âˆ˜ Î·) f = h$
    and $G f = (ğ’¯â„Šğ“‰ âˆ˜ Î·) f = k$.

    Thus, we have $G f âˆ˜ Î·_A = Î·_B âˆ˜ F f$.

 + A natural transformation is invertible precisely when its components are all invertible; i.e., (Î·Ë˜)â‚ = (Î·â‚)Ë˜.

* COMMENT Maybe move to README
** Gluing functions which do not compose -- products         :move_to_readme:

 Composition is the basis for gluing morphisms together to build more complex morphisms.
 However, not every two morphisms can be glued together by composition.

 # $f : A â† C$ and $g : B â† C$
 For instance, functions $A \overset{f}{\longleftarrow} C \overset{g}{\longrightarrow} B$
 do not compose with each other since the source of one is not the target of the other.

 \room

 Since $f$ and $g$ share the same source, their outputs can be paired: $c â†¦ (f\, c, g\, c)$.
 We may think of the operation which pairs the outputs of $f$ and $g$ as a new function
 combinator: $âŸ¨f, gâŸ© = c â†¦ (f\, c, g\, c)$ ---read â€œ$f$ /split/ $g$â€.

 \room

 $âŸ¨f, gâŸ©\, c = âŸ¨f\, c, g\, câŸ©$ duplicates $c$ so that $f$ and $g$ can be executed in â€œparallelâ€ on it.

 \room

 Function $âŸ¨f,gâŸ©$ keeps the information of both $f$ and $g$ in the same way
 Cartesian product $A Ã— B$ keeps the information of $A$ and $B$.
 So, in the same way that $A$ data or $B$ data can be retrieved from $A Ã— B$ data
 via the projections $A \overset{\fst}{\longleftarrow} A Ã— B \overset{\snd}{\longrightarrow} B$,
 $f$ and $g$ can be retrieved via the same projections:
 \eqn{Cancellation}{\fst âˆ˜ âŸ¨f, gâŸ© = f \lands \snd âˆ˜ âŸ¨f, gâŸ© = g}

 \room

 A /split/ arises wherever two functions do not compose but share the same source.

 \room

 How do we glue functions that fail such a requisite, say $f : A â† C$ and $g : B â† D$?
 We regard their sources as projections of a product:
 $A \overset{f âˆ˜ \fst}{\longleftarrow} C Ã— D \overset{g âˆ˜ \snd}{\longrightarrow} B$.
 Now they have the same source and so the split combinator can be used:
 $f Ã— g = (c, d) â†¦ (f\, c, g\, d)$.
 #
 This corresponds to the â€œparallelâ€ application of $f$ and $g$, each with its /own/ input.

 \room

 What is the /interplay/ among the functional combinators: Composition, split, product?
 The first two relate to each other via the fusion law
 \eqn{Fusion}{âŸ¨f, gâŸ© âˆ˜ c = âŸ¨f âˆ˜ c, g âˆ˜ câŸ©}
 Notice how it looks like the /definition/ of the split operator but all applications have
 been lifted to compositions! Woah!
   + Moreover, the absorption property is just the lifting of the pointwise definition! Woah!

 \room

 All three combinators interact via the Ã—-absorption property.

 # \room
 #
 # The following is self-inverse,
 # \eqn{swap-Def}{swap = âŸ¨\snd, \fstâŸ© : A Ã— B â‰… B Ã— A}

** Gluing functions which do not compose -- coproducts       :move_to_readme:

 In the scenario $A \overset{f}{\rightlongarrow} C \overset{g}{\leftlongarrow} B$,
 it is clear that the kind of glue we need should make it possible to apply $f$
 if the input is from the â€œ$A$ sideâ€ or apply $g$ if it is from the â€œ$B$ sideâ€.

 \room

 We denote this new combinator â€œeither $f$ or $g$â€, $[f, g] : A + B â†’ C$, where the values of $A + B$
 can be thought of as â€œcopiesâ€ of $A$ or $B$ values which are â€œstampedâ€ with different
 tags in order to guarantee that values which are simultaneously in $A$ and $B$ do not
 get mixed up.

 \room

 Duality is of great /conceptual economy/ since everything that can be said of concept /X/
 can be rephrased for /co-X/. *That $\composition$-listing provides eloquent evidence of duality.*

 \room

 Notice that the fusion law,
 \eqn{Fusion}{ f âˆ˜ [g, h] = [f âˆ˜ g, f âˆ˜ h]}
 Is essentially the definition: If we're in the left `g` then the result is `f` applied to it;
 otherwise if we're in the right `h` then the result is `f` applied to it! Woah: From pointwise to pointfree.
** COMMENT An Introduction to Pointfree Programming

 # Functional programming literally means â€œprogramming with functionsâ€.
 The main emphasis is on /compositionality/, one of the main advantages
 of â€œthinking functionallyâ€, explaining how to construct new functions
 out of other functions using a minimal set of predefined functional
 combinators. This leads to a style which is /pointfree/ in the sense that
 function descriptions dispense with variables --also known as /points/.

 \room

 Why do people look for compact notations?
 A compact notation leads to shorter documents, less lines of code
 in programming, in which patterns are easier to identify and reason
 about. Properties can be stated in clear-cut, one-line long equations
 which are easy to memorise.
 #
 # Backus' FP and Iverson's APL are examples of pointfree programming
 # languages; look them up ;-)

 \room

 The notation â€œ$f : A â†’ B$â€ focuses on what is relevant about $f$
 and can it be regard as a kind of contract:
 #+BEGIN_QUOTE
 $f$ /commits itself/ to producing a /B/-value provided it is supplied
 with an /A/-value.
 #+END_QUOTE

 \room

 Types provide the â€œglueâ€, or interface, for putting functions
 together to obtain more complex functions.

 \room

 What do we want functions for?
 One wants functions for modelling and reasoning about the
 behaviour of real things.

 \room

 In order to switch between the pointwise and pointfree settings
 we need two â€œbridgesâ€: One lifting equality to the function level
 and the other lifting function application.

 \room

 Two functions $f, g : A â†’ B$ are the same function iff they agree
 at the pointwise level:
 \eqn{Extensionality}{ f = g \equivS \left(âˆ€ a : A â€¢ f\, a = g\, a \right)}

** COMMENT Gluing functions which do not compose --exponentials

 Functional application is the bridge between the pointfree and pointwise worlds.

 Suppose we are given the task to combine two functions, one binary
 $B \overset{f}{\leftlongarrow}{C Ã— A}$ and the other unary $D \overset{g}{\leftlongarrow} A$.
 Clearly none of the combinations $f âˆ˜ g, âŸ¨f, gâŸ©$, or $[f, g]$ is well-typed. Hence $f$ and $g$
 cannot be put together and require some extra interfacing.

 \room

 Note that $âŸ¨f, gâŸ©$ would be well-defined in case the $C$ components of $f$'s source
 could be somehow â€œignoredâ€. Suppose, in fact, that in some particular context
 the first argument of $f$ happens to be â€œirrelevantâ€, or to be frozen to some $c : C$.
 It is easy to derive a new function $f_c : A â†’ B : a â†¦ f(c, a)$ from $f$, combines
 nicely with $g$ via the split combinator: $âŸ¨f_c , gâŸ©$ is well-typed and bears the type
 $B Ã— D â† A$. ---MA: The typing looks erroneous.

 \room

 Since $f_c$ denotes a function of type $B â† A$, we will say $f_c : B^A$, where the exponential
 is an alternate notation for the arrow --the reason to adopt it is that it is functorial, as will be shown.

 We want functions so as to apply them, so it's natural to introduce the /apply/ operator which applies
 a function to an argument:
 \[ ap : B â† B^A Ã— A \]
 \[ ap(f, a) = f \ a \]

 #+LaTeX: \def\transpose#1{ \overline{#1} }

 #+BEGIN_EXPORT latex
 The above â€œtransposeâ€ operation $f â†¦ f_c$ is of type $B^A â† C$.
 It expresses $f$ as a kind of /C/-indexed family of functions $B â† A$.
 It will be denoted by $\transpose{f}$ with the definition $(\transpose{f}\, c)\, a = f(c, a)$.

 Observe that $\transpose{f}$ is more tolerant than $f$: The latter is a binary operator and so
 requires /both/ arguments $(c,a)$ to become available before application, whereas the former is happy
 to be produced with $c$ first  and with $a$ later on.

 \eqn{exponential-Char}{ k = \transpose{f} \equivS f = ap âˆ˜ (k Ã— \Id) }

 Pointwise, $(ap âˆ˜ (\transpose{f} Ã— \Id))\, (c, a) = ap \, (\transpose{f}\, c, a) = \transpose{f} \, c\, a$
 which should be equal to $f(c,a)$ and indeed it is --in pointfree form:

 \eqn{exponential-Cancellation}{ f = ap âˆ˜ (\transpose{f} Ã— \Id) }

 \eqn{exponential-Id}{ \transpose{ap} = \Id_{B^A} }

 \eqn{exponential-Fusion}{ \transpose{g âˆ˜ (f Ã— \Id)} \eqS \transpose{g} âˆ˜ f }

 We can thus introduce a new functional combinator, which arises as the transpose
 of $f âˆ˜ ap$:

 \eqn{exponential-Functor-Type}{ f^A : C^A â† B^A \providedS f : C â† B}

 $B^A$ can be thought of as a /syntactical/ representation of the /semantical/ $A â†’ B$
 --they are the `same', but the way we treat them and think of them is different.
 [ This differs from $(-â†’-)$ when discussing adjunctions earlier, since that was
   the `external hom-/set/'; this is the /internal/ hom! ]

 \eqn{exponential-Functor-Defn}{ f^A = \transpose{f âˆ˜ ap} }
 #+END_EXPORT

 Pointwise: $f^A\, g = f âˆ˜ g$ --so $f^A$ is the â€œcompose with $f$â€ functional combinator.
 # Indeed:
 #
 #     fáµƒ = T(f . ap)
 # â‰¡   f . ap = ap âˆ˜ (fáµƒ Ã— Id)                 exp-Char
 # â‰¡   f . ap (g, a) = ap âˆ˜ (fáµƒ Ã— Id) (g, a)   extensionality
 # â‰¡   f (g a)       = ap âˆ˜ (fáµƒ g, a)          Ã—-simpl
 # â‰¡   f (g a)       = fáµƒ g a                  ap-simpl
 # â‰¡   f âˆ˜ g         = fáµƒ g                    extensionality

 \eqn{exponential-Functorial}{ (g âˆ˜ h)^A \eqs g^A âˆ˜ h^A \landS \Id^A = \Id }

 *Interestingly:*

 #+BEGIN_EXPORT latex
 \eqn{Ohâ‚€}{ \transpose{\snd} \eqs \const{\Id} }

 \eqn{Ohâ‚}{ \transpose{f}\, a \eqs f âˆ˜ âŸ¨ \constant{a} , \Id âŸ© }

 \eqn{Ohâ‚‚}{ \const{g} \eqs \transpose{g âˆ˜ \snd} }

 Also $\transpose{\snd}$ is a constant function:
 \eqn{Ohâ‚ƒ}{ \transpose{\snd} âˆ˜ f \eqs \transpose{\snd} }
 #+END_EXPORT


+ Absorption: gá´¬ âˆ˜ \transpose{h} = \transpose{g âˆ˜ h}

   Whence Closed definition: \transpose{g} = gá´¬ âˆ˜ \transpose{Id}.

+ Naturality: h âˆ˜ \transpose{Id} = \transpose{Id} âˆ˜ (há´¬ Ã— Idâ‚)

* COMMENT Maybe move to a different project
  F-Algebras: â€œRecursion in the Pointfree Styleâ€
** COMMENT Â§3 Recursion in the Pointfree Style

 This section provides a naive introduction to algorithm analysis and synthesis
 by showing how a quite elementary class of algorithms --equivalent to for-loops
 in C or any imperative language-- arise from elementary properties of the underlying
 maths domain.

 The /for-loop/ combinator is defined by
 \[ for \; f \; e \; 0 = e \]
 \[ for \; f \; e \; (n+1) = f (for \; f \; e \; n) \]

 Which can be implemented, in C:
 #+BEGIN_EXAMPLE
T total = e;
for(int j=0; j<n; j++)
   total = f(total);
 #+END_EXAMPLE

 This is the catamorphism for â„•!

 #+BEGIN_EXPORT latex
 \eqn{in-â„•}{ in = [ \const{0}, succ] : 1 + â„• â‰… â„• }

 The above witness: Every natural number is either 0 or the successor of another natural number.
 --That is, such $in$'s characterise inductive types!--

 \eqn{for-loop-universal}{ f = for \; g \; e \equivS f âˆ˜ in = [ \const{k}, g] âˆ˜ (\Id + f)] \eqs â¦‡ [\const{k}, g] â¦ˆ}

 \eqn{for-loop-Id}{ for\; succ\; 0 \eqs \Id}

 \eqn{for-loop-fusion}{ h âˆ˜ for\; g\; k \eqs for\; j \; (h\, k) \providedS h âˆ˜ g = j âˆ˜ h}
 % Proof: Universal property along with fact (a+) âˆ˜ Id = (a+).
 #+END_EXPORT

 *Heuristic* Implement a given function $f$ as a for-loop by using for-loop-Id then for-loop-fusion
 and solving for unknown $j$ along the way; e.g.,

 #+begin_calculation latex
   f
 \step{ Identities and for-loop-Id }
   f âˆ˜ for\; succ\; 0
 \step{ For-loop-fusion, calculute for unknown $j$ in $f âˆ˜ succ = j âˆ˜ f$ }
   for \; j \; (f\; 0)
 #+end_calculation

 Really the $j$ is what $f$ does at the inductive step
 --we cover the base step by computing $f\; 0$ directly.

 For example, in this way we obtain $(a+) = for\; succ\; a$.

 Other fun stuff:
 #+BEGIN_EXPORT latex
 \eqn{simple-optimisation}{ for\; \const{k} \; k \eqs for\; \Id \; k}
 #+END_EXPORT

** COMMENT Â§3.12 F-catamorphisms

 For a polynomial functor $F$ a particular iso /F/-algebra always exists, which
 is denoted by $in : Î¼F â† F (Î¼F)$ and has special properties. First, its carrier
 is the smallest among the carriers of other iso /F/-algebras, and this is why it
 is denoted by $Î¼F$ ---$Î¼$ for â€œminimalâ€.

 #+BEGIN_EXPORT latex
 \eqn{cata-Char}{k = â¦‡Î±â¦ˆ \equivS k âˆ˜ in = Î± âˆ˜ F k }

 \eqn{cata-Id}{ â¦‡inâ¦ˆ = \Id_{Î¼F} }


 \eqn{cata-cancellation}{â¦‡Î±â¦ˆ âˆ˜ in = Î± âˆ˜ F â¦‡Î±â¦ˆ}

 That is, since in is an iso, â€œDecompose input, via out, then recurse, then apply Î±â€:

 \eqn{cata-cancellation}{â¦‡Î±â¦ˆ = Î± âˆ˜ F â¦‡Î±â¦ˆ âˆ˜ out}

 \eqn{cata-Fusion}{ f âˆ˜ â¦‡Î±â¦ˆ = â¦‡Î²â¦ˆ \providedS f âˆ˜ Î± = Î² âˆ˜ F f}
 #+END_EXPORT

 *Go back and re-read Â§3.13* Then do exercises 3.11, 3.16, 3.17.

** COMMENT Â§3.13 Introducing Inductive Datatypes

 A comment on the particular choice of terminology: ~in~ suggests that we are
 going inside, or constructing values of the datatype; whereas ~out~ suggests
 that we are going out, or destructing, analysing, values of the datatype.

 A nifty duality.

** COMMENT Â§3.16 The mutual-recursion law

 Suppose we are executing two (possibly mutually recursive) functions $f$ and $g$ in parallel, which may be catamorphisms
 and so somewhat costly. Could we turn these into a single function call to a catamorphism?

 Solving for $h, k$ in $âŸ¨f, gâŸ© = â¦‡âŸ¨h, kâŸ©â¦ˆ$ yields â€œFokkinga's Lawâ€, aka

 #+BEGIN_EXPORT latex
 \eqn{Mutual-Recursion-Law}{ âŸ¨f, gâŸ© = â¦‡âŸ¨h, kâŸ©â¦ˆ \equivS f âˆ˜ in = h âˆ˜ F âŸ¨f, gâŸ© \landS g âˆ˜ in = k âˆ˜ FâŸ¨f, gâŸ©}
 #+END_EXPORT

 This law is useful in combining two mutually recursive functions into a single catamorphism
 --with added efficiency by possibly using the exchange law.

 *Heuristic*
 We can apply this law to a single function by looking at an â€œinefficientâ€ subpart and
 extracting that into its own function, then trying to apply the law, with possibly more extractions.

  - Isn't the computing-cubes-in-linear-time problem solved this way?

 For example, there is much re-calculation in the definition of fib:
 \[ fib \; 0 = 1 \]
 \[ fib \; 1 = 1 \]
 \[ fib \; (n+2) = fib \; (n+1) + fib\; n \]
 Can we improve its performance?
 The clue is to regard the two instances of $fib$ in the recursive branch as mutually recursive
 over the natural numbers. The clue is /suggested/ not only by $fib$ having two base cases
 --so, perhaps it hides two functions-- but also by the lookahead $n + 2$ in the recursive clause.
 Introducing $f$ by the /specification/ $f\; n = fib(n + 1)$ then using that to obtain a recursive definition,
 possibly depending on $fib$, allows us to use the mutual recursive law to obtain a linear function
 from which $fib$ is obtained via a simple projection --say $\snd$.

 The mutual recursive law generalises to more than two mutually recursive functions, in this case three:
 #+BEGIN_EXPORT latex
 \eqn{Mutual-Recursion-Law}{ âŸ¨fâ‚€, âŸ¨fâ‚, fâ‚‚âŸ©âŸ© = â¦‡âŸ¨hâ‚€, âŸ¨hâ‚, hâ‚‚âŸ©âŸ©â¦ˆ \equivS âˆ€i:0..2 â€¢ fáµ¢ âˆ˜ in = háµ¢ âˆ˜ FâŸ¨fâ‚€, âŸ¨fâ‚, fâ‚‚âŸ©âŸ© }
 #+END_EXPORT

 Now go do exercises 3.23, 3.26, 3.28 ;-)

 An immediate consequence of Mutual-Recursion-Law is
 #+BEGIN_EXPORT latex
 \eqn{Banana-Split-Law}{ âŸ¨â¦‡iâ¦ˆ, â¦‡jâ¦ˆâŸ© = â¦‡(i Ã— j) âˆ˜ âŸ¨F \fst, F \sndâŸ© â¦ˆ }

 This law provides us with a very useful tool for â€œparallel loopâ€ fusion.
 #+END_EXPORT

** COMMENT Â§2.14 Guards and McCarthy's conditional

 Define
 \eqn{?-Defn}{(p?)\, a = \mathsc{If}\, p\, a \,\mathsc{Then}\, \inl\, a \,\mathsc{Else}\, \inr\, a \mathsc{Fi}}

 We call $p? : A â†’ A + A$ the /guard/ associated to predicated /p : A â†’ ğ”¹/.
 The guard is more informative than /p/ alone: It provides information about the outcome of testing /p/
 on some input /a/, encoded in terms of the sum injections, $\inl$ for /true/ and $\inr$ for /false/,
 without losing the input /a/ itself.

 #+BEGIN_EXPORT latex
 \begineqns
 \eqn{McCarthy-Conditional-Defn}{ p â†’ g,h \quad=\quad [g, h] âˆ˜ p?}

 Hence to reason about conditionals one may seek help in the algebra of sums.

 \eqn{conditional-Fusionâ‚}{ f âˆ˜ (p â†’ g, h) \quad=\quad p â†’ f âˆ˜ h, f âˆ˜ h}

 \eqn{conditional-Fusionâ‚‚}{ (p â†’ g, h) âˆ˜ k \quad=\quad p âˆ˜ k â†’ g âˆ˜ k, h âˆ˜ k}

 \eqn{conditional-product-Fusion}{ k âˆ˜ âŸ¨ (p â†’ f,h) , (p â†’ g, i) âŸ©  \eqs p â†’ k âˆ˜ âŸ¨f, gâŸ© , k âˆ˜ âŸ¨h, iâŸ© }

 %
 %   k âˆ˜ âŸ¨ (p â†’ f,h) , (p â†’ g, i) âŸ©
 % = k âˆ˜ âŸ¨ [f,h] âˆ˜ p? , [g, i] âˆ˜ p? âŸ©
 % = k âˆ˜ âŸ¨ [f,h] , [g, i] âŸ© âˆ˜ p?
 % = k âˆ˜ [ âŸ¨f, gâŸ© , âŸ¨h, iâŸ© ] âˆ˜ p?
 % = [ k âˆ˜ âŸ¨f, gâŸ© , k âˆ˜ âŸ¨h, iâŸ© ] âˆ˜ p?
 % = p â†’ k âˆ˜ âŸ¨f, gâŸ© , k âˆ˜ âŸ¨h, iâŸ©

 \eqn{conditional-Abides-Distributivity}{ âŸ¨ (p â†’ f,h) , (p â†’ g, i) âŸ©  \eqs p â†’ âŸ¨f, gâŸ© , âŸ¨h, iâŸ© }

 \eqn{conditional-Idempotency}{p â†’ f,f \eqs f}

 \eqn{conditional-âŸ¨âŸ©-Distributivity}{ âŸ¨ f, (p â†’ g, h)âŸ© \quad=\quad p â†’ âŸ¨f, gâŸ©, âŸ¨f, hâŸ© }

 \eqn{conditional-Ã—-Distributivity}{ (p â†’ g, h) Ã— f \quad=\quad p âˆ˜ \fst â†’ g Ã— f, h Ã— f}

 \endeqns
 #+END_EXPORT

 \room

 *As well as their duals!*

** COMMENT Wow --another example-- involving conditionals
  Just as in the case of proving /tails/ is a natural transformation *without* using
  any implementing definitions, here's a similar example.

  \room

  Here's an illustration of how /smart/ pointfree algebra can be in reasoning about
  functions that /one does not actually defined explicitly/.

  It also shows how relevant the /natural properties/ are.

  The issue is that our definition of a guard, \ref{?-Defn}, is pointwise and most
  likely unsuitable to prove facts such as, for instance,
  \[ p? âˆ˜ f = (f + f) âˆ˜ (p âˆ˜ f)? \]
  Thinking better, instead of `inventing' \ref{?-Defn}, we might --and perhaps should!--
  have defined
  \eqn{two-Defn}{2  \;â‰…\; 1 + 1}
  \eqn{double-Defn}{2 Ã— A \;â‰…\; A + A}
  \eqn{?-Defn-Pointfree}{ p? = double âˆ˜ âŸ¨p, \IdâŸ© }

  which actually express rather closely our strategy of switching from products
  to coproducts in the definition of $(p?)$.

  In particular, *we do not need to define /double/ explicitly*.
  From its type we immediately infer its natural, or free, property:
  \[ double âˆ˜ (\Id Ã— f) \eqs (f + f) âˆ˜ double \]

  It turns out that this is the /knowledge/ we need about $double$
  in order to prove the above mentioned property.

  \room

  The less one has to write to solve a problem, the better.
  One saves time and one's brain, adding to productivity.
  This is often called /elegance/ when applying a scientific method.
** Importance of Compositional Combinators                           :ignore:

 # Paraphrasing Oliveira, Â§2.13
 \vspace{3ex}
 The compositional combinators put forward here are equipped with a concise /set of properties/
 which enable programmers to transform programs, reason about them, and perform useful calculations.
 This raises a /programming methodology/ which is scientific and stable.

** Compact Notation

 Why do people look for compact notations? A compact notation leads to
 shorter documents (less lines of code in programming) in which patterns are easier
 to identify and to reason about. Properties can be stated in clear-cut, one-line long
 equations which are easy to memorize. And diagrams such as (2.4) can be easily
 drawn which enable us to visualize maths in a graphical format.

* COMMENT ToDo's
Include:
+ Page 23: Associativity of ~C~ product structs.
+ Page 25: Realisation of sums in ~C~.
+ Pages 31-32: Datatypes, void, unit, bool, maybe.
+ Page 51: Table 2.1 Abstract notation versus programming language data-structures.

+ Go do all the exercises --we have a handy cheat-sheet to help! Let the exercises reinforce the learning!
  - Some of the insightful, or fun ones, maybe include here ;-)
  - Â§2.19 is all exercises!

+ Also, notice that we have natural transformations
   $\initial{-} : \K 0 \natTo \mathsf{Id}$
   and $\final{-} : \mathsf{Id} \natTo \K 1$

   Claim?

   the cat has a final element iff identity endofunctor has a colimit.
   ie the final element is the union of all elements and so is the
   top or maximal element ;)

   $1 \cong CoLim \, \mathsf{Id}$
   where $\final{-} = \gamma$ is the witnessing family?

+ Provide a /calculational/ proof of: if a category J has a terminal object 1,
   then the colimit of any functor F:Jâ†’C is just F(1), with the cocone defined by the maps F(Ï„j):F(j)â†’F(1).

   Hint: https://math.stackexchange.com/questions/1210440/given-a-mathcalc-rightarrow-mathcale-when-does-operatornamecolim
** COMMENT TODO Generalise Exchange Law to arbitrary colimits and limits xD

* COMMENT Duality: Sums & Products :uniform_notation:artefact:

#  \NTHM{}{Principle of Duality}{
#    A statement $S$ is true about $\mathcal{C}$
#    iff it's dual $S[\fcmp \,:=\, \circ]$ is true about $\mathcal{C}^{op}$. }
#
#+BEGIN_EXPORT latex
\def\fst{\mathsf{fst}}
\def\snd{\mathsf{snd}}
#+END_EXPORT

#+LaTeX_HEADER: \usepackage{stackengine}
#
# \topinset{*}{O}{1pt}{}
#
#+LaTeX: \def\composition{ \topinset{ï¹”}{âˆ˜}{0.4pt}{} }

In category theory there are two popular notations for composition,
$f ï¹” g = g âˆ˜ f$, and there are two arrow notations $A â†’ B \;=\; B \leftarrow A$,
known as the â€œforwardsâ€ and â€œbackwardsâ€ notations.

\room

Some people prefer one notation and stick with it; however having both in-hand
allows us to say: The /dual/ of a categorical statement formed with
$ï¹”,â†’$ is obtained by syntactically replacing these two with $âˆ˜, \leftarrow$ respectively
while leaving variables and $\Id$'s alone.

\room

#+LaTeX: \def\dual{\mathsf{dual}}
#
#
For example, applying this process to sums yields /products/:
\vspace{-0.5em}
#+begin_calculation latex
   h = âŸ¨f, gâŸ©
\step{  We define products as dual to sums }
   h = \dual [f, g]
\step{  dual operation definition }
  \dual\left( h = [f,g] \right)
\step{  []-Char and Leibniz }
  \dual\left( \inl ï¹” h = f  \lands  \inr ï¹” h = g\right)
\step{  dual operation definition }
  \dual\left(\inl ï¹” h\right) = f  \lands  \dual\left(\inr ï¹” h\right) = g)
\step{  dual operation definition }
  \dual \text{ } \inl âˆ˜ h = f  \lands  \dual \text{ }  \inr âˆ˜ h = g
\step{  Define: $\fst = \dual\, \inl$, $\snd = \dual\, \inr$ }
  \fst âˆ˜ h = f  \lands  \snd âˆ˜ h = g
\step{  Switch back to ï¹”-notation }
   h ï¹” \fst = f  \lands  h ï¹” \snd = g
#+end_calculation

# Dualising the other sum artefacts yields:
/$(\fst, \snd, A Ã— B)$ form a product of A and B/ if there is an operation
$âŸ¨-,-âŸ©$ satisfying the Char and Type laws below; from which we obtain a host of corollaries.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\langle\rangle$-Type}{f : C â†’ A \lands g : C â†’ B \impliesS âŸ¨f, gâŸ© : C â†’ A Ã— B}

\eqn{$\langle\rangle$-Char}{ x ï¹” \fst = f \lands x ï¹” \snd = g \equivS x = âŸ¨f, gâŸ© }

\eqn{$\langle\rangle$-Cancellation; $\langle\rangle$-Self}{ âŸ¨f, gâŸ© ï¹” \fst = f \landS âŸ¨f, gâŸ© ï¹” \snd = g}

\eqn{$\langle\rangle$-Id}{ âŸ¨\fst, \sndâŸ© = \Id}

\eqn{$\langle\rangle$-Unique}{ x ï¹” \fst = y ï¹”\fst  \lands x; \snd = y ï¹” \snd \impliesS x = y}

\eqn{$\langle\rangle$-Fusion}{ x ï¹” âŸ¨f , gâŸ© = âŸ¨xï¹”f, x ï¹” gâŸ© }

\eqn{$\langle\rangle$-Functor-Dist}{F \, âŸ¨f, gâŸ©_ğ’ = âŸ¨F \, f , F \, gâŸ©_ğ’Ÿ \qquad\text{ where } F : ğ’ â†’ ğ’Ÿ}

\endeqns
#+END_EXPORT

\room

These are essentially a re-write of the sum laws; let's write the next set of laws only once.

\room

Let the tuple â€œâŸ… âŸ†, â‹†, $l$, $r$, $\composition$â€ be either
â€œâŸ¨ âŸ©, Ã—, $\fst$, $\snd$, $âˆ˜$â€ or â€œ[ ], +, $\inl$, $\inr$, $ï¹”$â€.

\room

For categories in which sums and products exist, we define for $f : A â†’ B$ and $g : C â†’ D$,

\begineqns

\eqn{$\star$-Definition}{ f â‹† g = âŸ… f \composition l, g \composition râŸ† : A â‹† C â†’ B â‹† D}

\eqn{$l,r$-Naturality}{ l \composition (f â‹† g) = f \composition l \landS r \composition (f â‹† g) = g \composition r }

\eqn{Extensionality}{ âŸ…l \composition h, r \composition hâŸ† = h}

\eqn{Absorption}{ (f â‹† g) \composition âŸ…h, jâŸ† = âŸ…f \composition h, g \composition jâŸ† }

\eqn{$\star$-Bi-Functoriality}{ \Id â‹† \Id = \Id \landS (f â‹† g) \composition (h â‹† j) = (f \composition h) â‹† (g \composition j)}

\eqn{Structural Equality}{ âŸ…f,gâŸ† = âŸ…h, jâŸ† \equivS f = h \lands g = j }

\eqn{Interchange Rule}{ âŸ¨[f,g], [h,j]âŸ© = [âŸ¨f,hâŸ©,âŸ¨g,jâŸ©] }

\endeqns

\room

Notice that the last law above is self-dual.

* COMMENT more
  + Naturality actually corresponds to the idea of parametricity in functional
programming: rev is defined uniformly for all sets. It is good to look at a
counterexample but actually we cannot define one because in type theory and
in functional programming all functions are parametric and hence natural. To
get our hands on an unnatural function let us allow for the moment to define a
weird function which analyses a set, that is we define

weird : Î A:SetList A ! List A
weird_Bool = rev_Bool
weird_A = id_{List A} for all other A : Set

That is weird reverses lists of booleans but is the identity every else. weird
has the same type as rev hence we have to check the same naturality square.
It is unnatural.

* COMMENT README

C-c C-c: evalute src block

#+NAME: make-readme
#+BEGIN_SRC emacs-lisp :results none
(with-temp-buffer
    (insert
    "#+EXPORT_FILE_NAME: README.md
     #+HTML: <h1> CatsCheatSheet </h1>
     # OPTIONS: toc:nil

     This project is to contain a listing of common theorems in elementary category theory.

     *The repo contains other articles I've written on Category Theory;*
     *which may be read in a blog-format at:*
     https://alhassy.github.io/blog/categories/#categorytheory

     *The listing sheet, as PDF, can be found [[https://github.com/alhassy/CatsCheatSheet/blob/master/CheatSheet.pdf][here]]*,
     while below is an unruly html rendition.

     This reference sheet is built around the system https://github.com/alhassy/CheatSheet

     #+TOC: headlines 2
     #+INCLUDE: CheatSheet.org
    ")
    ;; (set-visited-file-name "ReadIt2.md")
    (org-mode)
    (org-md-export-to-markdown)
)
#+END_SRC

* COMMENT footer

(find-file "CheatSheet.el")

# Local Variables:
# eval: (org-babel-tangle)
# eval: (progn (org-babel-goto-named-src-block "make-readme") (org-babel-execute-src-block) (outline-hide-sublevels 1))
# eval: (load-file "CheatSheet.el")
# compile-command: (my-org-latex-export-to-pdf)
# End:
