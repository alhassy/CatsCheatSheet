#+TITLE: Reference Sheet for Elementary Category Theory
# SUBTITLE: Cheat Sheet Template
# DATE: << Spring 2018 >>
# When we don't provide a date, one is provided for us.
#+AUTHOR: [[http://www.cas.mcmaster.ca/~alhassm/][Musa Al-hassy]] @@latex:{\tiny\hspace{5.5em}\url{https://github.com/alhassy/CatsCheatSheet}}@@
#+EMAIL: alhassy@gmail.com
#+DESCRIPTION: This document is written by Musa Al-hassy for his learning in the spring of 2018.
#+STARTUP: hideblocks
#+STARTUP: overview
#+TODO: TODO { BEGIN-IGNORE(b) END-IGNORE(e) } | DONE(d)

# Important shortcuts:
# f7 preview changes
# f8 commit each change
# f9 push changes

#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage{newunicodechar}
#+LATEX_HEADER: \newunicodechar{ï¹”}{\ensuremath{\raisebox{0.4ex}{\tiny \,;\,}}}  %% forward composition ï¹”

#+LATEX_HEADER: \usepackage{calculation} 

#+INCLUDE: ~/Dropbox/MyUnicodeSymbols.org
#+INCLUDE: CheatSheet/CheatSheetSetup.org

# https://en.wikipedia.org/wiki/Linguistic_relativity#Programming_languages

* COMMENT In Defence of Backwards Composition :for_future_self:

For some time, I've preferred forwards composition as in Z notation.
Especially since my academic setting has people using that notation as well.

However here are some reasons I've decided to switch to the traditional format:

+ It is much more ubiquitous; therefore easier to understand without the necessary simple explanation.

+ It is Haskell's notion of function composition, therefore allowing the immediate translation
  of categorically derived programs to be implemented in Haskell. It also permits a nice
  implementation of some categorical concepts without must trouble.

+ I want to motivate the â€œcalculational styleâ€, so I do not want to deviate in orthogonal
  directions.

* COMMENT TODO commit info                                           :ignore:

#+begin_tiny
  \vspace{-3.5em}
  \hspace{19em} [[https://github.com/alhassy/CatsCheatSheet/blob/984969814031f56f53732419f297e668d94df863/CheatSheet.pdf][~commit 9849698~]]
  \vspace{2em}
#+end_tiny

* LaTeX Setup :ignore:

#+BEGIN_EXPORT latex
\def\providedS{ \qquad\Leftarrow\qquad }

\def\impliesS{ \qquad\Rightarrow\qquad }

\def\landS{ \qquad\land\qquad }
\def\lands{ \quad\land\quad }

\def\eqs{ \quad=\quad}

\def\equivs{ \quad\equiv\quad}
\def\equivS{ \qquad\equiv\qquad}

\def\begineqns{ \begingroup\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt} }
\def\endeqns{ \endgroup }
% \def\endeqns{ \endgroup \setlength{\belowdisplayskip}{2pt} } % put belowspace back to desired setting
#+END_EXPORT

# See defn-Type, below for an expanded usage; \eqn{name}{formula}
# LaTeX: \setlength{\abovedisplayskip}{5pt} \setlength{\belowdisplayskip}{2pt}
#+LaTeX: \def\eqn#1#2{ \begin{flalign*} #2 && \tag*{\sc #1} \label{#1} \end{flalign*}  }

# LATEX_HEADER: \setlength{\parskip}{1em}
# LaTeX: \setlength{\parskip}{0.5em}

#+LaTeX: \def\room{\vspace{0.5em}}

#+BEGIN_EXPORT latex
\def\Obj{\mathsf{Obj}}
\def\Hom{\mathsf{Hom}}
\def\src{\mathsf{src}}
\def\tgt{\mathsf{tgt}}
\def\Id{\mathsf{Id}}

\def\bin{I\!\!I}
#+END_EXPORT

* Categories
#
# The morphisms are a "dependent type" ;-)
#
A *category* ğ’ consists of a collection of â€œobjectsâ€ $\Obj\, ğ’$,
  a collection of  â€œ(homo)morphismsâ€ $\Hom_ğ’(a,b)$ for any $a,b : \Obj\,ğ’$
  --also denoted â€œ$a \,\to_ğ’\, b$â€--,
  an operation $\Id$ associating a morphism $\Idâ‚ : \Hom(a,a)$ to each object $a$,
  and a dependently-typed â€œcompositionâ€ operation
  $\_âˆ˜\_ : âˆ€\{A \, B \, C : \Obj\} â†’ \Hom(B,C) â†’ \Hom(A,B) â†’ \Hom(A,C)$
  that is required to be associative with $\Id$ as identity.
 
# For other approaches see https://tex.stackexchange.com/a/12035/69371
#
# As we can see from \eqref{defn-Type}\ldots
#

#+LaTeX: \room
 
  It is convenient to define a pair of operations $\src, \tgt$ from morphisms to objects
  as follows:
\begin{flalign*}
    f : X \to_ğ’ Y \quad\equiv\quad \mathsf{src}\; f = X \;\land\; \mathsf{tgt}\; f = Y 
   &&
   \tag*{$\src,\tgt$-Definition}
   \label{src-tgt-Definition}
\end{flalign*}
 
Instead of $\Hom_ğ’$ we can instead assume primitive a ternary relation
$\_:\_â†’_ğ’\_$ and regain $\Hom_ğ’$ precisely when the relation is functional
in its last two arguments:
\eqn{type-Unique}{f : A \to_ğ’ B \;\;\land\;\; f : A' \to_ğ’ B' \;\implies\; A=A' \;\land\; B=B'}
When this condition is dropped, we obtain a /pre-category/; e.g., the familiar /Sets/
is a pre-category that is usually treated as a category by making morphisms
contain the information about their source and target: $(A, f, B) : A â†’ B$
rather than just $f$.
\newline
 /This is sometimes easier to give than Hom! C.f. Alg(F)./
\room

Here's an equivalence-preserving property that is useful in algebraic calculations,
#+LaTeX: \eqn{Composition}{ f : A â†’ B \lands g : B â†’ A \equivs g âˆ˜ f : A â†’ A \lands f âˆ˜ g : B â†’ B}

# A categorical statement is an expression built from notations for objects,
# typing, morphisms, composition, and identities by means of the usual logical
# connectives and quantifications and equality.
#

Example Categories.
+ Matrices with real number values determine a category whose objects are the natural numbers,
  morphisms $n â†’ m$ are $n Ã— m$ matrices, $\Id$ is the identity matrix, and composition
  is matrix multiplication.
+ Each preorder determines a category: The objects are the elements
  and there is a morphism $a â†’ b$ named, say, â€œ$(a, b)$â€, precisely when $a \leq b$;
  composition boils down to transitivity of $\leq$.
+ Each digraph determines a category: The objects are the nodes
  and the paths are the morphisms typed with their starting and ending node.
  Composition is catenation of paths and identity is the empty path.
+ Suppose we have an `interface', in the programming sense,
  of constant, function, and relation symbols --this is also called a /signature/.

  Let ğ’¯ be any collection of sentences in the first-order language of signature $\Sigma$.
  Then we can define a category $\mathsf{Mod}\,ğ’¯$ whose objects are
  implementations of interface $\Sigma$ satisfying constraints ğ’¯, and whose morphisms
  are functions that preserve the $\Sigma$ structure. 
  Ignoring ğ’¯, gives us `functor algebras'.

  Particular examples include monoids and structure-preserving maps between them;
  likewise digraphs, posets, rings, etc and their homomorphisms.
  
\room

Even when morphisms are functions, the objects need not be sets:
Sometimes the objects are /operations/ --with an appropriate definition
of typing for the functions. The categories of /F/-algebras are an example
of this.


\newpage

* â€œGluingâ€ Morphisms Together

Traditional function application is replaced by the more generic concept of
functional /composition/ suggested by morphism-arrow chaining:
Whenever we have two morphisms such that the target type of one
of them, say $g : B â† A$ is the same as the source type of the other,
say $f : C â† B$ then â€œ$f$ after $g$â€, their /composite morphism/,
$f âˆ˜ g : C â† A$ can be defined. It â€œgluesâ€ $f$ and $g$ together,
â€œsequentiallyâ€:
#+BEGIN_EXPORT latex
\eqn{composition-Type}{ 
  C \overset{f}{\longleftarrow} % B \lands 
  B \overset{g}{\longleftarrow} A \impliesS
  C \overset{\;f âˆ˜ g}{\longleftarrow} A
}
#+END_EXPORT

Composition is the basis for gluing morphisms together to build more complex morphisms.
However, not every two morphisms can be glued together by composition.

\room

Types provide the interface for putting morphisms together to obtain more complex functions.

\room

A /split/ arises wherever two morphisms do not compose but share the same source.
  - Since they share the same source, their outputs can be paired: $c â†¦ (f\, c, g\, c)$.
  - This duplicates the input so that the functions can be executed in â€œparallelâ€ on it.

\room

A /product/ appears when there is no explicit relationship between the types of the morphisms.
  - We regard their sources as projections of a product, whence they can be seen as /splits/.
  - This $(c, d) â†¦ (f\, c, g\, d)$ corresponds to the â€œparallelâ€ application of $f$ and $g$, 
     each with its /own/ input.

\room

An /either/ arises wherever two morphisms do not compose but share the same target.
  - Apply $f$ if the input is from the â€œ$A$ sideâ€ or apply $g$ if it is from the â€œ$B$ sideâ€.
  - This is a â€œcase analysisâ€ of the input with branches being either $f$ or $g$.

\room

A /sum/ appears when there is no explicit relationship between the types of the morphisms.
  - We regard their targets as injections into a sum, whence they can be seen as /eithers/.

\room

#+LaTeX: \def\transpose#1{ \overline{#1} }
A /transpose/ arises when we need to combine a binary morphism with a unary morphism.
  - I.e., it arises when a composition chain is interrupted by an extra product argument.
  - Express $f$ as a /C/-indexed family, $f_c : A â†’ B$, of morphisms such that applying a function at any index
    behaves like $f$; i.e., $f_c \, a = f(c, a)$. Each $f_c$ can now be composed with $g$.
    Let $\transpose{(\;)}$ denote the operation $f â†¦ f_c$.

\vspace{-0.5em}

#+BEGIN_EXPORT latex
\begineqns

\eqn{split-Type}{ A \overset{f}{\longleftarrow} C \overset{g}{\longrightarrow} B 
 \hspace{5.8em}\equivS A Ã— B \overset{\;\;âŸ¨f,gâŸ©}{\xleftarrow{\hspace*{0.5cm}}} C }

\eqn{$\times$-Type}{ A \overset{f}{\longleftarrow} C \lands B \overset{g}{\longleftarrow} D
 \hspace{1.9em} \equivS A Ã— B \overset{f Ã— g}{\xleftarrow{\hspace*{0.5cm}}} C Ã— D }

\eqn{either-Type}{ A \overset{f}{\longrightarrow} C \overset{g}{\longleftarrow} B
 \hspace{5.9em}\equivS A + B \overset{[f,g]}{\xrightarrow{\hspace*{0.5cm}}} C }

\eqn{+-Type}{ A \overset{f}{\longrightarrow} C \lands B \overset{g}{\longrightarrow} D
 \hspace{2em} \equivS A + B \overset{f + g}{\xrightarrow{\hspace*{0.5cm}}} C + D }

\eqn{transpose-Type}{
  B \overset{f}{\longleftarrow}{C Ã— A} 
  \hspace{6.9em} \equivS
  B^A \overset{\transpose{f}}{\longleftarrow} C
  }

\eqn{transpose-combination}{
  B \overset{f}{\longleftarrow}{C Ã— A} 
  \lands
  C \overset{g}{\longleftarrow} D
  \impliesS
  B^A \overset{\transpose{f} âˆ˜ g}{\xleftarrow{\hspace*{1cm}}} D
  }
\endeqns
#+END_EXPORT

\vspace{1em}

* Functors

A *functor* /F : ğ’œ â†’ â„¬/ is a pair of mappings, denoted by one name,
from the objects, and morphisms, of ğ’œ to those of â„¬ such that
it respects the categorical structure:

#+BEGIN_EXPORT latex 
{\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

\eqn{functor-Type}{F\, f : F\, A \to_â„¬ F\, B \quad\Leftarrow\quad f : A \to_ğ’œ B }

\eqn{Functor}{F\, \Id_A \;=\; \Id_{F\, A}} 

\eqn{Functor}{F\, (f âˆ˜ g) \;=\; F\, f \circ F\, g}

}
#+END_EXPORT

\vspace{1em}

The two axioms are equivalent to the single statement that 
/functors distribute over finite compositions, with $\Id$ being the empty composition:/
\[ F(f_0 âˆ˜ \cdots âˆ˜ f_{n-1}) \;=\; F\, f_0 âˆ˜ \cdots âˆ˜ F\, f_{n-1} \]

Use of Functors.
+ In the definition of a category, â€œobjectsâ€ are â€œjust thingsâ€ for which no internal
  structure is observable by categorical means --composition, identities, morphisms, typing.

  Functors form the tool to deal with â€œstructuredâ€ objects

  Indeed in ğ’®â„¯ğ“‰ the aspect of a structure is that it has â€œconstituentsâ€, and that it is possible
  to apply a function to all the individual constituents; this is done by
  /F f : F A â†’ F B/.

+  For example, let $\bin A = A Ã— A$ and $\bin f = (x, y) â†¦ (f\, x, f\, y)$.
  So $\bin$ is or represents the structure of pairs; $\bin\, A$ is the set of pairs of /A/,
  and $\bin\, f$ is the function that applies /f/ to each constituent of a pair.

  - A /binary operation on A/ is then just a function $\bin A â†’ A$;
    in the same sense we obtain /F-ary operations/.

+ Also, /Seq/ is or represents the structure of sequences; /Seq A/ is the structure of sequences
  over /A/, and /Seq f/ is the function that applies /f/ to each constituent of a sequence.

+  Even though /F A/ is still just an object, a thing with no observable internal structure, the
  functor properties enable to exploit the â€œstructureâ€ of /F A/ by allowing us to â€œapplyâ€
  a /f/ to each â€œconstituentâ€ by using /F f/.

\vspace{1em}

Category $ğ’œlâ„Š(F)$
+ For a functor /F : ğ’œ â†’ ğ’Ÿ/, this category has /F-algebras/, /F/-ary operations in ğ’Ÿ as, objects
  -- i.e., objects are ğ’Ÿ-arrows $F\, A â†’ A$ --
  and /F/-homomorphisms as morphisms, and it inherits composition and identities from ğ’Ÿ.

  #+BEGIN_EXPORT latex 
  {\setlength{\abovedisplayskip}{-1pt}\setlength{\belowdisplayskip}{-1pt}

  \eqn{defn-Homomorphism}{f : âŠ• â†’_F âŠ— \quad\equiv\quad âŠ• ï¹” f = F\, f ï¹” âŠ— }

  \eqn{id-Homomorphism}{ \Id : âŠ• â†’_F âŠ• }

  \def\providedS{\qquad\Leftarrow\qquad}

  \eqn{comp-Homomorhism}{ g âˆ˜ f : âŠ™ â†_F âŠ• \providedS g : âŠ™ â†_F âŠ— \;\land\; f : âŠ— â†_F âŠ•}
  }
  #+END_EXPORT

  Note that category axiom \eqref{unique-Type} is not fulfilled since a function can be
  a homomorphism between several distinct operations. However, we pretend it is a category
  in the way discussed earlier, and so the carrier of an algebra is fully determined by
  the operation itself, so that the operation itself can be considered the algebra.

  #+BEGIN_CENTER
  /\ref{comp-Homomorhism} renders a semantic property as a syntactic condition!/
  #+END_CENTER

\vspace{1em}

+ A *contravariant functor* ğ’ â†’ ğ’Ÿ is just a functor /ğ’áµ’áµ– â†’ ğ’Ÿáµ’áµ–/.
+ A *bifunctor* from ğ’ to ğ’Ÿ is just a functor /ğ’Â² â†’ ğ’Ÿ/.

* Naturality

A natural transformation is nothing but a structure preserving map between functors.
â€œStructure preservationâ€ makes sense, here, since we've seen already that a functor
is, or represents, a structure that objects might have.

\room

As discussed before for the case /F : ğ’ â†’ ğ’®â„¯ğ“‰/, each /F A/ denotes a structured set
and /F/ denotes the structure itself.

# \room
#
#+LaTeX: \paragraph{\footnotesize \textnormal{Example Structures}}\label{SeqPair-is-Pair-Seq}
\hspace{-1em}:
$\bin$ is the structure of pairs, /Seq/ is the structure of sequences,
/Seq Seq/ the structure of sequences of sequences, 
$\bin \, Seq$ the structure of pairs of sequences --which is naturally isomorphic
to $Seq \, \bin$ the structure of sequences of pairs!--, and so on.

\room

A â€œtransformationâ€ from structure /F/ to structure /G/ is a family of functions \newline
$Î· : âˆ€\{A\} â†’ F\, A â†’ G\, A$; and it is â€œnaturalâ€ if each $Î·_A$ doesn't affect the /constituents/
of the structured elements in /F A/ but only reshapes the structure of the elements,
from an /F/-structure to a /G/-structure.

\vspace{0em}

#+BEGIN_CENTER
/Reshaping the structure by Î· commutes with subjecting the constituents to an arbitrary morphism./
#+END_CENTER
# That is, $F\, f ï¹” t_B \;=\; t_A ï¹” G\, f$ for all /f : A â†’ B./

\vspace{-2em}
#+LaTeX: \eqn{ntrf-Def}{ Î· : F â†’Ì£ G \quad\equiv\quad âˆ€ f \;â€¢\; Î·_{\tgt\, f} âˆ˜ F\, f \;=\; G\, f âˆ˜ Î·_{\src\, f} }

This is `naturally' remembered: Morphism $Î·_{\tgt\, f}$ has type $F (\tgt\, f) â†’ G(\tgt\, f)$ and therefore
appears at the target side of an occurrence of /f/; similarly $Î·_{\src\, f}$ occurs at the source side of an /f/.
/Moreover/ since Î· is a transformation /from/ /F/ to /G/, functor /F/ occurs at the source side of an Î·
and functor /G/ at the target side.

\room

+ One also says /Î·â‚ is natural in/ its parameter /a/.

+ If we take $G = \Id$, then natural transformations $F â†’Ì£ \Id$ are precisely /F/-homomorphisms.
+ Indeed, a natural transformation is a sort-of homomorphism in that the image of a morphism
  after reshaping is the same as the reshaping of the image.

\room

Example natural transformations
+ /rev : Seq â†’Ì£ Seq : [aâ‚, â€¦, aâ‚™] â†¦ [aâ‚™, â€¦, aâ‚]/
  reverses its argument thereby reshaping a sequence structure into a sequence structure without affecting the constituents.

+ /inits : Seq â†’Ì£ Seq Seq : [aâ‚, â€¦, aâ‚™] â†¦ [[], [aâ‚], â‹¯, [aâ‚, â€¦, aâ‚™]]/
  yields all initial parts of its argument
  thereby reshaping a sequence structure into a sequence of sequences structure, not affecting
  the constituents of its argument.

\room

#+BEGIN_EXPORT latex
\begineqns

\eqn{ntrf-Ftr}{ JÎ· : JF â†’Ì£ JG \providedS Î· : F â†’Ì£ G \quad \text{ where } (JÎ·)_A â‰” J(Î·_A) }

\eqn{ntrf-Poly}{ Î·K : FK â†’Ì£ GK  \hspace{-2ex}\providedS Î· : F â†’Ì£ G \quad \text{ where } (Î·K)_A â‰” Î·_{(K\, A) } }

\eqn{ntrf-Id}{ \Id_F : F â†’Ì£ F \text{\hspace{12em} where } (\Id_F)_A â‰” \Id_{(F\, A)} }

\eqn{ntrf-Compose}{ Î· âˆ˜ Îµ : H â†Ì£ F \hspace{2ex}\providedS Î· : H â†Ì£ G \lands Îµ : G â†Ì£ F
\\ \text{ where } (Î· âˆ˜ Îµ)_A = Î·_A âˆ˜ Îµ_A
 }

\endeqns
#+END_EXPORT

\room

*Category â„±ğ“Šğ“ƒğ’¸(ğ’, ğ’Ÿ)*
consists of functors /ğ’ â†’ ğ’Ÿ/ as objects and natrual transformations between them as objects.
The identity transformation is indeed an identity for transformation composition, which is associative. 

\room

*Heuristic* To prove $Ï† = Ï†â‚ âˆ˜ â‹¯ âˆ˜ Ï†â‚™ : F â†’Ì£ G$ is a natural transformation, it suffices
to show that each $Ï†áµ¢$ is a natural transformation.
E.g., without even knowing the definitions, naturality of
/tails = Seq rev âˆ˜ inits âˆ˜ rev/ can be proven --just type checking!
# thanks to \eqref{ntrf-Compose}!

\iffalse
 + Theorem \eqref{ntrf-Compose} renders proofs of semantic properties to be trivial type checking!
 + E.g., It's trivial to prove /tails = rev ï¹” inits ï¹” Seq rev/ is a natural transformation
   by type checking, but to prove the naturality equation by using the naturality equations of
   /rev/ and /inits/ --no definitions required-- necessitates more writing, and worse: Actual thought! 
\fi

* Adjunctions

An adjunction is a particular one-one correspondence between different kinds of
morphisms in different categories.

\room

An *adjunction* consists of two functors $L : ğ’œ â†’ â„¬$ and $R : â„¬ â†’ ğ’œ$,
as well as two (not necessarily natural!) transformations
$Î· : \Id â†’ RL$ and $Îµ : LR â†’ \Id$ such that

\vspace{-1em}

#+BEGIN_EXPORT latex
\eqn{Adjunction}{\text{\tiny Provided $f : A â†’_ğ’œ R B$ and $g : L A â†’_â„¬ B$ }
 \\ f = R g âˆ˜ Î·_A \equivS Îµ_B âˆ˜ L f = g
}
#+END_EXPORT

Reading right-to-left: In the equation $Îµ_B âˆ˜ L f = g$ there is a unique solution to the unknown $f$.
Dually for the other direction.

\room

That is,
/each L-algebra g is uniquely determined --as an L-map followed by an Îµ-reduce--/
/by its restriction to the adjunction's unit Î·./

\room

A famous example is â€œFree âŠ£ Forgetfulâ€, e.g. to /define/ the list datatype, for which the above
becomes: Homomorphisms on lists are uniquely determined, as a map followed by a reduce,
by their restriction to the singleton sequences.

\room

We may call $f$ the restriction, or lowering, of $g$ to the â€œunital caseâ€
and write $f = âŒŠgâŒ‹ = R g âˆ˜ Î·_A$. Also, we may call $g$ the extension, or raising,
of $f$ to an /L/-homomorphism and write $g = âŒˆfâŒ‰ = Îµ_B âˆ˜ L f$. The above equivalence
now reads:

#+BEGIN_EXPORT latex
\begineqns

\eqn{Adjunction-Inverse}{f = âŒŠgâŒ‹ \equivS âŒˆfâŒ‰ = g}

\eqn{lad-Type}{âŒŠgâŒ‹_{A,B} = R g âˆ˜ Î·_A \; : \; A â†’_ğ’œ R B
 \text{ where } g : L A â†’_â„¬ B
 }

\eqn{rad-Type}{âŒˆfâŒ‰_{A,B} = Îµ_B âˆ˜ L f \; : \; L A â†’_â„¬ B
 \text{ where } f : A â†’_ğ’œ R B
 }

\endeqns
#+END_EXPORT

\room
\vspace{1ex} 
Note that âŒˆ is like `r' and the argument to âŒˆâŒ‰ must involve the /R/-ight adjoint in its type;
# likewise for 
#+LaTeX: {\textbf L}ad takes morphisms involving the {\textbf L}eft adjoint ;)

\room

This equivalence expresses that `lad' $âŒŠâŒ‹$, from \emph{l}eft \emph{ad}jungate,
and `rad' $âŒˆâŒ‰$, from \emph{r}ight \emph{ad}jungate, are each other's inverses
and constitute a correspondence between certain morphisms.
/Being a bijective pair, lad and rad are injective, surjective, and undo one another./

\room

We may think of â„¬ as having all complicated problems so we abstract
away some difficulties by \emph{r}aising up to a cleaner, simpler, domain
via rad âŒˆâŒ‰; we then solve our problem there, then go back \emph{down} to
the more complicated concrete issue via âŒŠâŒ‹, lad.
\newline
( E.g., â„¬ is the category of monoids, and ğ’œ is the category of sets; $L$ is the list functor. )

# Some useful results,
#+BEGIN_EXPORT latex
\begineqns

\eqn{ntrf-Adj}{\text{The Î· and Îµ determine each other and they are \emph{natural} transformations.}}

\vspace{2ex}
â€œzig-zag lawsâ€ The unit has a post-inverse while the counit has a pre-inverse:

\eqn{unit-Inverse}{ \Id = R Îµ âˆ˜ Î·}

\eqn{Inverse-counit}{ \Id = Îµ âˆ˜ L Î·} 

\vspace{2ex}
The unit and counit can be regained from the adjunction inverses,

\eqn{unit-Def}{ Î· = âŒŠ\IdâŒ‹}

\eqn{counit-Def}{ Îµ = âŒˆ\IdâŒ‰ }

\vspace{2ex}
Lad and rad themselves are solutions to the problems of interest, \eqref{Adjunction}.

\eqn{lad-Self}{Îµ âˆ˜ L âŒŠgâŒ‹ = g}

\eqn{rad-Self}{R âŒˆfâŒ‰ âˆ˜ Î· = f }

\vspace{2ex}
The following laws assert a kind of
monoic-ness for Îµ and a kind of epic-ness for Î·.
Pragmatically they allow us to prove an equality
by shifting to a possibly easier equality obligation.

\eqn{lad-Unique}{ R g âˆ˜ Î· = R gâ€² âˆ˜ Î· \equivS g = gâ€²}

\eqn{rad-Unique}{Îµ âˆ˜ L f \,= Îµ âˆ˜ L fâ€² \,\equivS f = fâ€²}

\vspace{2ex}
Lad and rad are natural transformations in the category $â„±ğ“Šğ“ƒğ’¸(ğ’œáµ’áµ– Ã— â„¬, ğ’®â„¯ğ“‰)$ realising
$(L X â†’ Y) â‰… (X â†’ G Y)$ where $X, Y$ are the first and second projection functors
and $(-â†’-) : ğ’áµ’áµ– Ã— ğ’ â†’ ğ’®â„¯ğ“‰$ is the hom-functor such that $(f â†’ g) h = g âˆ˜ h âˆ˜ f$.
\\ By extensionality in ğ’®â„¯ğ“‰, their naturality amounts to the laws:

\eqn{lad-Fusion}{âŒŠ y âˆ˜ g âˆ˜ L x  âŒ‹ \;=\; R y âˆ˜ âŒŠgâŒ‹ âˆ˜ x }

\eqn{rad-Fusion}{âŒˆ R y âˆ˜ f âˆ˜  x âŒ‰ \;=\; y âˆ˜ âŒˆfâŒ‰ âˆ˜ L x }

\room
\endeqns
#+END_EXPORT

Also,
+ Left adjoints preserve colimits such as initial objects and sums.
+ Right adjoints preserve limits such as terminal objects and products.


* Constant Combinators

#+LaTeX: \def\const#1{ \underline{#1} }

Opposite to the identity functions which do not lose any information,
we find functions which lose all, or almost all, information.
Regardless of their input, the output of these functions is always the
same value.

# \room
#
#+LaTeX: \def\K{\mathcal{K}}
#+LaTeX: \paragraph{\footnotesize \textnormal{The Constant Combinator}}\label{constant-combinator}
$\K : ğ’ â†’ â„±ğ“Šğ“ƒğ’¸(ğ’Ÿ,ğ’)$
+ For objects $x$, the ``constant functor'': \\
   $\K{x}\, y = x$ and $\K{x}\, f = \Id_x$ for objects $y$ and morphisms $f$.
+ For morphisms $f$, the ``constant natural transformation'': \\
   $\K{f} : \K{(\src f)} â†’Ì£ \K{(\tgt f)}$
   sending objects $y$ to morphism $\K{f}\, y = f$.

\room
Sometimes it is convenient to notate $\const{c} = \K \, c$
and refer to this as the /everywhere c/ operation.
#+LaTeX: \eqn{constant-Defn}{\const{c}\, a = c}

The following property defines constant functions at the `pointfree level':
#+LaTeX: \eqn{constant-Fusion}{\const{c} âˆ˜ f = \const{c} }

Constant functions force any difference in behaviour for any two
functions to disappear:
#+LaTeX: \eqn{constant-Equality}{\const{c} âˆ˜ f = \const{c} âˆ˜ g}

Interestingly in ğ’®â„¯ğ“‰, composition and application
are bridged explicitly by the constant functions:
#+BEGIN_EXPORT latex
\eqn{Set-Bridge}{ f âˆ˜ \const{c} = \const{f \, c}}
#+END_EXPORT

\newpage
* Monics and Epics

Identity functions and constant functions are limit points of the 
functional spectrum with respect to information preservation.
All the other functions are in-between: They â€œloseâ€ some information,
which is regarded as uninteresting for some reason.

\room

How do functions lose information?
Basically in two ways: They may be â€œblindâ€ enough to confuse different
inputs, by mapping them to the same output, or they may ignore values
of their target. For instance, $\const{c}$ confuses all inputs
by mapping them all onto $c$. Moreover, it ignores all values of its
target apart from $c$.

\room

Functions which do not confuse their inputs are called /monics/:
They are â€œpost-cancellableâ€:

#+BEGIN_EXPORT latex
\eqn{monic-Defn}{ f \; \mathsf{monic} \equivS 
  \left(âˆ€ h,k \;â€¢\; f âˆ˜ h = f âˆ˜ k \equivs h = k\right)
}
#+END_EXPORT

Functions which do not ignore values of their target are called
/epics/: They are â€œpre-cancellableâ€:
#+BEGIN_EXPORT latex
\eqn{epic-Defn}{ f \; \mathsf{epic} \equivS 
  \left(âˆ€ h,k \;â€¢\; h âˆ˜ f = k âˆ˜ f \equivs h = k\right)
}
#+END_EXPORT

Intuitively, $h = k$ on all points of their source precisely when
they are equal on all image points of $f$, since $f$ being epic means 
it outputs all values of their source.

\room

It is easy to check that â€œtheâ€ identity function is monic and epic, 
while any constant function $\const{c}$ is not monic and is only
epic when its target consists only of $c$.

* Isos

#+LaTeX: \def\inverse{^{-1}}

An arrow is an /iso/ iff it is invertible; i.e., there is an â€œinverseâ€ morphism
$f\inverse$ with
\eqn{inverse-Char}{ f âˆ˜ f\inverse = \Id \landS f\inverse âˆ˜ f = \Id}

To /construct/ $f\inverse$, we begin by identifying its type which may give
insight into its necessary `shape' --e.g., as a sum or a product--
then we pick one of these equations and try to reduce it as much as possible
until we arrive at a definition of $fË˜$, or its `components'.
  + E.g., $coassocr = [\Id + \inl, \inr âˆ˜ \inr] : (A + B) + C â‰… A + (B + C)$, its inverse
    /coassocl/ must be of the shape $[x, [y, z]]$ for unknowns $x,y,z$ which can be calculated
    by solving the equation $[x, [y, z]] âˆ˜ coassocr = \Id$ --Do it!

# --E.g., the inverse of ~assoc~.

\room

The following rules can be of help if $f\inverse$ is found handier than isomorphism $f$
in reasoning,

\begineqns

\eqn{inverse-Shunting1}{ f âˆ˜ x = y \equivS x = f\inverse âˆ˜ y}

\eqn{inverse-Shunting2}{ x âˆ˜ f = y \equivS x = y âˆ˜ f\inverse}

\endeqns

\room
\room

Isos are necessarily monic and epic, but in general the other way
around is not true.

\room

Isomorphisms are very important because they convert data from one
â€œformatâ€, say $A$, to another format, say $B$, without losing 
information. So $f$ and $fË˜$ are faithful protocols between the two
formats $A$ and $B$. 
Of course, these formats contain the same â€œamountâ€ of information
although the same data adopts a â€œdifferentâ€ shape in each of them.
â”€c.f. \nameref{SeqPair-is-Pair-Seq}.

\room

Isomorphic data domains are regarded as â€œabstractlyâ€ the same;
then one write $A â‰… B$.

Finally, note that all classes of functions referred to so far
---identities, constants, epics, monics, and isos---
are closed under composition.

Monics to the initial object are necessarily isos! 
# Simialr to the bottom of a poset.

# \newpage.\newpage
* COMMENT More Morphism Properties

# \NTHM{}{Monic}{ compositionally post-cancellable ---nearly the same formulation as injective! }

If a composition is monic then its first arrow is monic.

\NTHM{}{Epic}{ compositionally pre-cancellable }

If a composition is epic then its last arrow is epic.

\NTHM{}{Split Monic}{ compositionally post-invertible }

\NTHM{}{Split Epic}{ compositionally pre-invertible }

\NTHM{}{Iso}{ Both epic and monic ; i.e., monic and split epic }


* Skolemisation
# â€œUp to Isomorphismâ€

If a property $P$ holds for precisely one class of isomorphic objects,
and for any two objects in the same class there is precisely one
isomorphism from one to the other, then we say that
/the P-object is unique up to unique isomorphism/. 
For example, in ğ’®â„¯ğ“‰ the one-point set is unique up to a unique isomorphism,
but the two-point set is not.

\room

For example, an object /A/ is ``initial'' iff
$âˆ€ B  \;â€¢\;  âˆƒâ‚ f  \;â€¢\;  f : A â†’ B$, and such objects are unique
up to unique isomorphism --prove it!
The formulation of the definition is clear but it's not very well suited for /algebraic manipulation/.

\room

A convenient formulation is obtained by `skolemisation': An assertion of the form
\[ âˆ€ x \;â€¢\; âˆƒâ‚ y \;â€¢\; R \, x \, y \]
is equivalent to: There's a function â„± such that
\[ \, \hspace{13em} âˆ€ x, y \;â€¢\; R \, x \, y \;â‰¡\; y = â„±\, x  \hspace{8em}\text{\sc(â„±-Char)} \]
In the former formulation it is the existential quantification â€œ$âˆƒ y$â€ inside the scope of a universal
one that hinders effective calculation. In the latter formulation the existence claim is brought to a
more global level: A reasoning need no longer be interrupted by the declaration and naming of the
existence of a unique $y$ that depends on $x$; it can be denoted just $â„±\, x$.
As usual, the final universal quantification can be omitted, thus simplifying the formulation once more.

\room

In view of the important role of the various $y$'s, these $y$'s deserve a particular notation that
triggers the reader of their particular properties. We employ bracket notation such as $â¦‡ x â¦ˆ$
for such $â„±\, x$: An advantage of the bracket notation is that no extra parentheses are needed
for composite arguments $x$, which we expect to occur often.

\room

The formula /characterising/ $â„±$ may be called `â„±-Char' and it immediately give us some results
by truthifying each side, namely `Self' and `Id'. A bit more on the naming:

| Type        | Possibly non-syntactic constraint on notation being well-formed |
| Self        | It, itself, is a solution                                       |
| Id          | How $\Id$ can be expressed using it                             |
| Uniq        | Its problem has a unique solution                               |
| Fusion      | How it behaves with respect to composition                      |
| Composition | How two instances, in full subcategories, compose               |

Note that the last 3 indicate how the concept interacts with the categorical structure:
$=, ï¹”, \Id$. Also note that Self says there's at least one solution and Uniq says there is
at most one solution, so together they are equivalent to â„±-Char --however those two proofs
are usually not easier nor more elegant than a proof of â„±-Char directly.

\room

*Proving â„±-Char* is straightforwardly accomplished by providing a definition for â„±
and establishing â„±-Char --these two steps can be done in parallel! Almost every such
proof has the following format, or a circular implication thereof: For arbitrary $x$ and $y$,

# \vspace{2em}
#+begin_calculation latex
    R \kern0.5ex x \kern0.5ex y
\step[â‰¡]{}
    â‹®
\step[â‰¡]{}
    y = \text{â€œan expression not involving $y$â€}
\step[â‰¡]{ ğ’¹â„¯ğ’»ğ’¾ğ“ƒâ„¯ $â„± \, x$ to be the right side of the previous equation }
    y = â„± \kern0.5ex x
#+end_calculation

\newpage

#  [Dangerous] Convention:
#  free variables are quantified implicitly in such a way that the laws in which they appear
#  are well-defined.

* Initiality

# Convenient definition: 
An object /0 is initial/ if there's a mapping $â¦‡-â¦ˆ$, from objects to morphisms,
such that \ref{initial-Char} holds; from which we obtain a host of useful corollaries.
Alternative notations for $â¦‡ B â¦ˆ$ are $\text{!`}_B$, or $â¦‡0 â†’ Bâ¦ˆ$ to make the
dependency on 0 explicit.

#+BEGIN_EXPORT latex 
\begineqns

\eqn{initial-Char}{ f : 0 â†’ B \equivS f = â¦‡ B â¦ˆ }

\eqn{initial-Self}{ â¦‡ B â¦ˆ : 0 â†’ B }

\eqn{initial-Id}{ \Idâ‚€ = â¦‡ 0 â¦ˆ }

\eqn{initial-Uniq}{ f, g : 0 â†’ B \impliesS f = g }

% Pre-absorption
\eqn{initial-Fusion{ f : B â†’ C \impliesS f âˆ˜ â¦‡ B â¦ˆ = â¦‡ C â¦ˆ }

\vspace{2ex}
{\tiny Provided objects $B, C$ are both in ğ’œ and â„¬,
which are full subcategories of some category ğ’:}
\eqn{initial-Compose}{ â¦‡ C â† B â¦ˆ_â„¬ âˆ˜ â¦‡B â† Aâ¦ˆ_ğ’œ = â¦‡ C â† Aâ¦ˆ_ğ’œ }
%
% Recall: ğ’Ÿ is a full-subcategory of ğ’ means: ğ’Ÿ(x,y) = ğ’(x,y) for x,y : Obj ğ’Ÿ.

\vspace{2ex}
{\tiny Provided ğ’Ÿ is built on top of ğ’; i.e., ğ’Ÿ-objects are composite entities in ğ’: } 
\eqn{initial-Type}{ B \text{ is an object in ğ’Ÿ } \impliesS â¦‡ B â¦ˆ \text{ is a morphism in ğ’} }

\endeqns
#+END_EXPORT

\vspace{1em}

These laws become much more interesting when the category is built upon another
one and the typing is expressed as one or more equations in the underlying
category. In particular the importance of fusion laws cannot be over-emphasised;
it is proven by a strengthening step of the form
$f âˆ˜ â¦‡Bâ¦ˆ : 0 â†’ C \providedS â¦‡Bâ¦ˆ : 0 â†’ B \lands f : B â†’ C$.

\room

For example, it can be seen that the datatype of sequences is `the' initial object
in a suitable category, and the mediator $â¦‡-â¦ˆ$ captures
â€œdefinitions by induction on the structureâ€! Hence induction arguments
can be replaced by initiality arguments! Woah!

** COMMENT Proving Initiality :moved_to_skolemisation_section:
\room

*Proving Initiality* One may prove that an object $0$ is initial by providing
a definition for $â¦‡-â¦ˆ$ and establishing initial-Char. Almost every such
proof has the following format, or a circular implication thereof: For arbitrary /f/ and /B/,

\vspace{2em}
#+begin_calculation latex
    f : A â†’ B
\step[â‰¡]{}
    â‹®
\step[â‰¡]{}
    f = \text{â€œan expression not involving $f$â€}
\step[â‰¡]{ ğ’¹â„¯ğ’»ğ’¾ğ“ƒâ„¯ $â¦‡ B â¦ˆ$ to be the right side of the previous equation }
    f = â¦‡ B â¦ˆ
#+end_calculation

\vspace{-2em}

# With the characterisation and init-Identity rules, one can show that initial objects are unique
# up to isomorphism and so I've opted out to naming a representative `0`. Dually, for final objects.
# %
# % See page 38, par 2.22 of Fokkinga's gentle introduction for a nice calculational proof.
# 

\newpage

** COMMENT full, luff, subcategory definitions

 \NTHM{}{Subcategory}{ Subcollection of objects and a subcollection of morphisms
   that contains the identities for them and is closed under composition.
 }

 \NTHM{}{Full subcategory}{ Each homset of the morphism subcollection is the whole homset of the parent category.
 That is, it is generated by the objects subcollection.
 }

 \NTHM{}{lluf subcategory}{ The object subcollection contains all objects of the parent category. }

* Colimits

Each colimit is a certain initial object, and each initial object is a certain colimit.

# Colimits are precisely initiality, and generalise sums, coequalisers, and pushouts.

+ A /diagram in ğ’/ is a functor $D : ğ’Ÿ â†’ ğ’$.

    # Let $D : \mathcal{D} \to \mathcal{A}$ be a functor
    #  ---in-fact it suffices to consider this as a graph homomorphism
    #  reifying the nodes and vertices of $\mathcal{D}$ as objects
    #  and morphisms of $\mathcal{A}$.
    # 

    #  When giving $D$ and $\mathcal{D}$ via a diagram, as is done below in some
    #  example constructions, then $\mathcal{D}$ is just a subcategory and so
    #  $D$ is just the inclusion functor :-)
    # 

+ Recall \nameref{constant-combinator} yielding a functor on objects ---$\const{C}\, x = C$ for objects $x$ and
  $\const{C}\, f = \Id_C$ for morphisms /f/--- and a natural transformation on arrows
  ---$\const{g} = x \mapsto g : \const{A} â†’Ì£ \const{B}$ for morphism $g : A â†’ B$.

+ The category $â‹D$, built upon ğ’, has objects $Î³ : D â†’Ì£ \const{C}$ called â€œco-conesâ€, for
  some object $C =: \tgt\, Î³$, and a morphism from $Î³$ to $Î´$ is a ğ’-morphism $x$ such that $\const{x} âˆ˜ Î³ = Î´$.

 /`Cones' sit upright on their base, $D$, on a table; `CoCones' sit upright on a co-table!/
 
+ A /colimit for D/ is an initial object in $â‹ D$; which may or may not exist.

\room

Writing $-â•±Î³$ for $â¦‡-â¦ˆ$ and working out the definition of co-cone in terms of equations in ğ’,
  we obtain: /$Î³ : Obj(â‹D)$ is a colimit for $D$/ if there is a mapping $-â•±Î³$ such that /â•±-Type/ and
/â•±-Char/ hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\slash$-Type}{ Î´ \text{ cocone for } D \impliesS Î´â•±Î³ : \tgt\, Î³ â†’ \tgt\,Î´}

\vspace{2ex}
Well-formedness convention: In each law the variables are quantified
in such a way that the premise of $\slash$-Type is met.
The notation $Î´â•±â‹¯$ is only senseful if Î´ is a co-cone for $D$,
like in arithmetic where the notation $m \div n$ is only sensful if $n$ differs from 0.

\eqn{$\slash$-Char}{ \const{x} âˆ˜ Î³ = Î´ \equivS x = Î´ â•± Î³ }

\vspace{2ex}
The fraction notation better suggests the \emph{calculational} properties.

% # Maybe call the colimiting object $\bigsqcup D$ ? Since it is similar to the `join'
% # and a cocone is a `proof' that an object is an upper bound?

\vspace{2ex}
Notice that for given $x : C â†’ C'$ the equation $Î´ â•± Î³ = x$ \underline{defines}
Î´, since by $\slash$-Char that one equation equivales the family of equations
$Î´_A = x âˆ˜ Î³_A$. This allows us to define a natural transformation --or `eithers' in
the case of sums-- using a single function \emph{having} the type of the mediating arrow.

\eqn{$\slash$-Self ; Cancellation}{ \const{Î´â•±Î³} âˆ˜ Î³ = Î´}

\eqn{$\slash$-Id}{Î³â•±Î³ = \Id}

\eqn{$\slash$-Fusion}{x âˆ˜ Î´â•±Î³ = (\const{x} âˆ˜ Î´) â•± Î³}
% Direct Proof:
%
%    Î³â•²Î´ ï¹” x = Î³ â•² (Î´ ï¹” _x_)
% â‰¡  Î³ ï¹” \const{Î³â•²Î´ ï¹” x} = Î´ ï¹” _x_          â•²-char
% â‰¡  Î³ ï¹” \const{Î³â•²Î´} ï¹” _x_ = Î´ ï¹” _x_          const functor
% â‰¡  Î´ ï¹” _x_ = Î´ ï¹” _x_                         â•²-self
% â‰¡ true                                        =-reflexivity

\eqn{$\slash$-Unique ; JointlyEpic}{\const{x} âˆ˜ Î³ = \const{y} âˆ˜ Î³ \impliesS x = y}

% \vspace{2ex}
% This expresses that colimits Î³ have an epic-like property: \\
% The component morphisms $Î³_A$ are \emph{jointly epic}.

\vspace{2ex}
The following law confirms the choice of notation once more
--just as the above fusion law is nearly mutliplication into the numerator.

\eqn{$\slash$-Compose}{Îµâ•±Î´ âˆ˜ Î´â•±Î³  = Îµâ•±Î³}

% # ? \alert{where} $\delta$ and $\epsilon$ are $D$ cocones
% # ? and anything appearing to the left? of $\under$ being a colimit!

\vspace{2ex}
The next law tells us that functors distribute over the â•±-notation
provided the implicit well-formedness condition that 
$FÎ³$ is a colimit holds --clearly this condition is valid when $F$
preserves colimits.

\eqn{$\slash$-Functor-Dist}{F(Î´â•±Î³) = FÎ´ â•± FÎ³}

\eqn{$\slash$-Pre-Functor-Elim}{Î´Fâ•±Î³F = Î´â•±Î³}

% Recall $\eta F$ be the natural transformation $x \mapsto \eta_{F\, x}$.
%  Let $\gamma$ and $\gamma F$ be colimits for the same diagram, then

\endeqns
#+END_EXPORT

\room
\room

Let $()_x : \varnothing â†’Ì£ \K{x}$ be the natural transformation from the
empty functor $\varnothing : \mathbf{0} \to ğ’$ to the constant functor.
\vspace{-0.8em}
#+LaTeX:  \eqn{Initiality as colimit}{â¦‡Bâ¦ˆ \eqs ()_B â•± ()_0}

# Needless to say, my favourite:
Cocones under $D$ correspond one-to-one with arrows from its colimit:
\vspace{-0.8em}
#+LaTeX:  \eqn{Colimit as Adjunction}{(D â†’Ì£ \K C) \;\cong\; (\mathsf{CoLim}\,D \to C) \!\qquad \text{ where } \mathsf{CoLim}\,D = \tgt Î³ }
* Limits

Dually, the category $â‹€D$ has objects being â€œconesâ€ $Î³ : \const{C} â†’Ì£ D$ where $C =: \src\, Î³$
is a ğ’-object, and a morphism to $Î³$ /from/ $Î´$ is a ğ’-morphism $x$ such that $Î³ âˆ˜ \const{x} = Î´$.
In terms of ğ’, /$Î³ : Obj(â‹€ D)$ is a limit for $D$/ if there is a mapping $Î³â•²-$ such that
the following â•²-Type and â•²-Char hold, from which we obtain a host of corollaries.
As usual, there is the implicit well-formedness condition. 
# Theorem â•²-Unique expresses that limits Î³ have an monic-like property:
# The component morphisms $Î³_A$ are \emph{jointly monic}.
#
# Well-formedness convention: In each law the variables are quantified
# in such a way that the premise of $/$-Type is met.
# The notation $Î´â•±â‹¯$ is only senseful if Î´ is a cone for $D$,
# like in arithmetic where the notation $m/n$ is only sensful if $n$ differs from 0.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\backslash$-Type}{ Î´ \text{ cone for } D \impliesS Î³ â•² Î´ : \src\, Î´ â†’ \src\,Î³}

\eqn{$\backslash$-Char}{ Î³ âˆ˜ \const{x}  = Î´ \equivS x = Î³ â•² Î´ }

\eqn{$\backslash$-Self}{ Î³ âˆ˜ \const{Î³ â•² Î´}  = Î´}

\eqn{$\backslash$-Id}{Î³â•²Î³ = \Id}

\eqn{$\backslash$-Fusion}{Î³â•²Î´  âˆ˜ x = Î³ â•² (Î´ âˆ˜ \const{x})}

\eqn{$\backslash$-Unique ; JointlyMonic}{Î³ âˆ˜ \const{x} = Î³ âˆ˜ \const{y} \impliesS x = y}

\eqn{$\backslash$-Functor-Dist}{F(Î³ â•² Î´) = FÎ³ â•² FÎ´}

\eqn{$\backslash$-Pre-Functor-Elim}{Î³F â•² Î´F = Î³â•²Î´}
% Direct proof:
%
%    Î´Fâ•±Î³F = Î´â•±Î³
% â‰¡  Î´F = (Î´ â•± Î³) ï¹” Î³F
% â‰¡  (Î´ â•± Î³ ï¹” Î³)F = (Î´ â•± Î³) ï¹” Î³F
% â‰¡  (Î´ â•± Î³ ï¹” Î³)FA = ((Î´ â•± Î³) ï¹” Î³F)A
% â‰¡  Î´ â•± Î³ ; Î³FA  = Î´â•±Î³ ï¹” Î³FA
% â‰¡  true
% 
\endeqns
#+END_EXPORT

\vspace{-1em}

\newpage
* Sums
#+LaTeX: \def\inl{\mathsf{inl}} 
#+LaTeX: \def\inr{\mathsf{inr}}

Take $D$ and $ğ’Ÿ$ as suggested by $Dğ’Ÿ = \left( \overset{A}{â€¢} \;\;\; \overset{B}{â€¢} \right)$.
Then a cocone Î´ for $D$ is a two-member family $Î´ = (f, g)$ with
$f : A â†’ C$ and $g : B â†’ C$, where $C = \tgt\, Î´$.

\room

Let $Î³=(\inl, \inr)$ be a colimit for $D$, let $A + B = \tgt\,Î³$, and write
$[f, g]$ in-place of $Î³â•²(f, g)$, then the â•²-laws yield:
/$(\inl, \inr, A+B)$ form a sum of $A$ and $B$/ if there is a mapping $[-,-]$
such that \ref{[]-Type} and \ref{[]-Char} hold.

#  Take $D$ and $\mathcal{D}$ as suggested by $D\,\mathcal{D} = (\spot\!\!\!\!^A \; \spot\!\!\!\!^B)$, for given
#  objects $A = \src f$ and $B = \src g$, with colimit $\gamma = (\mathsf{inl}, \mathsf{inr})$.
# #+LaTeX:  \eqn{Sum as colimit, â•±-[]}{[f,g] \eqs (\mathsf{inl}, \mathsf{inr}) \under (f,g)}

#+BEGIN_EXPORT latex
\begineqns

\eqn{[]-Type}{f : A â†’ C \lands g : B â†’ C \impliesS [f, g] : A + B â†’ C}

\eqn{[]-Char}{ x âˆ˜ \inl = f \lands x âˆ˜ \inr = g \equivS x = [f, g] }

\eqn{[]-Cancellation; []-Self}{ [f, g] âˆ˜ \inl = f \landS [f, g] âˆ˜ \inr = g}

\eqn{[]-Id}{ [\inl, \inr] = \Id}

\eqn{[]-Unique}{ x âˆ˜ \inl = y âˆ˜ \inl \lands x âˆ˜ \inr = y âˆ˜ \inr \impliesS x = y}

\eqn{[]-Fusion}{ x âˆ˜ [f , g] = [x âˆ˜ f, x âˆ˜ g] }

\vspace{2ex}
The implicit well-formedness condition in the next law is that
$(F\,\inl, F\,\inr, F(A+B))$ form a sum of $F\,A$ and $F\, B$.
%  --which clearly holds if $F$ preserves sums.

\eqn{[]-Functor-Dist}{F \, [f, g]_ğ’ = [F \, f , F \, g]_ğ’Ÿ \qquad\text{ where } F : ğ’ â†’ ğ’Ÿ}
%
% f : A â†’ C
% g : B â†’ C
% F[f,g]     : F(A + B) â†’ F C
% [F f, F g] : F A + F B â†’ F C
%
% Types are okay since, by assumption, F(A + B) forms the sum of FA and FB,
% That is, (F A + F B) â‰” F(A + B)
%
% Direct proof: 
% 
%    F[f,g] = [Ff, Fg]
% â‰¡  F[f,g] ï¹” F inl = F f  âˆ§ F[f,g] ï¹” F inr = F g
% â‰¡  F([f,g] ï¹” inl) = F f  âˆ§ F([f,g] ï¹” inr) = F g
% â‡ [f,g] ï¹” inl = f  âˆ§ [f,g] ï¹” inr = g
% â‰¡  [f,g] = [f,g]
% â‰¡  true

\endeqns
#+END_EXPORT

\room
\room

In the pointwise setting, notice that the cancellation law serves to define the casing construct [-,-].
Then that casing is a form of conditional can be read from the characterisation.
With this view, fusion, post-distributivity of composition over casing is just
the usual law that function application distributes over conditionals
and the casing extensionality law is the body-idempotency of conditionals.

\room

For categories in which sums exist, we define for $f : A â†’ B$ and $g : C â†’ D$,

\begineqns

\eqn{+-Definition}{ f + g = [ \inl âˆ˜ f, \inr âˆ˜ g] : A + C â†’ B + D}

\eqn{Injections-Naturality}{ (f + g) âˆ˜ \inl = f âˆ˜ \inl \landS (f + g) âˆ˜ \inr = \inr âˆ˜ g }

\eqn{Extensionality}{ [h âˆ˜ \inl , h âˆ˜ \inr] = h}

\eqn{Absorption}{ [h, j] âˆ˜ (f + g) = [h âˆ˜ f, j âˆ˜ g] }

\eqn{+-BiFunctoriality}{ \Id + \Id = \Id \landS (f + g) âˆ˜ (h + j) = (f âˆ˜ h) + (g âˆ˜ j)}

\eqn{Structural Equality}{ [f,g] = [h, j] \equivS f = h \lands g = j }

\endeqns

\newpage

* Products

#+BEGIN_EXPORT latex
\def\fst{\mathsf{fst}}
\def\snd{\mathsf{snd}}
#+END_EXPORT

Take $D$ and $ğ’Ÿ$ as suggested by $Dğ’Ÿ = \left( \overset{A}{â€¢} \;\;\; \overset{B}{â€¢} \right)$.
Then a cone Î´ for $D$ is a two-member family $Î´ = (f, g)$ with
$f : C â†’ A$ and $g : C â†’ B$, where $C = \tgt\, Î´$.

\room

Let $Î³=(\fst, \snd)$ be a limit for $D$, let $A + B = \tgt\,Î³$, and write
$âŸ¨f, gâŸ©$ in-place of $(f, g)â•±Î³$, then the â•±-laws yield:
/$(\fst, \snd, A Ã— B)$ form a product of A and B/ if there is an operation
$âŸ¨-,-âŸ©$ satisfying the Char and Type laws below; from which we obtain a host of corollaries.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\langle\rangle$-Type}{f : C â†’ A \lands g : C â†’ B \impliesS âŸ¨f, gâŸ© : C â†’ A Ã— B}

\eqn{$\langle\rangle$-Char}{ \fst âˆ˜ x = f \lands \snd âˆ˜ x = g \equivS x = âŸ¨f, gâŸ© }

\eqn{$\langle\rangle$-Cancellation; $\langle\rangle$-Self}{ \fst âˆ˜ âŸ¨f, gâŸ© = f \landS \snd âˆ˜ âŸ¨f, gâŸ© = g}

\eqn{$\langle\rangle$-Id}{ âŸ¨\fst, \sndâŸ© = \Id}

\eqn{$\langle\rangle$-Unique}{ \fst âˆ˜ x = \fst âˆ˜ y  \lands \snd âˆ˜ x = \snd âˆ˜ y \impliesS x = y}

\eqn{$\langle\rangle$-Fusion}{ âŸ¨f , gâŸ© âˆ˜ x = âŸ¨f âˆ˜ x , g âˆ˜ xâŸ© }

\eqn{$\langle\rangle$-Functor-Dist}{F \, âŸ¨f, gâŸ©_ğ’ = âŸ¨F \, f , F \, gâŸ©_ğ’Ÿ \qquad\text{ where } F : ğ’ â†’ ğ’Ÿ}

\endeqns
#+END_EXPORT

\room
\room

The characterisation says that the essential properties of ordered pairs 
is that their components are retrievable and they are
completely determined by their components.

\room

Notice that the cancellation rule is essentially the /definitions of projections/ in the pointwise setting;
likewise absorption is akin to the pointwise definition of the product bi-map.

\room

The fusion laws give us a pointfree rendition of their usual pointwise
definitions: All applications have been lifted to compositions!
# Likewise for the absorption laws.

# These are essentially a re-write of the sum laws.

\room

For categories in which products exist, we define for $f : A â†’ B$ and $g : C â†’ D$,

\begineqns

\eqn{$\times$-Definition}{ f Ã— g = âŸ¨ f âˆ˜ \fst, g âˆ˜ \snd âŸ© : A Ã— C â†’ B Ã— D}

\eqn{Projections-Naturality}{ \fst âˆ˜ (f Ã— g) = f âˆ˜ \fst \landS \snd âˆ˜ (f Ã— g) = g âˆ˜ \snd }

\eqn{Extensionality}{ âŸ¨\fst âˆ˜ h, \snd âˆ˜ hâŸ© = h}

\eqn{Absorption}{ (f Ã— g) âˆ˜ âŸ¨h, jâŸ© = âŸ¨f âˆ˜ h, g âˆ˜ jâŸ© }

\eqn{$\times$-BiFunctoriality}{ \Id Ã— \Id = \Id \landS (f Ã— g) âˆ˜ (h Ã— j) = (f âˆ˜ h) Ã— (g âˆ˜ j)}

\eqn{Structural Equality}{ âŸ¨f,gâŸ© = âŸ¨h, jâŸ© \equivS f = h \lands g = j }

\endeqns

\newpage

* Finitary Sums and Products

All properties studied for binary /splits/ and binary /eithers/ extend to the
finitary case. For the particular situation $n = 1$, we will have $âŸ¨fâŸ©=[f]=f$
and $\inl = \fst = \Id$, of course.

\room

For the particular situation $n = 0$,
finitary products â€œdegenerateâ€ to terminal object 1 and finitary sums â€œdegenerateâ€ to initial object 0.
The standard notation for the empty split $âŸ¨âŸ©$ is $!_C$, where $C$ is the source.
Dually, the standard notation for the empty either $[]$ is $?_C$.

# Exercise 2.30: Particularise the exchange law to empty products and empty coproducts; i.e., 1 and 0!

\eqn{Empty Exchange Rule}{ âŸ¨âŸ©_0 = []_1 }

# \newpage

* Mixing products and coproducts

Any $f : A + B â†’ C Ã— D$ can be expressed alternatively as an /either/
or as a /split/. It turns out that both formats are identical: 
# The Interchange rule.
\vspace{-1.5em}
\eqn{Exchange Rule}{ âŸ¨[f,g], [h,j]âŸ© = [âŸ¨f,hâŸ©,âŸ¨g,jâŸ©] }

E.g., $\mathsf{undistr}  = âŸ¨[\fst, \fst], \snd + \sndâŸ© = [\Id Ã— \inl, \Id Ã— \inr] : (A Ã— B) + (A Ã— C) â†’ A Ã— (B + C)$.

# Notice that this law above is self-dual.

# Also, by the exchange rule,
\begineqns

\eqn{Cool-Property}{ [f Ã— g, h Ã— k] \;=\; âŸ¨ [f, h] âˆ˜ (\fst + \fst), [g, k] âˆ˜ (\snd + \snd)âŸ© }

\eqn{Co-cool-Property}{ âŸ¨f + g, h + kâŸ© \;=\; [ âŸ¨f, hâŸ© Í¾ (\inl Ã— \inl), âŸ¨g, kâŸ© Í¾ (\inr Ã— \inr)] }
# Direct proof:
# 
#   [ âŸ¨f, hâŸ© Í¾ (\inl Ã— \inl), âŸ¨g, kâŸ© Í¾ (\inr Ã— \inr)]
# = [ âŸ¨inl âˆ˜ f , inl âˆ˜ hâŸ©, âŸ¨inr âˆ˜ g, inr âˆ˜ kâŸ© ]      absorption
# = âŸ¨ [inl âˆ˜ f, inr âˆ˜ g], [inl âˆ˜ h, inr âˆ˜ k]âŸ©       exchange
# = âŸ¨ f + g, h + k âŸ©                               defns
\endeqns

\room

Also, since constants ignore their inputs,
#+LaTeX: \eqn{Exchange-with-constant}{ [âŸ¨ f, \const{k} âŸ©, âŸ¨ g , \const{k} âŸ©] = âŸ¨ [f, g], \const{k} âŸ© }
# Proof:
#
#   [âŸ¨ f, _k_ âŸ©, âŸ¨ g , _k_ âŸ©]
# = âŸ¨ [f, g], [ _k_ , _k_ ]âŸ©        exchange
# = âŸ¨ [f, g], _k_ âˆ˜ [ Id, Id ]âŸ©   fusion
# = âŸ¨ [f, g], _k_ âŸ©               constants


\newpage

* Coequaliser

# $Dğ’Ÿ = \left( \overset{A}{â€¢} \overset{\overset{f}{\rightrightarrows}}{g} \overset{B}{â€¢} \right)$.
#
Take $D$ and $ğ’Ÿ$ as suggested by $Dğ’Ÿ = \left( \overset{A}{â€¢} \rightrightarrows^f_g \overset{B}{â€¢} \right)$;
where $f,g : A â†’ B$ are given. 
Then a cocone Î´ for $D$ is a two-member family $Î´ = (q', q)$
with $q' : A â†’ C, q : B â†’ C, C = \tgt\,\delta$ and $Î´_A âˆ˜ \const{C} h = D h âˆ˜ Î´_B$; in-particular
$q' = f âˆ˜ q = g âˆ˜ q$ whence $q'$ is fully-determined by $q$ alone.

Let $Î³ = (p', p) : Obj(â‹D)$ be a colimit for $D$ and write $-pâ•±$ in-place of $-â•±Î³$, then the â•±-laws
yield: /$p$ is a coequaliser of $(f,g)$/ if there is a mapping $-â•±p$ such that /CoEq-Type/ and
/CoEq-Char/ hold.

#+BEGIN_EXPORT latex
\begineqns

\eqn{CoEq-Type}{ f âˆ˜ q =  g âˆ˜ q \impliesS qâ•±p : \tgt\, p â†’ \tgt\,q}

\vspace{2ex}
Well-formedness convention: In each law the variables are quantified
in such a way that the premise of \ref{CoEq-Type} is met.
The notation $qâ•±â‹¯$ is only senseful if $f âˆ˜ q = g âˆ˜ q$,
like in arithmetic where the notation $m \div n$ is only sensful if $n$ differs from 0.

\eqn{CoEq-Char}{ x âˆ˜ p = q \equivS x = q â•± p }

\eqn{CoEq-Self}{ qâ•±p âˆ˜ p = q}

\eqn{CoEq-Id}{pâ•±p = \Id}

\eqn{CoEq-Fusion}{x âˆ˜ qâ•±p = (x âˆ˜ q)â•±p}

\eqn{CoEq-Unique}{x âˆ˜ p = y âˆ˜ p \impliesS x = y}

\eqn{CoEq-Compose}{râ•±q âˆ˜ qâ•±p = râ•±p}
%%
%% \eqn{?â•²-Functor-Dist}{F(Î´â•±Î³) = FÎ´ â•± FÎ³}
%% 
%% \eqn{?â•²-Pre-Functor-Elim}{Î´Fâ•±Î³F = Î´â•±Î³}
%%
\endeqns
#+END_EXPORT

# CoEqualisers generalise the notion of induced equivalence relation.

\vfill

\iffalse

  Taking $D$ and $\mathcal{D}$ as suggested by
  $D\,\mathcal{D}:$
  $ \raisebox{6pt}{$\spot$}
  \overset{ \overset{f}{\text{\tiny$\longrightarrow$}}
    }{ \overset{\longrightarrow}{\text{\tiny$g$}}  }
  \raisebox{6pt}{$\spot$}
  $
  
Now call the category $\bigvee D$ by the name $\bigvee(f \,|\!|\, g)$:
it has objects morphisms that post-equalise $f$ and $g$, and morphisms
$x : p \to q \equivS x : \tgt p \to \tgt q \lands p \fcmp x \eqs q$ 

A \emph{coequaliser} of $f,g$ is an initial object in
$\bigvee(f \,|\!|\, g)$.

\fi

# coEqualiser-Fusion, ``Composition into Numerator''
* References

#+LaTeX: {\color{white}.}

[[https://maartenfokkinga.github.io/utwente/mmf92b.pdf][A Gentle Introduction to Category Theory â”€ the calculational approach]]
\newline
by [[https://maartenfokkinga.github.io/utwente/][Maarten Fokkinga]]

\vspace{1em}

An excellent introduction to category theory with examples motivated from programming, in-particular
working with sequences. All steps are shown in a calculational style --which Fokkinga
has made [[https://ctan.org/tex-archive/macros/latex/contrib/calculation][available]] for use with LaTeX-- thereby making it suitable for self-study.

\vspace{1em}

Clear, concise, and an illuminating read.

\vspace{1em}

I've deviated from his exposition by using backwards composition `âˆ˜' rather
than diagrammatic composition `;', as such my limit notation is his colimit notation! Be careful.

\vspace{1em}

I've also consulted the delightful read [[http://www4.di.uminho.pt/~jno/ps/pdbc_part.pdf][Program Design by Calculation]] of [[http://www4.di.uminho.pt/~jno/][JosÃ© Oliveira]].

\vspace{0.5em}

Very accessible for anyone who wants an introduction to functional programming!
The category theory is mostly implicit, but presented elegantly!

\vspace{-0.5em}

** COMMENT Further Reads

 + Roland Backhouse
 + Grant Malcolm
 + Lambert Meertens
 + Jaap van der Woude

 + /Adjunctions/ by Fokkinga and Meertens

 + Backus' FP and Iverson's APL are examples of pointfree programming
   languages; look them up ;-)


* COMMENT FIXES
** Â§4 FIX

 0. Â§4.7, page 135: ?

** Â§3 FIX
 0. Â§3.1, page 68, exercise 3.3: The arguments to the ~for~ are in the wrong order; moreover the notation
    for multiplication is inconsistent with in (3.6).

 0. Â§3.3, page 79: The top-most code fragment mentions ~Eval T~ but nothing in the surrounding text mentions
    ~Eval~. Is this a typo?

 0. Â§3.4, page 83: In the fourth equation in the bottom-most calculation, there seems to be a missing
    ~cons~ tag on the right-hand side --c.f. (3.27)--. The right-most should be /Cons(a, x)/ instead of just /(a,x)/.

 0. Â§3.6, page 87: Right after (3.41), there is an inconsistency with the notation of multiplication, or there lack of.
    Perhaps consider using a Ã— or *, rather than just a space.

 0. Â§3.9, page 96, equation (3.63): Do you mean ${}_n C_p$ instead of ${}^n C_p$?
    If so: It might also help to mention that this is the combinatorial choice operator
    and is not the same as the coefficients $C_i$ of (3.62); otherwise the overloaded $C$ might
    cause more trouble than desired. Alternatively maybe use notation $\binom{n}{p}$ instead.

 0. Â§3.9, page 97, exercise 3.10: At the end of the first line, you want $(1 + X)Â²$ rather than $(1 + XÂ²)$.

 0. Â§3.12, page 99: When you first introduced isos back in Â§2, you said they were functions that were both
    monic and epic, which is the case for your ambient category, Sets. However, in the first line of this
    subsection, it seems you are stating that an /F/-algebra that is both epic and monoic is necessarily an iso
    --which as far as I know is not true, even when the underling category is Set.

 0. Â§3.15, page 110: First sentence after the functor instance should include a definite article; it should be
    /in the language's/.

 0. Â§3.16, page 115, line 10: Typo, /more that two/ should be /more than two/.

 0. Â§3.19, page 123, last paragraph, third line: Typo /such as discipline/ should be /such a discipline/.

** Â§2 FIX

 0. Â§2.4, last line:  Should be /`natural'/ instead of /'natural'./

 0. Â§2.5, exercise 2.1: Maybe place this at the end of Â§2.4 since
      it's a problem on identity functions,
      rather than at the end of Â§2.5 which is on constant functions.

 0. Â§2.6, end: It's easy to see how monics generalise injective functions
    but it's not as easy for the surjective-epic case. I've conjured up
    the following explanation, which might be helpful to include.
    #+BEGIN_QUOTE
 
 #+BEGIN_EXPORT latex
 \eqn{epic-Defn}{ f \; \mathsf{epic} \equivS 
   \left(âˆ€ h,k \;â€¢\; h âˆ˜ f = k âˆ˜ f \equivs h = k\right)
 }
 #+END_EXPORT

    Intuitively, $h = k$ on all points of their source precisely when
    they are equal on all image points of $f$, since $f$ being epic
    means it outputs all values of their source.
    #+END_QUOTE

 0. Â§2.7, before equation 2.18:
    I think you meant /pointfree/ not /pointwise/ as clearly 2.18 involves
    no points.

 0. Â§2.13, Ã—-fusion calculation: In the final hint, perhaps add /after (2.63)/
    so that it is clear which /derived above/ law is being referenced
    --since its name is in the surrounding text and may not be easy to locate.

 0. Â§2.14, page 39, second paragraph: Typo /market/ should be /marked/.
 0. Â§2.14, page 39, diagram: Typos, left arrow should be labelled /g/ not /f/,
    and the right arrow should be labelled /h/ not /g/.

 0. Â§2.14, exercise 2.21: It seems you have the accidental typos /Î»ap/ appearing twice.

 0. Â§2.15, second paragraph: For $f : B â† C Ã— A$, the phrase /f c âˆˆ B/ is ill-typed
    and it may be incorrect to say it, /f c/, /denotes a value of type B/.

 0. Â§2.15, typing (2.81): The name /ap/ appears on the arrow, perhaps you do not want it appearing 
    twice in one line like that.

 0. Â§2.15, first paragraph after (2.81): Perhaps it may be better to use a definite article:
    /Back to the generic binary operator/ rather than /Back to generic binary operator/.

 0. Â§2.18, page 56: In the definition of type ~Point~, maybe mention that the Haskell ecosystem
    uses distinct name-spaces for types and their constructors whence the overloading of ``Point''
    is non-ambiguous.

 0. Â§2.19, exercises 2.40: You have $\,.\, swap$ but you likely wanted to use a ~\cdot~ to obtain $Â·\, swap$.
* COMMENT TODO Generalise Exchange Law to arbitrary colimits and limits xD
* COMMENT Old Category Theory Theorem List

** Category Theory Notation :latex:ignore:
#+BEGIN_EXPORT latex
% identity morphism
\def\Id{\mathsf{id}}

% source and target
\def\src{\mathsf{src}\,}
\def\tgt{\mathsf{tgt}\,}

% constant functor
\def\K#1{ \mathcal{K}\, #1 }

% natural transformation
\def\natTo{\mathrel{\ooalign{\,$\longrightarrow$\cr\quad.}}}

\def\under{ \backslash }
#+END_EXPORT

** COMMENT Pullbacks
 \heading{Pullbacks}

 \def\Pb{ \operatorname{\mathsf{Pb}} }

 A \emph{pullback} of $f,g$ is an object $\Pb \, f \, g$ with two morphisms $f \nearrow g$ and $g \nearrow f$,
 and an operation ``named'' $(g \nearrow f , f \nearrow g)\under(-,-)$ with the following properties

 \NTHM{}{Axiom, Pullback Definition}{ $(g \nearrow f) \fcmp f \eqs (f \nearrow g) \fcmp g$}

 \NTHM{}{Axiom, Pullback Typing}{
   $p \fcmp f \eqs q \fcmp f \impliesS (g \nearrow f , f \nearrow g)\under(p, q) : \Pb\,f\,g \to \src p$}

 \NTHM{}{Axiom, Pullback Char}{
   $(g \nearrow f , f \nearrow g) \fcmp (x, x) \eqs (p, q) \;\equivs\; x \eqs (g \nearrow f , f \nearrow g)\under(p, q) $}

 \NTHM{}{Pullback Self, projections}{ \\
   {\color{white}.\hfill}
   $
   (g \nearrow f) \fcmp \big((g \nearrow f , f \nearrow g)\under(p, q)\big) \eqs p
   \landS
   (f \nearrow g) \fcmp \big((g \nearrow f , f \nearrow g)\under(p, q)\big) \eqs q
   $
   }

 \NTHM{}{Pullback Identity}{$\Id_{\Pb\,f\,g} \eqs (g \nearrow f , f \nearrow g)\under (g \nearrow f , f \nearrow g)$}

 \NTHM{}{$\nearrow$-Uniqueness, ``Jointly Epic''}{ \\
   {\color{white}.\hfill}
   $
   (g \nearrow f , f \nearrow g) \fcmp (x, x)
   \eqs
   (g \nearrow f , f \nearrow g) \fcmp (y, y)
   \impliesS x \eqs y
   $}

 \NTHM{}{Pullback-Fusion, ``Composition into Numerator''}{ \\
     {\color{white}.\hfill}
   $
   \bigg((g \nearrow f , f \nearrow g)\under(p, q)\bigg) \fcmp x
   \eqs
   (g \nearrow f , f \nearrow g)\under\bigg( (p, q) \fcmp (x,x) \bigg)
   $
   }

 \def\eq{\approx}
 \def\Eq{\mathsf{Eq}}

** COMMENT Equalisers
 \heading{Equalisers}

 Equalisers generalise the notion of ``solution set to an equation''
 or the domain of the common image of two morphisms.
  
 An \emph{equaliser} of $f,g$, with $\src f = \src g$, is an object $\Eq\, f\, g$, a morphism $[f \eq g] : \Eq\,f\, g \to \src\, f$, and a
 function $-\over[f \eq g]$ with the following properties

 \NTHM{}{Axiom, Definition}{ $[f \eq g] \fcmp f \eqs [f \eq g] \fcmp g$}

 \NTHM{}{Axiom, Typing}{
   $q \fcmp f \eqs q \fcmp g \impliesS q \over [f \eq g] : \src q \to \Eq\,f\,g$}

 \NTHM{}{Axiom, Characterisation}{
   $x \fcmp\, [f \eq g] \eqs q \equivS x \eqs q \over [f \eq g] $}

 \NTHM{}{Identity}{$\Id_{\Eq\,f\,g} \eqs [f \eq g] \over [f \eq g]$}

 \NTHM{}{PreFusion, ``PreComposition into Numerator''}{ 
   $x \fcmp ( q \over [f \eq g]) \eqs (x \fcmp q) \over [f \eq g]$}

 \NTHM{}{Self, Denominator Cancellation}{
   $ \big(q \over [f \eq g]\big) \fcmp [f \eq g] \eqs q
   \\
     \text{\hspace{18.4em}} \big(q \fcmp [f \eq g]\big) \over [f \eq g] \eqs q
   $
 }

 \NTHM{}{Epic}{
   $x \fcmp [f \sim g] \eqs y \fcmp [f \sim g] \impliesS x \eqs y$}

 \newpage

 \NTHM{}{ X }{ Y }

 \todo rewrite this section: Too much text. Introduce heuristics.
** COMMENT Finality

 \heading{Finality}
 \def\final#1{ \mbox{!}_#1 }

 \NTHM{}{Axiom, final-Characterisation}{$f : B \to 1 \equivS f \eqs \final{B}$}

 \NTHM{}{final-Self}{$\final{1} : 1 \to 1$}

 \NTHM{}{final-Identity}{$\Id_1 \eqs \final{1}$}

 \NTHM{}{final-Uniqueness}{$f,g : B \to 1 \impliesS f \eqs g$}

 \NTHM{}{final-Fusion / Post-absorption}{$f : B \to C \impliesS f \fcmp \final{B} \eqs \final{C} $}

 \NTHM{}{initial-final Coincidence}{$\initial{1} \eqs \final{0}$}
** COMMENT Pushouts
 \kern4ex
 \heading{Pushouts}

   Taking $D$ and $\mathcal{D}$ as suggested by
   $D\,\mathcal{D}:
     \spot\!\!\!\!_B\overset{f}{\text{\tiny$\longleftarrow$}} \!\!
     \spot\!\!\!\!_A\overset{g}{\text{\tiny$\longrightarrow$}}\!\!\spot\!\!\!\!_C
   $
  
 Working out the details and simplifying, we find that $\bigvee D$
 has objects morphism-pairs $p,q$ that post-equalise $f$ and $g$ in that $f \fcmp p \eqs g \fcmp q$, and morphisms
 $x : (p,q) \to (p',q') \equivS x : \tgt p \to \tgt p' \lands (p', q') \eqs (p,q) \fcmp x$.
 \iffalse details

 Since D is just the inclusion, we have that a cocone for the above diagram
 is a triple (p,q,r) to a single object and have sources (B,A,C) respectively
 such that r = f;p = g;q and so r is determined uniquely by p and q; whence it
 can be ignored.

 then a morphism between such pairs (p,q) and (p',q') is a morphism from the target
 of the former to the target of the latter such that the resulting triangles commute :-)

 Draw the diagrams!

 A \emph{pushout} of $f,g$ is an inital object in this category.

 Initiality means there exists an
 object $Po\, f\, g$ with two morphisms $f \searrow g$ and $g \searrow f$,
 and there exists a function $(f , g)\under(-,-)$ with the following properties

 \NTHM{}{Axiom, Pushout-Definition}{ $f \fcmp (f \searrow g) \eqs g \fcmp (g \searrow f)$}

 \NTHM{}{Axiom, Pushout-Typing}{
   $f \fcmp p \eqs g \fcmp q \impliesS (f , g) \under (p , q) : Po\,f\,g \to \tgt p$}

 \todo this looks wrong!\\
 \NTHM{}{Axiom, Pushout-Characterisation}{
   $(f , g) \fcmp (x , x) \eqs (p , q) \equivS x \eqs (f , g) \under (p , q) $}

 \NTHM{}{Pushout-Self, projections}{
   $f \fcmp (f , g) \under (p , q) \eqs p
   \landS
   g \fcmp (f , g) \under (p , q) \eqs q$
   }

 \NTHM{}{Pushout-Identity}{$\Id_{Po\,f\,g} \eqs (f , g) \under (f , g)$}

 \NTHM{}{$\under$-Uniqueness, ``Jointly Epic''}{
   $(f , g) \fcmp (x , x) \eqs (f , g) \fcmp (y , y) \impliesS x \eqs y$}

 \NTHM{}{Pushout-Fusion, ``Composition into Numerator''}{ \\
   $\big((f , g) \under (p , q)\big) \fcmp x \eqs (f , g) \under \big( (p , q) \fcmp (x,x)\big)$}
* COMMENT Content from older repo
** Compact Notation

 Why do people look for compact notations? A compact notation leads to
 shorter documents (less lines of code in programming) in which patterns are easier
 to identify and to reason about. Properties can be stated in clear-cut, one-line long
 equations which are easy to memorize. And diagrams such as (2.4) can be easily
 drawn which enable us to visualize maths in a graphical format.

** Functions and Contracts

 What do we want functions for? If we ask this question to a physician or engineer
 the answer is very likely to be: one wants functions for modelling and reasoning
 about the behaviour of real things.

 So we get a naive purpose of functions: we want them to be applied to arguments
 in order to obtain results.

 A function /f : B âŸµ A/ can be regarded as a kind of â€œcontractâ€: /f/ commits
 itself to producing a /B/-value provided it is supplied with an /A/-value. How is such
 a value produced? In many situations one wishes to ignore it because one is just
 using function /f/. In others, however, one may want to inspect the internals of the
 â€œblack boxâ€ in order to know the functionâ€™s computation rule.

 How free are we to fulfill the â€œgive me an /A/ and I will give you a /B/ â€ contract of
 declaration /f : B âŸµ A/? In general, the choice of /f/ is not unique. Some /f/s will do as little
 as possible while others will laboriously compute non-trivial outputs. At one of
 the extremes, we find functions which â€œdo nothingâ€ for us, that is, the added-value
 of their output when compared to their input amounts to nothing:
 /f a = a/.
 In this case /B = A/, of course, and /f/ is said to be the identity function on /A/.

 Observe that â€œnaturality lawsâ€ /f Â· h = h Â· f = f/ have a unique solution for /h/: â€œtheâ€ id-entity function.

** Constants

 #+BEGIN_QUOTE
 Opposite to the identity functions, which do not lose any information, we find
 functions which lose all (or almost all) information. Regardless of their input, the
 output of these functions is always the same value.

 Let C be a nonempty data domain and let and /c âˆˆ C/. Then we define the
 /everywhere c/ function as follows, for arbitrary /A/:
 /c : A â†’ C/,
 /_c_ a = c/
 #+END_QUOTE

 Observe that /Constant arrows that are epic necessarily target a terminal object./

** Epics and Monics

 #+BEGIN_QUOTE
 Identity functions and constant functions are limit points of the functional spectrum
 with respect to information preservation. All the other functions are in between:
 they lose â€œsomeâ€ information, which is regarded as uninteresting for some
 reason. This remark supports the following aphorism about a facet of functional
 programming: it is the art of transforming or losing information in a controlled
 and precise way. That is to say, the art of constructing the exact observation of
 data which fits in a particular context or requirement.

 How do functions lose information? Basically in two different ways: they may
 be â€œblindâ€ enough to confuse different inputs, by mapping them onto the same
 output, or they may ignore values of their codomain. For instance, /_c_/ confuses all
 inputs by mapping them all onto /c/. Moreover, it ignores all values of its codomain
 apart from /c/.

 Functions which do not confuse inputs are called monics (or injective functions)

 Functions which do not ignore values of their codomain are called epics (or
 surjective functions).
 #+END_QUOTE

** Isos
 #+BEGIN_QUOTE
 Isomorphisms are very important functions because they convert data from
 one â€œformatâ€, say /A/, to another format, say /B/, without losing information. 

 So /f/ and /fâ»Â¹/ are faithful protocols between the two formats /A/ and /B/. Of course,
 these formats contain the same â€œamountâ€ of information, although the same data
 adopts a different â€œshapeâ€ in each of them. In mathematics, one says that /A/ is
 isomorphic to /B/ and one writes /A â‰… B/ to express this fact.

 Isomorphic data domains are regarded as â€œabstractlyâ€ the same. Note that, in
 general, there is a wide range of isos between two isomorphic data domains.
 #+END_QUOTE

** Sums
 Given two functions /f : C âŸµ A/ and /g : C âŸµ B/ with a common target, we can â€œglueâ€ them
 together to obtain a function â€œeither /f/ or /g/â€ /[f , g] : C âŸµ A + B/ to that same common target but whose
 source is the sum of the sources. This function behaves like /f/ when the input comes from the
 â€œ/A/-sideâ€ and behaves like /g/ when the input comes from the â€œ/B/-sideâ€.

 The inputs can be â€œinjectedâ€ into this new composite source via two new combinators:
 /inl : A + B âŸµ A/ and /inr : A + B âŸµ B/.

 The gluing is specfied by the equation: /h = [f , g]  â‰¡  h . inl = f âˆ§ h . inr = g/
 Read, â€œ/h/ is the sum of /f/ and /g/ precisely when it behaves like /f/ when following a left
 injection and behaves like /g/ when following a right injection.â€

 # From this property it follows that the injections do not confuse inputs, i.e., are jointly-injective.
 Indeed the property above could be thought of as ensuring /no-junk and no-confusion/.
 - /no-junk/: Everything starting at a sum /h : C âŸµ A + B/ is necessarily expressible as a case: /h = [ h . inl , h . inr].
 - /no-confusion/: Casing does not mix-up distinct inputs, /[f , g] = [h , k] â‡’ f = h âˆ§ g = k/.

** Products

 When two morphisms do not compose but instead have a common source then we can /glue/
 them with the /pairing/ combinator /âŸ¨-,-âŸ©/ by forming the product of their targets.
 When the morphisms are not necessarily related, we can still /glue/ them together via the /product/ combinator;
 namely if /f : A âŸµ C/ and /g : B âŸµ D/ then, /product of f and g/, /f Ã— g : A Ã— B âŸµ C Ã— D/.

 Both gluing combinators maintain the information of their individual arguments, in the same way
 (Cartesian) product /A Ã— B/ keeps the information of /A/ and /B/.

 + Product is essentially symmetric
   - /swap := âŸ¨ snd , fst âŸ©/ is an involution witnessing /A Ã— B â‰… B Ã— A/
   - In programming, where products are records, /swap/ can be re-interpreted as a guarantee that one
     does not lose (or gain) anything in swapping fields in struct or record datatypes. 

 + Product is essentially associative
   - â€œassociate to the rightâ€, /assocr := âŸ¨ fst . fst , snd Ã— Id âŸ© : A Ã— (B Ã— C) âŸµ (A Ã— B) Ã— C/
   - This is an isomorphism with inverse /assocl := âŸ¨ Id Ã— fst , snd âˆ™ snd âŸ© : (A Ã— B) Ã— C âŸµ A Ã— (B Ã— C)/.

** The Exchange Law

 /Datatype constructions, such as A + B and A Ã— B, arise as devices for expressing
 the types of the results of gluing given morphisms./

 Two morphisms /Aâ€² âŸµ A/ and /Bâ€² âŸµ B/ can thus be combined by our combinators to
 obtain a morphism /Aâ€² Ã— Bâ€² âŸµ A + B/. Since such a morphism has source a sum, it
 can be obtained via the casing combinators; conversely, since it has target a product,
 it can be obtained via the pairing combinators. Naturally both forms coincide due to
 the /Exchange Law/ which expressed the equivalence of case-of-pairs and pair-of-cases:
 /[ âŸ¨f,gâŸ© , âŸ¨h,kâŸ© ]  =  âŸ¨ [f,h] , [g,k] âŸ©/.

 For example, applying this rule to the expression /âŸ¨ [ fst , fst ] , [ inl . snd , inr . snd] âŸ©/
 yields the familiar /un-distribute-right/ operation /undistr = [ Id Ã— inl , Id Ã— inr ] : A Ã— (B + C) âŸµ (A Ã— B) + (A Ã— C)/.

** material to be included

 *types*

  Formally, a â€œtype checkingâ€ discipline can be expressed in terms of compositional rules which
 check for functional expression wellformedness.

 *?*

 A case arises wherever two functions do not compose but share the same codomain.
 [...] then we use the sum combinator.

 Composition is certainly the most basic of all functional combinators. It is the
 first kind of â€œglueâ€ which comes to mind when programmers need to combine, or
 chain functions (or processes) to obtain more elaborate functions (or processes).
 This is because of one of its most relevant properties, associtivity: /(f Â· g) Â· h = f Â· (g Â· h)/.

 (

 I really like this approach you are promoting: Construing high-order combinators as
 variations on composition. I particularly like this idea when we get to the product
 and sum functors. Excellent! :-)

 )

** LaTeX Matter To Utilise

%{{{ header

\documentclass[11pt]{article}

\usepackage[hmargin=10mm, vmargin=10mm, landscape]{geometry}
\usepackage{multicol}

\usepackage[svgnames]{xcolor}
%
% See Section-4 for colors by name at:
% ftp://ftp.mackichan.com/swandswp55/tcitex/doc/latex/contrib/xcolor/xcolor.pdf

\usepackage{amsmath,amssymb}

\usepackage{CalcStyleV9} % http://calccheck.mcmaster.ca/ 
\usepackage{calculation} % https://ctan.org/pkg/calculation
\parindent0pt\parskip0.6ex % Better for literate code.

\pagestyle{empty}

%{{{ main control macros

% bold item and place it in a box; intended to section-off related theorems
\def\heading#1{\fbox{\bf #1}}

% handy-dandy things to try in this domain we're learning.
\newcommand\heuristic[1][]{ {\color{red} \bf Heuristic \if#1\empty\else\ldq#1\rdq\fi:} }

% a notice ``to do'' sign for current construction sites
\def\todo{ {\color{DarkSlateBlue}\;\fbox{\sc \textbf{To Do ::}}\;} }

% 1 number or referencing item
% 2 name of theorem
% 3 theorem statement
\def\theorem#1#2#3{\parbox{3em}{#1} \ \textbf{#2:} \ \ {#3}}

%}}}
%{{{ logical operators with padding

% suffixes:
% 's' for a ``little'' bit    of space
% 'S' for a ``large''  amount of space
 
\def\equivS{\qquad \equiv \qquad}

\def\eqs{\quad = \quad}

\def\lands{\quad \land \quad}
\def\landS{\qquad \land \qquad}

%}}}
%{{{ category theory identifiers

% identity morphism
\def\Id{\mathsf{id}}

% source and target
\def\src{\mathsf{src}\,}
\def\tgt{\mathsf{tgt}\,}

% converse morphism notation
\def\from{\leftarrow}

% natural transformation
\def\natTo{\mathrel{\ooalign{\,$\longrightarrow$\cr\quad.}}}
%}}}

\theorem{}{Principle of Duality}{
  A statement $S$ is true about $\mathcal{C}$
  iff it's dual $S[\fcmp \,:=\, \circ]$ is true about $\mathcal{C}^{op}$. }

\theorem{}{{\color{gray} Cancellation / } Projection Definitions}{ $\fst \cdot \< f , g \> = f \landS \snd \cdot \< f , g \> = g$ }

\theorem{}{{\color{gray}Fusion / } Pointwise Definition of Pairing}{ $\<f , g \> \cdot h \eqs \<f \cdot h , g \cdot h\>$ }

\theorem{}{Axiom, Product bi-map definition}{ $f \x g \,:=\, \< f \cdot \fst , g \cdot \snd \>$ }

\theorem{}{{\color{gray} Induced} Pointwise Definition of Product bi-map }
        { \hspace{-1em} $(f \x g) \cdot \<i , j \> \eqs \<f \cdot i , g \cdot j\>$ }

\theorem{}{{\color{gray}Absorption /} Induced Definition of Casing}
        { $\[ f , g \] \cdot (i \+ j) = \[ f \cdot i , g \cdot j \]$ }

* COMMENT Maybe move to README
** Gluing functions which do not compose -- products         :move_to_readme:

 Composition is the basis for gluing morphisms together to build more complex morphisms.
 However, not every two morphisms can be glued together by composition.

 # $f : A â† C$ and $g : B â† C$ 
 For instance, functions $A \overset{f}{\longleftarrow} C \overset{g}{\longrightarrow} B$
 do not compose with each other since the source of one is not the target of the other.

 \room

 Since $f$ and $g$ share the same source, their outputs can be paired: $c â†¦ (f\, c, g\, c)$.
 We may think of the operation which pairs the outputs of $f$ and $g$ as a new function
 combinator: $âŸ¨f, gâŸ© = c â†¦ (f\, c, g\, c)$ --read â€œ$f$ /split/ $g$â€.

 \room

 $âŸ¨f, gâŸ©\, c = âŸ¨f\, c, g\, câŸ©$ duplicates $c$ so that $f$ and $g$ can be executed in â€œparallelâ€ on it.

 \room

 Function $âŸ¨f,gâŸ©$ keeps the information of both $f$ and $g$ in the same way
 Cartesian product $A Ã— B$ keeps the information of $A$ and $B$.
 So, in the same way that $A$ data or $B$ data can be retrieved from $A Ã— B$ data
 via the projections $A \overset{\fst}{\longleftarrow} A Ã— B \overset{\snd}{\longrightarrow} B$,
 $f$ and $g$ can be retrieved via the same projections:
 \eqn{Cancellation}{\fst âˆ˜ âŸ¨f, gâŸ© = f \lands \snd âˆ˜ âŸ¨f, gâŸ© = g}

 \room

 A /split/ arises wherever two functions do not compose but share the same source.

 \room

 How do we glue functions that fail such a requisite, say $f : A â† C$ and $g : B â† D$?
 We regard their sources as projections of a product:
 $A \overset{f âˆ˜ \fst}{\longleftarrow} C Ã— D \overset{g âˆ˜ \snd}{\longrightarrow} B$.
 Now they have the same source and so the split combinator can be used:
 $f Ã— g = (c, d) â†¦ (f\, c, g\, d)$.
 #
 This corresponds to the â€œparallelâ€ application of $f$ and $g$, each with its /own/ input.

 \room

 What is the /interplay/ among the functional combinators: Composition, split, product?
 The first two relate to each other via the fusion law
 \eqn{Fusion}{âŸ¨f, gâŸ© âˆ˜ c = âŸ¨f âˆ˜ c, g âˆ˜ câŸ©}
 Notice how it looks like the /definition/ of the split operator but all applications have
 been lifted to compositions! Woah!
   + Moreover, the absorption property is just the lifting of the pointwise definition! Woah!

 \room

 All three combinators interact via the Ã—-absorption property.

 # \room
 #
 # The following is self-inverse,
 # \eqn{swap-Def}{swap = âŸ¨\snd, \fstâŸ© : A Ã— B â‰… B Ã— A}

** Gluing functions which do not compose -- coproducts       :move_to_readme:

 In the scenario $A \overset{f}{\rightlongarrow} C \overset{g}{\leftlongarrow} B$,
 it is clear that the kind of glue we need should make it possible to apply $f$
 if the input is from the â€œ$A$ sideâ€ or apply $g$ if it is from the â€œ$B$ sideâ€.

 \room

 We denote this new combinator â€œeither $f$ or $g$â€, $[f, g] : A + B â†’ C$, where the values of $A + B$
 can be thought of as â€œcopiesâ€ of $A$ or $B$ values which are â€œstampedâ€ with different
 tags in order to guarantee that values which are simultaneously in $A$ and $B$ do not
 get mixed up.

 \room

 Duality is of great /conceptual economy/ since everything that can be said of concept /X/
 can be rephrased for /co-X/. *That $\composition$-listing provides eloquent evidence of duality.*

 \room

 Notice that the fusion law,
 \eqn{Fusion}{ f âˆ˜ [g, h] = [f âˆ˜ g, f âˆ˜ h]}
 Is essentially the definition: If we're in the left `g` then the result is `f` applied to it;
 otherwise if we're in the right `h` then the result is `f` applied to it! Woah: From pointwise to pointfree.
** COMMENT An Introduction to Pointfree Programming

 # Functional programming literally means â€œprogramming with functionsâ€.
 The main emphasis is on /compositionality/, one of the main advantages
 of â€œthinking functionallyâ€, explaining how to construct new functions
 out of other functions using a minimal set of predefined functional
 combinators. This leads to a style which is /pointfree/ in the sense that
 function descriptions dispense with variables --also known as /points/.

 \room

 Why do people look for compact notations?
 A compact notation leads to shorter documents, less lines of code
 in programming, in which patterns are easier to identify and reason
 about. Properties can be stated in clear-cut, one-line long equations
 which are easy to memorise. 
 #
 # Backus' FP and Iverson's APL are examples of pointfree programming
 # languages; look them up ;-)

 \room

 The notation â€œ$f : A â†’ B$â€ focuses on what is relevant about $f$
 and can it be regard as a kind of contract:
 #+BEGIN_QUOTE
 $f$ /commits itself/ to producing a /B/-value provided it is supplied
 with an /A/-value.
 #+END_QUOTE

 \room

 Types provide the â€œglueâ€, or interface, for putting functions
 together to obtain more complex functions.

 \room

 What do we want functions for?
 One wants functions for modelling and reasoning about the
 behaviour of real things.

 \room

 In order to switch between the pointwise and pointfree settings
 we need two â€œbridgesâ€: One lifting equality to the function level
 and the other lifting function application.

 \room

 Two functions $f, g : A â†’ B$ are the same function iff they agree
 at the pointwise level:
 \eqn{Extensionality}{ f = g \equivS \left(âˆ€ a : A â€¢ f\, a = g\, a \right)}

** COMMENT Gluing functions which do not compose --exponentials

 Functional application is the bridge between the pointfree and pointwise worlds.

 Suppose we are given the task to combine two functions, one binary
 $B \overset{f}{\leftlongarrow}{C Ã— A}$ and the other unary $D \overset{g}{\leftlongarrow} A$.
 Clearly none of the combinations $f âˆ˜ g, âŸ¨f, gâŸ©$, or $[f, g]$ is well-typed. Hence $f$ and $g$
 cannot be put together and require some extra interfacing.

 \room

 Note that $âŸ¨f, gâŸ©$ would be well-defined in case the $C$ components of $f$'s source
 could be somehow â€œignoredâ€. Suppose, in fact, that in some particular context
 the first argument of $f$ happens to be â€œirrelevantâ€, or to be frozen to some $c : C$.
 It is easy to derive a new function $f_c : A â†’ B : a â†¦ f(c, a)$ from $f$, combines
 nicely with $g$ via the split combinator: $âŸ¨f_c , gâŸ©$ is well-typed and bears the type
 $B Ã— D â† A$. ---MA: The typing looks erroneous.

 \room

 Since $f_c$ denotes a function of type $B â† A$, we will say $f_c : B^A$, where the exponential
 is an alternate notation for the arrow --the reason to adopt it is that it is functorial, as will be shown.

 We want functions so as to apply them, so it's natural to introduce the /apply/ operator which applies
 a function to an argument:
 \[ ap : B â† B^A Ã— A \]
 \[ ap(f, a) = f \ a \]

 #+LaTeX: \def\transpose#1{ \overline{#1} }

 #+BEGIN_EXPORT latex
 The above â€œtransposeâ€ operation $f â†¦ f_c$ is of type $B^A â† C$.
 It expresses $f$ as a kind of /C/-indexed family of functions $B â† A$.
 It will be denoted by $\transpose{f}$ with the definition $(\transpose{f}\, c)\, a = f(c, a)$.

 Observe that $\transpose{f}$ is more tolerant than $f$: The latter is a binary operator and so
 requires /both/ arguments $(c,a)$ to become available before application, whereas the former is happy
 to be produced with $c$ first  and with $a$ later on.

 \eqn{exponential-Char}{ k = \transpose{f} \equivS f = ap âˆ˜ (k Ã— \Id) }

 Pointwise, $(ap âˆ˜ (\transpose{f} Ã— \Id))\, (c, a) = ap \, (\transpose{f}\, c, a) = \transpose{f} \, c\, a$
 which should be equal to $f(c,a)$ and indeed it is --in pointfree form:

 \eqn{exponential-Cancellation}{ f = ap âˆ˜ (\transpose{f} Ã— \Id) }

 \eqn{exponential-Id}{ \transpose{ap} = \Id_{B^A} }

 \eqn{exponential-Fusion}{ \transpose{g âˆ˜ (f Ã— \Id)} \eqS \transpose{g} âˆ˜ f }

 We can thus introduce a new functional combinator, which arises as the transpose
 of $f âˆ˜ ap$:

 \eqn{exponential-Functor-Type}{ f^A : C^A â† B^A \providedS f : C â† B}

 $B^A$ can be thought of as a /syntactical/ representation of the /semantical/ $A â†’ B$
 --they are the `same', but the way we treat them and think of them is different.
 [ This differs from $(-â†’-)$ when discussing adjunctions earlier, since that was
   the `external hom-/set/'; this is the /internal/ hom! ]

 \eqn{exponential-Functor-Defn}{ f^A = \transpose{f âˆ˜ ap} }
 #+END_EXPORT

 Pointwise: $f^A\, g = f âˆ˜ g$ --so $f^A$ is the â€œcompose with $f$â€ functional combinator.
 # Indeed:
 # 
 #     fáµƒ = T(f . ap)
 # â‰¡   f . ap = ap âˆ˜ (fáµƒ Ã— Id)                 exp-Char
 # â‰¡   f . ap (g, a) = ap âˆ˜ (fáµƒ Ã— Id) (g, a)   extensionality
 # â‰¡   f (g a)       = ap âˆ˜ (fáµƒ g, a)          Ã—-simpl
 # â‰¡   f (g a)       = fáµƒ g a                  ap-simpl
 # â‰¡   f âˆ˜ g         = fáµƒ g                    extensionality

 \eqn{exponential-Functorial}{ (g âˆ˜ h)^A \eqs g^A âˆ˜ h^A \landS \Id^A = \Id }

 *Interestingly:*

 #+BEGIN_EXPORT latex
 \eqn{Ohâ‚€}{ \transpose{\snd} \eqs \const{\Id} }

 \eqn{Ohâ‚}{ \transpose{f}\, a \eqs f âˆ˜ âŸ¨ \constant{a} , \Id âŸ© }

 \eqn{Ohâ‚‚}{ \const{g} \eqs \transpose{g âˆ˜ \snd} }

 Also $\transpose{\snd}$ is a constant function:
 \eqn{Ohâ‚ƒ}{ \transpose{\snd} âˆ˜ f \eqs \transpose{\snd} }
 #+END_EXPORT
* COMMENT Maybe move to a different project
  F-Algebras: â€œRecursion in the Pointfree Styleâ€
** COMMENT Â§3 Recursion in the Pointfree Style

 This section provides a naive introduction to algorithm analysis and synthesis
 by showing how a quite elementary class of algorithms --equivalent to for-loops
 in C or any imperative language-- arise from elementary properties of the underlying
 maths domain.

 The /for-loop/ combinator is defined by
 \[ for \; f \; e \; 0 = e \]
 \[ for \; f \; e \; (n+1) = f (for \; f \; e \; n) \]

 Which can be implemented, in C:
 #+BEGIN_EXAMPLE
T total = e;
for(int j=0; j<n; j++)
   total = f(total);
 #+END_EXAMPLE

 This is the catamorphism for â„•!

 #+BEGIN_EXPORT latex
 \eqn{in-â„•}{ in = [ \const{0}, succ] : 1 + â„• â‰… â„• }

 The above witness: Every natural number is either 0 or the successor of another natural number.
 --That is, such $in$'s characterise inductive types!--

 \eqn{for-loop-universal}{ f = for \; g \; e \equivS f âˆ˜ in = [ \const{k}, g] âˆ˜ (\Id + f)] \eqs â¦‡ [\const{k}, g] â¦ˆ}

 \eqn{for-loop-Id}{ for\; succ\; 0 \eqs \Id}

 \eqn{for-loop-fusion}{ h âˆ˜ for\; g\; k \eqs for\; j \; (h\, k) \providedS h âˆ˜ g = j âˆ˜ h}
 % Proof: Universal property along with fact (a+) âˆ˜ Id = (a+).
 #+END_EXPORT

 *Heuristic* Implement a given function $f$ as a for-loop by using for-loop-Id then for-loop-fusion
 and solving for unknown $j$ along the way; e.g.,

 #+begin_calculation latex
   f
 \step{ Identities and for-loop-Id }
   f âˆ˜ for\; succ\; 0
 \step{ For-loop-fusion, calculute for unknown $j$ in $f âˆ˜ succ = j âˆ˜ f$ }
   for \; j \; (f\; 0)
 #+end_calculation

 Really the $j$ is what $f$ does at the inductive step
 --we cover the base step by computing $f\; 0$ directly.

 For example, in this way we obtain $(a+) = for\; succ\; a$.

 Other fun stuff:
 #+BEGIN_EXPORT latex
 \eqn{simple-optimisation}{ for\; \const{k} \; k \eqs for\; \Id \; k}
 #+END_EXPORT

** COMMENT Â§3.12 F-catamorphisms

 For a polynomial functor $F$ a particular iso /F/-algebra always exists, which
 is denoted by $in : Î¼F â† F (Î¼F)$ and has special properties. First, its carrier
 is the smallest among the carriers of other iso /F/-algebras, and this is why it
 is denoted by $Î¼F$ ---$Î¼$ for â€œminimalâ€.

 #+BEGIN_EXPORT latex
 \eqn{cata-Char}{k = â¦‡Î±â¦ˆ \equivS k âˆ˜ in = Î± âˆ˜ F k }

 \eqn{cata-Id}{ â¦‡inâ¦ˆ = \Id_{Î¼F} }


 \eqn{cata-cancellation}{â¦‡Î±â¦ˆ âˆ˜ in = Î± âˆ˜ F â¦‡Î±â¦ˆ}

 That is, since in is an iso, â€œDecompose input, via out, then recurse, then apply Î±â€:

 \eqn{cata-cancellation}{â¦‡Î±â¦ˆ = Î± âˆ˜ F â¦‡Î±â¦ˆ âˆ˜ out}

 \eqn{cata-Fusion}{ f âˆ˜ â¦‡Î±â¦ˆ = â¦‡Î²â¦ˆ \providedS f âˆ˜ Î± = Î² âˆ˜ F f}
 #+END_EXPORT

 *Go back and re-read Â§3.13* Then do exercises 3.11, 3.16, 3.17.

** COMMENT Â§3.13 Introducing Inductive Datatypes

 A comment on the particular choice of terminology: ~in~ suggests that we are
 going inside, or constructing values of the datatype; whereas ~out~ suggests
 that we are going out, or destructing, analysing, values of the datatype.

 A nifty duality.

** COMMENT Â§3.16 The mutual-recursion law

 Suppose we are executing two (possibly mutually recursive) functions $f$ and $g$ in parallel, which may be catamorphisms
 and so somewhat costly. Could we turn these into a single function call to a catamorphism?

 Solving for $h, k$ in $âŸ¨f, gâŸ© = â¦‡âŸ¨h, kâŸ©â¦ˆ$ yields â€œFokkinga's Lawâ€, aka

 #+BEGIN_EXPORT latex
 \eqn{Mutual-Recursion-Law}{ âŸ¨f, gâŸ© = â¦‡âŸ¨h, kâŸ©â¦ˆ \equivS f âˆ˜ in = h âˆ˜ F âŸ¨f, gâŸ© \landS g âˆ˜ in = k âˆ˜ FâŸ¨f, gâŸ©}
 #+END_EXPORT

 This law is useful in combining two mutually recursive functions into a single catamorphism
 --with added efficiency by possibly using the exchange law.

 *Heuristic*
 We can apply this law to a single function by looking at an â€œinefficientâ€ subpart and
 extracting that into its own function, then trying to apply the law, with possibly more extractions.

  - Isn't the computing-cubes-in-linear-time problem solved this way?

 For example, there is much re-calculation in the definition of fib:
 \[ fib \; 0 = 1 \]
 \[ fib \; 1 = 1 \]
 \[ fib \; (n+2) = fib \; (n+1) + fib\; n \]
 Can we improve its performance?
 The clue is to regard the two instances of $fib$ in the recursive branch as mutually recursive
 over the natural numbers. The clue is /suggested/ not only by $fib$ having two base cases
 --so, perhaps it hides two functions-- but also by the lookahead $n + 2$ in the recursive clause.
 Introducing $f$ by the /specification/ $f\; n = fib(n + 1)$ then using that to obtain a recursive definition,
 possibly depending on $fib$, allows us to use the mutual recursive law to obtain a linear function
 from which $fib$ is obtained via a simple projection --say $\snd$.

 The mutual recursive law generalises to more than two mutually recursive functions, in this case three:
 #+BEGIN_EXPORT latex
 \eqn{Mutual-Recursion-Law}{ âŸ¨fâ‚€, âŸ¨fâ‚, fâ‚‚âŸ©âŸ© = â¦‡âŸ¨hâ‚€, âŸ¨hâ‚, hâ‚‚âŸ©âŸ©â¦ˆ \equivS âˆ€i:0..2 â€¢ fáµ¢ âˆ˜ in = háµ¢ âˆ˜ FâŸ¨fâ‚€, âŸ¨fâ‚, fâ‚‚âŸ©âŸ© }
 #+END_EXPORT

 Now go do exercises 3.23, 3.26, 3.28 ;-)

 An immediate consequence of Mutual-Recursion-Law is
 #+BEGIN_EXPORT latex
 \eqn{Banana-Split-Law}{ âŸ¨â¦‡iâ¦ˆ, â¦‡jâ¦ˆâŸ© = â¦‡(i Ã— j) âˆ˜ âŸ¨F \fst, F \sndâŸ© â¦ˆ }

 This law provides us with a very useful tool for â€œparallel loopâ€ fusion.
 #+END_EXPORT

** COMMENT Â§2.14 Guards and McCarthy's conditional

 Define
 \eqn{?-Defn}{(p?)\, a = \mathsc{If}\, p\, a \,\mathsc{Then}\, \inl\, a \,\mathsc{Else}\, \inr\, a \mathsc{Fi}}

 We call $p? : A â†’ A + A$ the /guard/ associated to predicated /p : A â†’ ğ”¹/.
 The guard is more informative than /p/ alone: It provides information about the outcome of testing /p/
 on some input /a/, encoded in terms of the sum injections, $\inl$ for /true/ and $\inr$ for /false/,
 without losing the input /a/ itself.

 #+BEGIN_EXPORT latex
 \begineqns
 \eqn{McCarthy-Conditional-Defn}{ p â†’ g,h \quad=\quad [g, h] âˆ˜ p?}

 Hence to reason about conditionals one may seek help in the algebra of sums.

 \eqn{conditional-Fusionâ‚}{ f âˆ˜ (p â†’ g, h) \quad=\quad p â†’ f âˆ˜ h, f âˆ˜ h}

 \eqn{conditional-Fusionâ‚‚}{ (p â†’ g, h) âˆ˜ k \quad=\quad p âˆ˜ k â†’ g âˆ˜ k, h âˆ˜ k}

 \eqn{conditional-product-Fusion}{ k âˆ˜ âŸ¨ (p â†’ f,h) , (p â†’ g, i) âŸ©  \eqs p â†’ k âˆ˜ âŸ¨f, gâŸ© , k âˆ˜ âŸ¨h, iâŸ© }

 %
 %   k âˆ˜ âŸ¨ (p â†’ f,h) , (p â†’ g, i) âŸ© 
 % = k âˆ˜ âŸ¨ [f,h] âˆ˜ p? , [g, i] âˆ˜ p? âŸ©
 % = k âˆ˜ âŸ¨ [f,h] , [g, i] âŸ© âˆ˜ p?
 % = k âˆ˜ [ âŸ¨f, gâŸ© , âŸ¨h, iâŸ© ] âˆ˜ p?
 % = [ k âˆ˜ âŸ¨f, gâŸ© , k âˆ˜ âŸ¨h, iâŸ© ] âˆ˜ p?
 % = p â†’ k âˆ˜ âŸ¨f, gâŸ© , k âˆ˜ âŸ¨h, iâŸ©

 \eqn{conditional-Abides-Distributivity}{ âŸ¨ (p â†’ f,h) , (p â†’ g, i) âŸ©  \eqs p â†’ âŸ¨f, gâŸ© , âŸ¨h, iâŸ© }

 \eqn{conditional-Idempotency}{p â†’ f,f \eqs f}

 \eqn{conditional-âŸ¨âŸ©-Distributivity}{ âŸ¨ f, (p â†’ g, h)âŸ© \quad=\quad p â†’ âŸ¨f, gâŸ©, âŸ¨f, hâŸ© }

 \eqn{conditional-Ã—-Distributivity}{ (p â†’ g, h) Ã— f \quad=\quad p âˆ˜ \fst â†’ g Ã— f, h Ã— f}

 \endeqns
 #+END_EXPORT

 \room

 *As well as their duals!*

** COMMENT Wow --another example-- involving conditionals
  Just as in the case of proving /tails/ is a natural transformation *without* using
  any implementing definitions, here's a similar example.

  \room

  Here's an illustration of how /smart/ pointfree algebra can be in reasoning about
  functions that /one does not actually defined explicitly/. 

  It also shows how relevant the /natural properties/ are.

  The issue is that our definition of a guard, \ref{?-Defn}, is pointwise and most
  likely unsuitable to prove facts such as, for instance,
  \[ p? âˆ˜ f = (f + f) âˆ˜ (p âˆ˜ f)? \]
  Thinking better, instead of `inventing' \ref{?-Defn}, we might --and perhaps should!--
  have defined 
  \eqn{two-Defn}{2  \;â‰…\; 1 + 1}
  \eqn{double-Defn}{2 Ã— A \;â‰…\; A + A}
  \eqn{?-Defn-Pointfree}{ p? = double âˆ˜ âŸ¨p, \IdâŸ© }

  which actually express rather closely our strategy of switching from products
  to coproducts in the definition of $(p?)$.

  In particular, *we do not need to define /double/ explicitly*.
  From its type we immediately infer its natural, or free, property:
  \[ double âˆ˜ (\Id Ã— f) \eqs (f + f) âˆ˜ double \]

  It turns out that this is the /knowledge/ we need about $double$
  in order to prove the above mentioned property.

  \room

  The less one has to write to solve a problem, the better.
  One saves time and one's brain, adding to productivity.
  This is often called /elegance/ when applying a scientific method.
** Importance of Compositional Combinators                           :ignore:

 # Paraphrasing Oliveira, Â§2.13
 \vspace{3ex}
 The compositional combinators put forward here are equipped with a concise /set of properties/
 which enable programmers to transform programs, reason about them, and perform useful calculations.
 This raises a /programming methodology/ which is scientific and stable.

 # The relevance of universal properties, such as â„±-Char, is that it offers a way of /solving equations/
 # of the form $y = â„±\, x$. For example, can the identity be expressed, or `reflected', using this combinator?
 # We just solve the equation $\Id = â„±\, x$ for unknown(s) $x$ by appealing to the universal property.

* COMMENT ToDo's
Include:
+ Page 23: Associativity of ~C~ product structs.
+ Page 25: Realisation of sums in ~C~.
+ Pages 31-32: Datatypes, void, unit, bool, maybe.
+ Page 51: Table 2.1 Abstract notation versus programming language data-structures.

+ Go do all the exercises --we have a handy cheat-sheet to help! Let the exercises reinforce the learning!
  - Some of the insightful, or fun ones, maybe include here ;-)
  - Â§2.19 is all exercises!

+ Also, notice that we have natural transformations
   $\initial{-} : \K 0 \natTo \mathsf{Id}$
   and $\final{-} : \mathsf{Id} \natTo \K 1$
  
   Claim?
  
   the cat has a final element iff identity endofunctor has a colimit.
   ie the final element is the union of all elements and so is the
   top or maximal element ;)
  
   $1 \cong CoLim \, \mathsf{Id}$
   where $\final{-} = \gamma$ is the witnessing family?
  
+ Provide a /calculational/ proof of: if a category J has a terminal object 1, 
   then the colimit of any functor F:Jâ†’C is just F(1), with the cocone defined by the maps F(Ï„j):F(j)â†’F(1). 

   Hint: https://math.stackexchange.com/questions/1210440/given-a-mathcalc-rightarrow-mathcale-when-does-operatornamecolim

* COMMENT Duality: Sums & Products :uniform_notation:artefact:

#  \NTHM{}{Principle of Duality}{
#    A statement $S$ is true about $\mathcal{C}$
#    iff it's dual $S[\fcmp \,:=\, \circ]$ is true about $\mathcal{C}^{op}$. }
# 
#+BEGIN_EXPORT latex
\def\fst{\mathsf{fst}}
\def\snd{\mathsf{snd}}
#+END_EXPORT

#+LaTeX_HEADER: \usepackage{stackengine} 
#
# \topinset{*}{O}{1pt}{}
#
#+LaTeX: \def\composition{ \topinset{ï¹”}{âˆ˜}{0.4pt}{} }

In category theory there are two popular notations for composition,
$f ï¹” g = g âˆ˜ f$, and there are two arrow notations $A â†’ B \;=\; B \leftarrow A$,
known as the â€œforwardsâ€ and â€œbackwardsâ€ notations.

\room

Some people prefer one notation and stick with it; however having both in-hand
allows us to say: The /dual/ of a categorical statement formed with 
$ï¹”,â†’$ is obtained by syntactically replacing these two with $âˆ˜, \leftarrow$ respectively
while leaving variables and $\Id$'s alone.

\room

#+LaTeX: \def\dual{\mathsf{dual}}
#
#
For example, applying this process to sums yields /products/:
\vspace{-0.5em}
#+begin_calculation latex 
   h = âŸ¨f, gâŸ©
\step{  We define products as dual to sums }
   h = \dual [f, g]
\step{  dual operation definition }
  \dual\left( h = [f,g] \right)
\step{  []-Char and Leibniz }
  \dual\left( \inl ï¹” h = f  \lands  \inr ï¹” h = g\right) 
\step{  dual operation definition }
  \dual\left(\inl ï¹” h\right) = f  \lands  \dual\left(\inr ï¹” h\right) = g) 
\step{  dual operation definition }
  \dual \text{ } \inl âˆ˜ h = f  \lands  \dual \text{ }  \inr âˆ˜ h = g
\step{  Define: $\fst = \dual\, \inl$, $\snd = \dual\, \inr$ }
  \fst âˆ˜ h = f  \lands  \snd âˆ˜ h = g
\step{  Switch back to ï¹”-notation }
   h ï¹” \fst = f  \lands  h ï¹” \snd = g
#+end_calculation

# Dualising the other sum artefacts yields:
/$(\fst, \snd, A Ã— B)$ form a product of A and B/ if there is an operation
$âŸ¨-,-âŸ©$ satisfying the Char and Type laws below; from which we obtain a host of corollaries.

#+BEGIN_EXPORT latex
\begineqns

\eqn{$\langle\rangle$-Type}{f : C â†’ A \lands g : C â†’ B \impliesS âŸ¨f, gâŸ© : C â†’ A Ã— B}

\eqn{$\langle\rangle$-Char}{ x ï¹” \fst = f \lands x ï¹” \snd = g \equivS x = âŸ¨f, gâŸ© }

\eqn{$\langle\rangle$-Cancellation; $\langle\rangle$-Self}{ âŸ¨f, gâŸ© ï¹” \fst = f \landS âŸ¨f, gâŸ© ï¹” \snd = g}

\eqn{$\langle\rangle$-Id}{ âŸ¨\fst, \sndâŸ© = \Id}

\eqn{$\langle\rangle$-Unique}{ x ï¹” \fst = y ï¹”\fst  \lands x; \snd = y ï¹” \snd \impliesS x = y}

\eqn{$\langle\rangle$-Fusion}{ x ï¹” âŸ¨f , gâŸ© = âŸ¨xï¹”f, x ï¹” gâŸ© }

\eqn{$\langle\rangle$-Functor-Dist}{F \, âŸ¨f, gâŸ©_ğ’ = âŸ¨F \, f , F \, gâŸ©_ğ’Ÿ \qquad\text{ where } F : ğ’ â†’ ğ’Ÿ}

\endeqns
#+END_EXPORT

\room

These are essentially a re-write of the sum laws; let's write the next set of laws only once.

\room

Let the tuple â€œâŸ… âŸ†, â‹†, $l$, $r$, $\composition$â€ be either
â€œâŸ¨ âŸ©, Ã—, $\fst$, $\snd$, $âˆ˜$â€ or â€œ[ ], +, $\inl$, $\inr$, $ï¹”$â€.

\room

For categories in which sums and products exist, we define for $f : A â†’ B$ and $g : C â†’ D$,

\begineqns

\eqn{$\star$-Definition}{ f â‹† g = âŸ… f \composition l, g \composition râŸ† : A â‹† C â†’ B â‹† D}

\eqn{$l,r$-Naturality}{ l \composition (f â‹† g) = f \composition l \landS r \composition (f â‹† g) = g \composition r }

\eqn{Extensionality}{ âŸ…l \composition h, r \composition hâŸ† = h}

\eqn{Absorption}{ (f â‹† g) \composition âŸ…h, jâŸ† = âŸ…f \composition h, g \composition jâŸ† }

\eqn{$\star$-Bi-Functoriality}{ \Id â‹† \Id = \Id \landS (f â‹† g) \composition (h â‹† j) = (f \composition h) â‹† (g \composition j)}

\eqn{Structural Equality}{ âŸ…f,gâŸ† = âŸ…h, jâŸ† \equivS f = h \lands g = j }

\eqn{Interchange Rule}{ âŸ¨[f,g], [h,j]âŸ© = [âŸ¨f,hâŸ©,âŸ¨g,jâŸ©] }

\endeqns

\room

Notice that the last law above is self-dual.


* COMMENT README

C-c C-c: evalute src block

#+NAME: make-readme
#+BEGIN_SRC emacs-lisp :results none
(with-temp-buffer
    (insert 
    "#+EXPORT_FILE_NAME: README.md
     #+HTML: <h1> CatsCheatSheet </h1>
     # OPTIONS: toc:nil

     This project is to contain a listing of common theorems in elementary category theory.
     
     *The listing sheet, as PDF, can be found [[https://github.com/alhassy/CatsCheatSheet/blob/master/CheatSheet.pdf][here]]*, 
     while below is an unruly html rendition.
     
     This reference sheet is built around the system https://github.com/alhassy/CheatSheet
     
     #+TOC: headlines 2
     #+INCLUDE: CheatSheet.org     
    ")
    ;; (set-visited-file-name "ReadIt2.md")
    (org-mode)
    (org-md-export-to-markdown)
)
#+END_SRC


* COMMENT footer

(find-file "CheatSheet.el")

# Local Variables:
# eval: (org-babel-tangle)
# eval: (progn (org-babel-goto-named-src-block "make-readme") (org-babel-execute-src-block) (outline-hide-sublevels 1))
# eval: (load-file "CheatSheet.el")
# compile-command: (my-org-latex-export-to-pdf)
# End:
